{
  
    
        "post0": {
            "title": "Convert A Raspberry Pi Into An Alexa Device",
            "content": "Convert your Raspberry Pi into an Alexa device using Alexa Voice Service SDK . Back in 2017 , my company was working on a product that embedded Amazon Alexa Voice Service (AVS) on device and my team was involved in the onboarding solution for the device. . In order to better understand the use cases, I installed Amazon Alexa Voice Service SDK on a Raspberry Pi . . Here is a video of a short interaction with Alexa Voice Service installed on Raspberry Pi . https://youtu.be/bajws_5RN8M . If you are interested in creating an Alexa device on Raspberry pi please follow the latest instructions using this link . https://developer.amazon.com/en-US/docs/alexa/avs-device-sdk/raspberry-pi.html . Provisioning an AVS device under your Amazon Account . If you are distributing such custom devices, you need to provide a way for the user to onboard Alexa under their Amazon account. I built a POC for this provisioning flow using Nodejs (and also a Java version). Here are the steps involved. . Create a developer account under Amazon if you don’t already have one. . | Browse to https://developer.amazon.com/alexa/console/avs/products . | Click on Add New Product . | Fill up the information regarding your device. Give it a Product Name and Product ID . | Here are the other options I chose for my device . . . Save the product details . | Choose Security Profile and select create new profile . . | Fill up the option for security profile name and description and click Next . . | The rest of the information like Profile ID, Client ID and Client secret are now automatically filled up for you . . | I kept the options to just Web based flow but you may choose to add Android and iOS app integration (as we did in the final product) . | Fill up the allowed origin and callback URL details. Callback URl is the address of the page to which Amazon authorization service will redirect to with the result of User authorization . This is the service or page that needs to handle the authorization response. . | AVS Device Provisioning App flow . This is how the flow looks like . . The code for this App flow can be found under this repository https://github.com/ravindrabharathi/AVS-provisioning-nodejs . . Although I have not updated the code for recent version of dependency libraries, the code still works as the screenshots above are recent. I’ll follow up with another post explaining the code structure and important parts that handle authorization, provisioning of device, etc .",
            "url": "https://ravindrabharathi.github.io/blog/2020/10/30/Convert-a-Raspberry-Pi-into-an-Alexa-device.html",
            "relUrl": "/2020/10/30/Convert-a-Raspberry-Pi-into-an-Alexa-device.html",
            "date": " • Oct 30, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "tf_utils",
            "content": "tf_utils is a Utility package for training CNN using tensorflow2, TFRecords, tf.data Supports the following . Download dataset (cifar10 for now..more will be added) from source url | Store the Dataset as TFRecords | Retreve image data as tf.data.TFRecordDataset | Image Augmentation (random-pad_crop, flip-left_right, cutout) of Image Batches | Plot images from Dataset | Plot misclassified images | Plot Confusion Matrix | This post contains the instructions on how to use this package and as an illustration we will build and train a Convolutional Neural Network to classiff images in CIFAR-10 using utility functions in this package. . Choose tf2 backend on colab . from __future__ import absolute_import, division, print_function, unicode_literals # Install TensorFlow try: # %tensorflow_version only exists in Colab. %tensorflow_version 2.x except Exception: pass import tensorflow as tf . TensorFlow 2.x selected. . Install tf_utils . !pip install --upgrade git+https://github.com/ravindrabharathi/tf_utils . Collecting git+https://github.com/ravindrabharathi/tf_utils Cloning https://github.com/ravindrabharathi/tf_utils to /tmp/pip-req-build-qburk2q2 Running command git clone -q https://github.com/ravindrabharathi/tf_utils /tmp/pip-req-build-qburk2q2 Building wheels for collected packages: tf-utils Building wheel for tf-utils (setup.py) ... done Created wheel for tf-utils: filename=tf_utils-0.1-cp36-none-any.whl size=8410 sha256=57f93a09fc190011514c76b43f58e32284ae1f6c1cffd4ccbd194aaa3f1d1029 Stored in directory: /tmp/pip-ephem-wheel-cache-h0gqcjtw/wheels/95/af/bb/690b94c65a5aad47a5c39e75f158a2b043448e908c5c121791 Successfully built tf-utils Installing collected packages: tf-utils Successfully installed tf-utils-0.1 . import the data module . import tf_utils.data as ds . Finished &#39;get_cpu_num&#39; in 0.0000 secs . set batch size . ds.batch_size=128 . download CIFAR-10 data and create tf records . ds.get_cifar10_and_create_tfrecords() . Finished &#39;download_file&#39; in 12.9797 secs Finished &#39;download_cifar10_files&#39; in 12.9802 secs Done Finished &#39;extract_cifar10_files&#39; in 2.0184 secs Finished &#39;_get_file_names&#39; in 0.0000 secs Generating ./train.tfrecords Finished &#39;read_pickle_from_file&#39; in 0.1625 secs Finished &#39;read_pickle_from_file&#39; in 0.1538 secs Finished &#39;read_pickle_from_file&#39; in 0.1396 secs Finished &#39;read_pickle_from_file&#39; in 0.1343 secs Finished &#39;read_pickle_from_file&#39; in 0.1334 secs Finished &#39;convert_to_tfrecord&#39; in 3.1668 secs Done! Generating ./eval.tfrecords Finished &#39;read_pickle_from_file&#39; in 0.1315 secs Finished &#39;convert_to_tfrecord&#39; in 0.6586 secs Done! Finished &#39;create_tf_records&#39; in 3.8268 secs Finished &#39;get_cifar10_and_create_tfrecords&#39; in 18.8262 secs . create train and test dataset . train_ds=ds.get_train_ds() test_ds=ds.get_eval_ds() . WARNING:tensorflow:Entity &lt;function parse_batch_distort at 0x7fb397563ea0&gt; could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module &#39;gast&#39; has no attribute &#39;Num&#39; WARNING: Entity &lt;function parse_batch_distort at 0x7fb397563ea0&gt; could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module &#39;gast&#39; has no attribute &#39;Num&#39; distorting image Tensor(&#34;Shape_2:0&#34;, shape=(4,), dtype=int32) Finished &#39;get_tf_dataset&#39; in 4.3847 secs Finished &#39;get_tf_dataset_in_batches&#39; in 4.3848 secs Finished &#39;get_train_ds&#39; in 4.3851 secs WARNING:tensorflow:Entity &lt;function parse_batch at 0x7fb397563e18&gt; could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module &#39;gast&#39; has no attribute &#39;Num&#39; WARNING: Entity &lt;function parse_batch at 0x7fb397563e18&gt; could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module &#39;gast&#39; has no attribute &#39;Num&#39; Finished &#39;get_tf_dataset&#39; in 0.1213 secs Finished &#39;get_tf_dataset_in_batches&#39; in 0.1214 secs Finished &#39;get_eval_ds&#39; in 0.1215 secs . import visualization module . import tf_utils.visualize as vz . plot images from train dataset , train dataset by default uses image augmenttation of cutout,flip-left-right,random-pad-crop . vz.plot_cifar10_files(train_ds) . Training - Build model , compile and train . Import necessary TF modules and define a convolution block which consists of a Convolution layer followed b batchNormalization and then ReLU activation . from tensorflow.keras.models import Model from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Activation from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization from tensorflow.keras.optimizers import Adam def conv(inp,f=32,k=3): conv_layer=Conv2D(f,k,use_bias=False,padding=&#39;same&#39;)(inp) conv_layer=BatchNormalization()(conv_layer) conv_layer=Activation(&#39;relu&#39;)(conv_layer) return conv_layer . Define the CNN model . def build_model(): inp=Input(shape=(32,32,3)) layer1=conv(inp,32,3) layer2=conv(layer1,32,3) mp1=MaxPooling2D(pool_size=(2,2))(layer2) layer3=conv(mp1,64,3) layer4=conv(layer3,64,3) mp2=MaxPooling2D(pool_size=(2,2))(layer4) layer5=conv(mp2,128,3) layer6=conv(layer5,128,3) layer7=Conv2D(10,1,use_bias=False)(layer6) layer8=GlobalAveragePooling2D()(layer7) out=Activation(&#39;softmax&#39;)(layer8) model=Model(inputs=[inp],outputs=[out]) return model . build and compile the model using Adam optimizer and categorical cross entropy as the loss function . model=build_model() model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;] ) . Train the model for 25 epochs . import numpy as np batch_size=128 model.fit(train_ds,epochs=25, steps_per_epoch=np.ceil(50000/batch_size), validation_data=test_ds, validation_steps=np.ceil(10000/batch_size), verbose=1) . Train for 391.0 steps, validate for 79.0 steps Epoch 1/25 WARNING:tensorflow:Entity &lt;function Function._initialize_uninitialized_variables.&lt;locals&gt;.initialize_variables at 0x7fb397559620&gt; could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module &#39;gast&#39; has no attribute &#39;Num&#39; WARNING: Entity &lt;function Function._initialize_uninitialized_variables.&lt;locals&gt;.initialize_variables at 0x7fb397559620&gt; could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module &#39;gast&#39; has no attribute &#39;Num&#39; 391/391 [==============================] - 18s 47ms/step - loss: 1.3526 - accuracy: 0.5111 - val_loss: 1.5464 - val_accuracy: 0.4656 Epoch 2/25 391/391 [==============================] - 14s 36ms/step - loss: 1.0219 - accuracy: 0.6366 - val_loss: 1.1223 - val_accuracy: 0.6303 Epoch 3/25 391/391 [==============================] - 14s 36ms/step - loss: 0.8953 - accuracy: 0.6814 - val_loss: 0.9815 - val_accuracy: 0.6649 Epoch 4/25 391/391 [==============================] - 14s 36ms/step - loss: 0.8130 - accuracy: 0.7158 - val_loss: 0.9184 - val_accuracy: 0.6879 Epoch 5/25 391/391 [==============================] - 14s 36ms/step - loss: 0.7453 - accuracy: 0.7398 - val_loss: 1.1683 - val_accuracy: 0.6241 Epoch 6/25 391/391 [==============================] - 14s 37ms/step - loss: 0.6941 - accuracy: 0.7580 - val_loss: 1.0465 - val_accuracy: 0.6707 Epoch 7/25 391/391 [==============================] - 14s 36ms/step - loss: 0.6569 - accuracy: 0.7702 - val_loss: 1.0750 - val_accuracy: 0.6390 Epoch 8/25 391/391 [==============================] - 14s 37ms/step - loss: 0.6242 - accuracy: 0.7829 - val_loss: 0.7580 - val_accuracy: 0.7443 Epoch 9/25 391/391 [==============================] - 14s 37ms/step - loss: 0.5996 - accuracy: 0.7922 - val_loss: 0.6795 - val_accuracy: 0.7642 Epoch 10/25 391/391 [==============================] - 14s 36ms/step - loss: 0.5790 - accuracy: 0.7985 - val_loss: 0.7111 - val_accuracy: 0.7598 Epoch 11/25 391/391 [==============================] - 14s 36ms/step - loss: 0.5529 - accuracy: 0.8070 - val_loss: 0.8552 - val_accuracy: 0.7270 Epoch 12/25 391/391 [==============================] - 14s 36ms/step - loss: 0.5371 - accuracy: 0.8149 - val_loss: 0.7650 - val_accuracy: 0.7474 Epoch 13/25 391/391 [==============================] - 14s 36ms/step - loss: 0.5202 - accuracy: 0.8196 - val_loss: 0.6854 - val_accuracy: 0.7816 Epoch 14/25 391/391 [==============================] - 14s 37ms/step - loss: 0.5036 - accuracy: 0.8264 - val_loss: 0.5586 - val_accuracy: 0.8126 Epoch 15/25 391/391 [==============================] - 14s 36ms/step - loss: 0.4856 - accuracy: 0.8317 - val_loss: 0.6011 - val_accuracy: 0.8037 Epoch 16/25 391/391 [==============================] - 14s 37ms/step - loss: 0.4780 - accuracy: 0.8329 - val_loss: 0.5151 - val_accuracy: 0.8314 Epoch 17/25 391/391 [==============================] - 14s 37ms/step - loss: 0.4667 - accuracy: 0.8388 - val_loss: 0.7123 - val_accuracy: 0.7809 Epoch 18/25 391/391 [==============================] - 14s 37ms/step - loss: 0.4551 - accuracy: 0.8417 - val_loss: 0.5962 - val_accuracy: 0.8099 Epoch 19/25 391/391 [==============================] - 14s 37ms/step - loss: 0.4450 - accuracy: 0.8458 - val_loss: 0.6972 - val_accuracy: 0.7747 Epoch 20/25 391/391 [==============================] - 14s 36ms/step - loss: 0.4379 - accuracy: 0.8478 - val_loss: 0.6022 - val_accuracy: 0.8059 Epoch 21/25 391/391 [==============================] - 14s 36ms/step - loss: 0.4237 - accuracy: 0.8539 - val_loss: 0.5210 - val_accuracy: 0.8326 Epoch 22/25 391/391 [==============================] - 14s 36ms/step - loss: 0.4201 - accuracy: 0.8538 - val_loss: 0.5323 - val_accuracy: 0.8283 Epoch 23/25 391/391 [==============================] - 14s 37ms/step - loss: 0.4051 - accuracy: 0.8587 - val_loss: 0.6117 - val_accuracy: 0.8053 Epoch 24/25 391/391 [==============================] - 14s 37ms/step - loss: 0.4031 - accuracy: 0.8596 - val_loss: 0.7287 - val_accuracy: 0.7778 Epoch 25/25 391/391 [==============================] - 14s 37ms/step - loss: 0.3946 - accuracy: 0.8623 - val_loss: 0.5481 - val_accuracy: 0.8227 . &lt;tensorflow.python.keras.callbacks.History at 0x7fb33e455630&gt; . We Trained the model for 5 epochs and have a validation accuracy of about 82% . plot misclassifed images . Let us now plot some mis-classified images using the visualization module to get an idea of what the model is getting wrong . res=vz.get_misclassified_images(model,test_ds) vz.plot_misclassified_images(res[0],res[1],res[2],res[3],52) . 79/79 [==============================] - 1s 10ms/step . First 52 misclassified images . . . . . . . . . . . . . . Confusion matrix . We can also get the confusion matrix using the plot_confusion_matrix function included in the Vizualization module . vz.plot_confusion_matrix(model,test_ds) . 79/79 [==============================] - 1s 10ms/step .",
            "url": "https://ravindrabharathi.github.io/blog/tensorflow2/image%20classification/cnn/trecords/confusion%20matrix/image%20augmentation/2020/04/05/tf-utils-utility-package-for-image-classification.html",
            "relUrl": "/tensorflow2/image%20classification/cnn/trecords/confusion%20matrix/image%20augmentation/2020/04/05/tf-utils-utility-package-for-image-classification.html",
            "date": " • Apr 5, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Face Aging using CycleGANs",
            "content": "In this post we will train a GAN model to map Age progression in faces- i.e generate an aged photo of a person given a photo at younger age (and also the reverse). We will use CycleGANs for this. . . . . CycleGANs were introduced in this paper titled Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks where the authors presented an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. . For the images of faces with various ages we will be using the UTKFace dataset which has a cropped image set of only faces marked with age , gender , race , etc. . We will be using following two good references that use CycleGAN in order to build and train our models . https://github.com/sungnam0/Face-Aging-with-CycleGAN 2.https://machinelearningmastery.com/cyclegan-tutorial-with-keras/ | Import necessary modules . import numpy as np import keras import tensorflow as tf from keras.layers import Input,Conv2D,MaxPooling2D,Flatten,Activation,BatchNormalization,K,UpSampling2D from keras.layers import Dropout,GlobalAveragePooling2D,LeakyReLU,Dense,Reshape, concatenate,Conv2DTranspose from keras.models import Model,load_model import matplotlib.pyplot as plt #import keras.backend as K import os import time from datetime import datetime from keras.applications import InceptionResNetV2 from keras.callbacks import TensorBoard from keras.optimizers import Adam from keras.utils import to_categorical from keras_preprocessing import image from numpy import asarray from numpy import vstack from keras.preprocessing.image import img_to_array from keras.preprocessing.image import load_img from numpy import savez_compressed import pandas as pd import os from matplotlib import pyplot from numpy import load from random import random from numpy import load from numpy import zeros from numpy import ones from numpy import asarray from numpy.random import randint from keras.optimizers import Adam . Set tf backend config to allocate memory as needed instead of pre-allocating . config = tf.ConfigProto() config.gpu_options.allow_growth = True # Create a session with the above options specified. keras.backend.tensorflow_backend.set_session(tf.Session(config=config)) . Mount google drive to retrieve and store files used in this project . from google.colab import drive drive.mount(&#39;/content/drive&#39;, force_remount=True) . Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&amp;response_type=code Enter your authorization code: ·········· Mounted at /content/drive . !ls -al &#39;/content/drive/My Drive/FaceGAN/&#39; . total 105354 drwx 2 root root 4096 Aug 22 10:59 results -rw- 1 root root 1239467 Aug 22 09:56 utk_data.csv drwx 2 root root 4096 Aug 21 08:41 UTKFace -rw- 1 root root 106634631 Aug 21 07:15 UTKFace.tar.gz . Get the UTKFace dataset . !tar zxf &#39;/content/drive/My Drive/FaceGAN/UTKFace.tar.gz&#39; UTKFace . Parse the data . The labels of each face image is embedded in the file name, formated like [age][gender][race]_[date&amp;time].jpg . [age] is an integer from 0 to 116, indicating the age [gender] is either 0 (male) or 1 (female) [race] is an integer from 0 to 4, denoting White, Black, Asian, Indian, and Others (like Hispanic, Latino, Middle Eastern). [date&amp;time] is in the format of yyyymmddHHMMSSFFF, showing the date and time an image was collected to UTKFace . data=[] for filename in os.listdir(&#39;./UTKFace&#39;): parts=filename.split(&#39;_&#39;) #print(parts[3]) item={} item[&#39;image&#39;]=filename item[&#39;age&#39;]=parts[0] item[&#39;gender&#39;]=parts[1] item[&#39;race&#39;]=parts[2] if (len(parts)==4): item[&#39;date_time&#39;]=parts[3] data.append(item) utk_data=pd.DataFrame(data) utk_data.describe() . age date_time gender image race . count 23708 | 23705 | 23708 | 23708 | 23708 | . unique 104 | 23479 | 2 | 23708 | 8 | . top 26 | 20170110173815028.jpg.chip.jpg | 0 | 26_1_0_20170112213001988.jpg.chip.jpg | 0 | . freq 2197 | 7 | 12391 | 1 | 10078 | . utk_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 23708 entries, 0 to 23707 Data columns (total 5 columns): age 23708 non-null object date_time 23705 non-null object gender 23708 non-null object image 23708 non-null object race 23708 non-null object dtypes: object(5) memory usage: 926.2+ KB . utk_data.head() . age date_time gender image race . 0 25 | 20170119172104288.jpg.chip.jpg | 1 | 25_1_3_20170119172104288.jpg.chip.jpg | 3 | . 1 25 | 20170117141726361.jpg.chip.jpg | 1 | 25_1_0_20170117141726361.jpg.chip.jpg | 0 | . 2 27 | 20170116001407357.jpg.chip.jpg | 0 | 27_0_1_20170116001407357.jpg.chip.jpg | 1 | . 3 10 | 20170103200501766.jpg.chip.jpg | 0 | 10_0_4_20170103200501766.jpg.chip.jpg | 4 | . 4 26 | 20170116184024662.jpg.chip.jpg | 1 | 26_1_0_20170116184024662.jpg.chip.jpg | 0 | . we do not need date time , so delete it . del utk_data[&#39;date_time&#39;] . define a category for ages and apply it to the dataset . def age_cat_fn(age): age=int(age) if (0&lt;age&lt;18): return 0 elif(18&lt;=age&lt;=25): return 1 elif (25&lt;age&lt;=39): return 2 elif (39&lt; age &lt;=49): return 3 elif (49 &lt; age &lt;=60): return 4 elif age&gt;60: return 5 . utk_data[&#39;age_cat&#39;]=utk_data.age.map(age_cat_fn) . utk_data.to_csv(&#39;utk_data.csv&#39;,sep=&#39;,&#39;) !cp &#39;utk_data.csv&#39; &#39;/content/drive/My Drive/EIP3/session7&#39; . data with age category . utk_data.head() . age gender image race age_cat . 0 25 | 1 | 25_1_3_20170119172104288.jpg.chip.jpg | 3 | 1 | . 1 25 | 1 | 25_1_0_20170117141726361.jpg.chip.jpg | 0 | 1 | . 2 27 | 0 | 27_0_1_20170116001407357.jpg.chip.jpg | 1 | 2 | . 3 10 | 0 | 10_0_4_20170103200501766.jpg.chip.jpg | 4 | 0 | . 4 26 | 1 | 26_1_0_20170116184024662.jpg.chip.jpg | 0 | 2 | . split our data into two Domains . Young : age category 1 | Old : Age category 4 | data_A=utk_data[utk_data.age_cat==1] data_B=utk_data[utk_data.age_cat==4] . print(data_A[&#39;age_cat&#39;].count()) print(data_B[&#39;age_cat&#39;].count()) . 3404 2592 . Get the images belonging to the two Domains and save as a compressed numpy array so that we can load them when necesary instead of processing the UTKFace dataset multiple times . image_dir=&#39;./UTKFace/&#39; image_paths_A = data_A[&#39;image&#39;].tolist() image_paths_B = data_B[&#39;image&#39;].tolist() #print(image_paths[:10]) . images_A=None images_B=None #store 2000 images for A for i, image_path in enumerate(image_paths_A): if (i&lt;2000): if (i%1000==0): print(&quot;processing set A image num &quot;+str(i)) try: # Load image loaded_image = image.load_img(image_dir+image_path, target_size=(128,128,3)) # Convert PIL image to numpy ndarray loaded_image = image.img_to_array(loaded_image) # Add another dimension (Add batch dimension) loaded_image = np.expand_dims(loaded_image, axis=0) # Concatenate all images into one tensor if images_A is None: images_A = loaded_image else: images_A = np.concatenate([images_A, loaded_image], axis=0) except Exception as e: print(&quot;Error:&quot;, i, e) #store 2000 images for B for i, image_path in enumerate(image_paths_B): if (i&lt;2000): if (i%999==0): print(&quot;processing set B image num &quot;+str(i)) try: # Load image loaded_image = image.load_img(image_dir+image_path, target_size=(128,128,3)) # Convert PIL image to numpy ndarray loaded_image = image.img_to_array(loaded_image) # Add another dimension (Add batch dimension) loaded_image = np.expand_dims(loaded_image, axis=0) # Concatenate all images into one tensor if images_B is None: images_B = loaded_image else: images_B = np.concatenate([images_B, loaded_image], axis=0) except Exception as e: print(&quot;Error:&quot;, i, e) . processing set A image num 0 processing set A image num 1000 processing set B image num 0 processing set B image num 999 processing set B image num 1998 . print(&#39;images_A :&#39;) print(images_A.shape) print(&#39;images_B :&#39;) print(images_B.shape) . images_A : (2000, 128, 128, 3) images_B : (2000, 128, 128, 3) . filename = &#39;/content/drive/My Drive/EIP3/session7/utkface_128.npz&#39; savez_compressed(filename, images_A, images_B) print(&#39;Saved dataset: &#39;, filename) . Saved dataset: /content/drive/My Drive/EIP3/session7/utkface_128.npz . Load the saved numpy arrays and plot some images from either domain . from numpy import load from matplotlib import pyplot # load the dataset data = load(&#39;/content/drive/My Drive/EIP3/session7/utkface_128.npz&#39;) dataA, dataB = data[&#39;arr_0&#39;], data[&#39;arr_1&#39;] print(&#39;Loaded: &#39;, dataA.shape, dataB.shape) # plot source images n_samples = 3 for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(dataA[i].astype(&#39;uint8&#39;)) # plot target image for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + n_samples + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(dataB[i].astype(&#39;uint8&#39;)) pyplot.show() . Loaded: (2000, 128, 128, 3) (2000, 128, 128, 3) . Install keras-contrib so that we can use InstanceNormalization instead of BatchNormalization . !pip install git+https://www.github.com/keras-team/keras-contrib.git . from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization from keras.initializers import RandomNormal . Define helper functions for the various components of the Model that we are going to build . Conv layers . def conv2d(input_, output_dim, ks=4, s=2, stddev=0.02, padding=&#39;SAME&#39;,name=&#39;c2d&#39;): return Conv2D(output_dim,kernel_size=ks,strides=s,padding=padding,kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),name=name)(input_) . Leaky Relu . def lrelu(input_,name=&#39;lr&#39;): return LeakyReLU(alpha=0.2,name=name)(input_) . InstanceNormalization . def iNorm(input_,name=&#39;iNorm&#39;): return InstanceNormalization(axis=-1,name=name)(input_) . Discriminator Model . def build_discriminator(image_shape): # weight initialization #init = RandomNormal(stddev=0.02) # source image input in_image = Input(shape=image_shape) #C1 d1 = lrelu(conv2d(in_image,64,4,name=&#39;d_c1&#39;),&#39;lr1&#39; ) # C2 d2 = lrelu(iNorm(conv2d(d1,128,4,name=&#39;d_c2&#39;),&#39;iN2&#39;),&#39;lr2&#39;) # C3 d3 = lrelu(iNorm(conv2d(d1,256,4,name=&#39;d_c3&#39;),&#39;iN3&#39;),&#39;lr3&#39;) # C4 d4 = lrelu(iNorm(conv2d(d3,512,4,name=&#39;d_c4&#39;),&#39;iN4&#39;),&#39;lr4&#39;) &#39;&#39;&#39; # second last output layer d = conv2d(in_image,128,3,1) d = iNorm(d) d = lrelu(d) &#39;&#39;&#39; # output d5 = conv2d(d4,1,4,1,name=&#39;d_c5&#39;) #Conv2D(1, 4,1, padding=&#39;same&#39;, kernel_initializer=init)(d) # define model model = Model(in_image, d5) # compile model model.compile(loss=&#39;mse&#39;, optimizer=Adam(lr=0.0002, beta_1=0.5), loss_weights=[0.5]) return model . disc=build_discriminator(dataB[0].shape) disc.summary() . WARNING: Logging before flag parsing goes to stderr. W0823 12:23:56.827917 140499755456384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead. W0823 12:23:56.872185 140499755456384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. W0823 12:23:57.044422 140499755456384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. . _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 128, 128, 3) 0 _________________________________________________________________ d_c1 (Conv2D) (None, 64, 64, 64) 3136 _________________________________________________________________ lr1 (LeakyReLU) (None, 64, 64, 64) 0 _________________________________________________________________ d_c3 (Conv2D) (None, 32, 32, 256) 262400 _________________________________________________________________ iN3 (InstanceNormalization) (None, 32, 32, 256) 512 _________________________________________________________________ lr3 (LeakyReLU) (None, 32, 32, 256) 0 _________________________________________________________________ d_c4 (Conv2D) (None, 16, 16, 512) 2097664 _________________________________________________________________ iN4 (InstanceNormalization) (None, 16, 16, 512) 1024 _________________________________________________________________ lr4 (LeakyReLU) (None, 16, 16, 512) 0 _________________________________________________________________ d_c5 (Conv2D) (None, 16, 16, 1) 8193 ================================================================= Total params: 2,372,929 Trainable params: 2,372,929 Non-trainable params: 0 _________________________________________________________________ . function to add padding . def padd3(input_): import tensorflow as tf return tf.pad(input_, [[0, 0], [3, 3], [3, 3], [0, 0]], &quot;REFLECT&quot;) def padd1(input_): import tensorflow as tf return tf.pad(input_, [[0, 0], [1, 1], [1, 1], [0, 0]], &quot;REFLECT&quot;) . The generator uses Resnet Blocks , as defined below . from keras.layers import Add,Lambda def res_block(input_,nf=64,ks=3,s=1,name=&#39;res_blk&#39;): p=int((ks-1)/2) y=Lambda(padd1)(input_) #(tf.pad(input_,[[0,0],[p,p],[p,p],[0,0]],&#39;REFLECT&#39;)) y=iNorm(conv2d(y,nf,ks,s,padding=&#39;VALID&#39;,name=name+&#39;_c1&#39;),name=name+&#39;_iN1&#39;) y=Lambda(padd1)(y) #(tf.pad(tf.nn.relu(y),[[0,0],[p,p],[p,p],[0,0]],&#39;REFLECT&#39;)) y=iNorm(conv2d(y,nf,ks,s,padding=&#39;VALID&#39;,name=name+&#39;_c2&#39;),name=name+&#39;_iN2&#39;) y1=keras.layers.Add()([y,input_]) return y1 . deconvolution layers . def deconv2d(input_, output_dim, ks=4, s=2, stddev=0.02, padding=&#39;SAME&#39;,name=&#39;dc2d&#39;): #Conv2DTranspose(64, (3,3), strides=(2,2), padding=&#39;same&#39;, kernel_initializer=init)(g) dcv=Conv2DTranspose(output_dim,(ks,ks),strides=(s,s),padding=padding,kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),name=name)(input_) return dcv . generator model . from keras.layers import Lambda,Conv2DTranspose def build_generator(image_shape): nf=64 # num filters for first layer input_=Input(shape=(128,128,3)) c0 = Lambda(padd3)(input_) c1 = Activation(&#39;relu&#39;)(iNorm(conv2d(c0, nf, 7, 1, padding=&#39;VALID&#39;, name=&#39;g_e1_c&#39;), &#39;g_e1_bn&#39;)) c2 = Activation(&#39;relu&#39;)(iNorm(conv2d(c1, nf*2, 3, 2, name=&#39;g_e2_c&#39;), &#39;g_e2_bn&#39;)) c3 = Activation(&#39;relu&#39;)(iNorm(conv2d(c2, nf*4 , 3, 2, name=&#39;g_e3_c&#39;), &#39;g_e3_bn&#39;)) r1 = res_block(c3, nf*4, name=&#39;g_r1&#39;) r2 = res_block(r1, nf*4, name=&#39;g_r2&#39;) r3 = res_block(r2, nf*4, name=&#39;g_r3&#39;) r4 = res_block(r3, nf*4, name=&#39;g_r4&#39;) r5 = res_block(r4, nf*4, name=&#39;g_r5&#39;) r6 = res_block(r5, nf*4, name=&#39;g_r6&#39;) r7 = res_block(r6, nf*4, name=&#39;g_r7&#39;) r8 = res_block(r7, nf*4, name=&#39;g_r8&#39;) r9 = res_block(r8, nf*4, name=&#39;g_r9&#39;) d1=Conv2DTranspose(nf*2, (3,3), strides=(2,2), padding=&#39;same&#39;, kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),name=&#39;g_d1_dc&#39;)(r9) d1=Activation(&#39;relu&#39;)(iNorm(d1,name=&#39;g_d1_bn&#39;)) d2=Conv2DTranspose(nf, (3,3), strides=(2,2), padding=&#39;same&#39;, kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),name=&#39;g_d2_dc&#39;)(d1) d2=Activation(&#39;relu&#39;)(iNorm(d2,name=&#39;g_d2_bn&#39;)) d2 = Lambda(padd3)(d2)#(tf.pad(d2, [[0, 0], [3, 3], [3, 3], [0, 0]], &quot;REFLECT&quot;)) d3=conv2d(d2, 3 , 7, 1, padding=&#39;VALID&#39;, name=&#39;g_pred_c&#39;) pred=Activation(&#39;tanh&#39;)(d3) model=Model(input_,pred) return model . gen=build_generator(dataA[0].shape) gen.summary() . __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_3 (InputLayer) (None, 128, 128, 3) 0 __________________________________________________________________________________________________ lambda_20 (Lambda) (None, 134, 134, 3) 0 input_3[0][0] __________________________________________________________________________________________________ g_e1_c (Conv2D) (None, 128, 128, 64) 9472 lambda_20[0][0] __________________________________________________________________________________________________ g_e1_bn (InstanceNormalization) (None, 128, 128, 64) 128 g_e1_c[0][0] __________________________________________________________________________________________________ activation_4 (Activation) (None, 128, 128, 64) 0 g_e1_bn[0][0] __________________________________________________________________________________________________ g_e2_c (Conv2D) (None, 64, 64, 128) 73856 activation_4[0][0] __________________________________________________________________________________________________ g_e2_bn (InstanceNormalization) (None, 64, 64, 128) 256 g_e2_c[0][0] __________________________________________________________________________________________________ activation_5 (Activation) (None, 64, 64, 128) 0 g_e2_bn[0][0] __________________________________________________________________________________________________ g_e3_c (Conv2D) (None, 32, 32, 256) 295168 activation_5[0][0] __________________________________________________________________________________________________ g_e3_bn (InstanceNormalization) (None, 32, 32, 256) 512 g_e3_c[0][0] __________________________________________________________________________________________________ activation_6 (Activation) (None, 32, 32, 256) 0 g_e3_bn[0][0] __________________________________________________________________________________________________ lambda_21 (Lambda) (None, 34, 34, 256) 0 activation_6[0][0] __________________________________________________________________________________________________ g_r1_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_21[0][0] __________________________________________________________________________________________________ g_r1_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r1_c1[0][0] __________________________________________________________________________________________________ lambda_22 (Lambda) (None, 34, 34, 256) 0 g_r1_iN1[0][0] __________________________________________________________________________________________________ g_r1_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_22[0][0] __________________________________________________________________________________________________ g_r1_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r1_c2[0][0] __________________________________________________________________________________________________ add_10 (Add) (None, 32, 32, 256) 0 g_r1_iN2[0][0] activation_6[0][0] __________________________________________________________________________________________________ lambda_23 (Lambda) (None, 34, 34, 256) 0 add_10[0][0] __________________________________________________________________________________________________ g_r2_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_23[0][0] __________________________________________________________________________________________________ g_r2_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r2_c1[0][0] __________________________________________________________________________________________________ lambda_24 (Lambda) (None, 34, 34, 256) 0 g_r2_iN1[0][0] __________________________________________________________________________________________________ g_r2_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_24[0][0] __________________________________________________________________________________________________ g_r2_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r2_c2[0][0] __________________________________________________________________________________________________ add_11 (Add) (None, 32, 32, 256) 0 g_r2_iN2[0][0] add_10[0][0] __________________________________________________________________________________________________ lambda_25 (Lambda) (None, 34, 34, 256) 0 add_11[0][0] __________________________________________________________________________________________________ g_r3_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_25[0][0] __________________________________________________________________________________________________ g_r3_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r3_c1[0][0] __________________________________________________________________________________________________ lambda_26 (Lambda) (None, 34, 34, 256) 0 g_r3_iN1[0][0] __________________________________________________________________________________________________ g_r3_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_26[0][0] __________________________________________________________________________________________________ g_r3_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r3_c2[0][0] __________________________________________________________________________________________________ add_12 (Add) (None, 32, 32, 256) 0 g_r3_iN2[0][0] add_11[0][0] __________________________________________________________________________________________________ lambda_27 (Lambda) (None, 34, 34, 256) 0 add_12[0][0] __________________________________________________________________________________________________ g_r4_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_27[0][0] __________________________________________________________________________________________________ g_r4_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r4_c1[0][0] __________________________________________________________________________________________________ lambda_28 (Lambda) (None, 34, 34, 256) 0 g_r4_iN1[0][0] __________________________________________________________________________________________________ g_r4_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_28[0][0] __________________________________________________________________________________________________ g_r4_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r4_c2[0][0] __________________________________________________________________________________________________ add_13 (Add) (None, 32, 32, 256) 0 g_r4_iN2[0][0] add_12[0][0] __________________________________________________________________________________________________ lambda_29 (Lambda) (None, 34, 34, 256) 0 add_13[0][0] __________________________________________________________________________________________________ g_r5_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_29[0][0] __________________________________________________________________________________________________ g_r5_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r5_c1[0][0] __________________________________________________________________________________________________ lambda_30 (Lambda) (None, 34, 34, 256) 0 g_r5_iN1[0][0] __________________________________________________________________________________________________ g_r5_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_30[0][0] __________________________________________________________________________________________________ g_r5_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r5_c2[0][0] __________________________________________________________________________________________________ add_14 (Add) (None, 32, 32, 256) 0 g_r5_iN2[0][0] add_13[0][0] __________________________________________________________________________________________________ lambda_31 (Lambda) (None, 34, 34, 256) 0 add_14[0][0] __________________________________________________________________________________________________ g_r6_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_31[0][0] __________________________________________________________________________________________________ g_r6_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r6_c1[0][0] __________________________________________________________________________________________________ lambda_32 (Lambda) (None, 34, 34, 256) 0 g_r6_iN1[0][0] __________________________________________________________________________________________________ g_r6_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_32[0][0] __________________________________________________________________________________________________ g_r6_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r6_c2[0][0] __________________________________________________________________________________________________ add_15 (Add) (None, 32, 32, 256) 0 g_r6_iN2[0][0] add_14[0][0] __________________________________________________________________________________________________ lambda_33 (Lambda) (None, 34, 34, 256) 0 add_15[0][0] __________________________________________________________________________________________________ g_r7_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_33[0][0] __________________________________________________________________________________________________ g_r7_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r7_c1[0][0] __________________________________________________________________________________________________ lambda_34 (Lambda) (None, 34, 34, 256) 0 g_r7_iN1[0][0] __________________________________________________________________________________________________ g_r7_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_34[0][0] __________________________________________________________________________________________________ g_r7_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r7_c2[0][0] __________________________________________________________________________________________________ add_16 (Add) (None, 32, 32, 256) 0 g_r7_iN2[0][0] add_15[0][0] __________________________________________________________________________________________________ lambda_35 (Lambda) (None, 34, 34, 256) 0 add_16[0][0] __________________________________________________________________________________________________ g_r8_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_35[0][0] __________________________________________________________________________________________________ g_r8_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r8_c1[0][0] __________________________________________________________________________________________________ lambda_36 (Lambda) (None, 34, 34, 256) 0 g_r8_iN1[0][0] __________________________________________________________________________________________________ g_r8_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_36[0][0] __________________________________________________________________________________________________ g_r8_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r8_c2[0][0] __________________________________________________________________________________________________ add_17 (Add) (None, 32, 32, 256) 0 g_r8_iN2[0][0] add_16[0][0] __________________________________________________________________________________________________ lambda_37 (Lambda) (None, 34, 34, 256) 0 add_17[0][0] __________________________________________________________________________________________________ g_r9_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_37[0][0] __________________________________________________________________________________________________ g_r9_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r9_c1[0][0] __________________________________________________________________________________________________ lambda_38 (Lambda) (None, 34, 34, 256) 0 g_r9_iN1[0][0] __________________________________________________________________________________________________ g_r9_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_38[0][0] __________________________________________________________________________________________________ g_r9_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r9_c2[0][0] __________________________________________________________________________________________________ add_18 (Add) (None, 32, 32, 256) 0 g_r9_iN2[0][0] add_17[0][0] __________________________________________________________________________________________________ g_d1_dc (Conv2DTranspose) (None, 64, 64, 128) 295040 add_18[0][0] __________________________________________________________________________________________________ g_d1_bn (InstanceNormalization) (None, 64, 64, 128) 256 g_d1_dc[0][0] __________________________________________________________________________________________________ activation_7 (Activation) (None, 64, 64, 128) 0 g_d1_bn[0][0] __________________________________________________________________________________________________ g_d2_dc (Conv2DTranspose) (None, 128, 128, 64) 73792 activation_7[0][0] __________________________________________________________________________________________________ g_d2_bn (InstanceNormalization) (None, 128, 128, 64) 128 g_d2_dc[0][0] __________________________________________________________________________________________________ activation_8 (Activation) (None, 128, 128, 64) 0 g_d2_bn[0][0] __________________________________________________________________________________________________ lambda_39 (Lambda) (None, 134, 134, 64) 0 activation_8[0][0] __________________________________________________________________________________________________ g_pred_c (Conv2D) (None, 128, 128, 3) 9411 lambda_39[0][0] __________________________________________________________________________________________________ activation_9 (Activation) (None, 128, 128, 3) 0 g_pred_c[0][0] ================================================================================================== Total params: 11,388,675 Trainable params: 11,388,675 Non-trainable params: 0 __________________________________________________________________________________________________ . composite Model with two genartors and discriminator . def build_composite_model(g_model_1, d_model, g_model_2, image_shape): # ensure the model we&#39;re updating is trainable g_model_1.trainable = True # mark discriminator as not trainable d_model.trainable = False # mark other generator model as not trainable g_model_2.trainable = False # discriminator element input_gen = Input(shape=image_shape) gen1_out = g_model_1(input_gen) output_d = d_model(gen1_out) # identity element input_id = Input(shape=image_shape) output_id = g_model_1(input_id) # forward cycle output_f = g_model_2(gen1_out) # backward cycle gen2_out = g_model_2(input_id) output_b = g_model_1(gen2_out) # define model graph model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b]) # define optimization algorithm configuration opt = Adam(lr=0.0002, beta_1=0.5) # compile model with weighting of least squares loss and L1 loss model.compile(loss=[&#39;mse&#39;, &#39;mae&#39;, &#39;mae&#39;, &#39;mae&#39;], loss_weights=[1, 5, 10, 10], optimizer=opt) return model . The original samples are over 3100 per domain and it is increasing the time for each epoch(has proven problematic in the initial training runs). So we will use a function to get a subsample of the training data , 1000 per Domain . def get_subsample(dataset): t1=np.random.randint(900) t2=np.random.randint(1200,2000) t3=np.random.randint(2500,2800) return np.vstack((dataset[0][t1:t1+300],dataset[0][t2:t2+400],dataset[0][t3:t3+300])),np.vstack((dataset[1][t1:t1+300], dataset[1][t2:t2+400],dataset[1][t3:t3+300])) . def get_subsample2(dataset): t0=np.random.randint(250) t1=np.random.randint(300) t2=np.random.randint(1200,2000) t3=np.random.randint(2500,2800) return np.vstack((dataset[0][t1:t1+800],dataset[0][t2:t2+200])),np.vstack((dataset[1][t1:t1+100], dataset[2][t0:t0+900])) . Utility Functions to load the image samples , generate fake images , save Models , Save genrated images , etc . def load_real_samples2(filename): data = load(filename) X1,X2,X3 = data[&#39;arr_0&#39;],data[&#39;arr_1&#39;],data[&#39;arr_2&#39;] X1= (X1-127.5)/127.5 X2 = (X2-127.5)/127.5 X3 = (X3-127.5)/127.5 return X1,X2,X3 . def load_real_samples(filename): # load the dataset data = load(filename) # unpack arrays X1, X2 = data[&#39;arr_0&#39;], data[&#39;arr_1&#39;] # scale from [0,255] to [-1,1] X1 = (X1 - 127.5) / 127.5 X2 = (X2 - 127.5) / 127.5 return [X1, X2] # select a batch of random samples, returns images and target def generate_real_samples(dataset, n_samples, patch_shape): # choose random instances ix = randint(0, dataset.shape[0], n_samples) # retrieve selected images X = dataset[ix] # generate &#39;real&#39; class labels (1) y = ones((n_samples, patch_shape, patch_shape, 1)) return X, y # generate a batch of images, returns images and targets def generate_fake_samples(g_model, dataset, patch_shape): # generate fake instance X = g_model.predict(dataset) # create &#39;fake&#39; class labels (0) y = zeros((len(X), patch_shape, patch_shape, 1)) return X, y # save the generator models to file def save_models(step, g_model_AtoB, g_model_BtoA): # save the first generator model filename1 = &#39;/content/drive/My Drive/EIP3/session7/g_model_AtoB_%06d.h5&#39; % (step+1) g_model_AtoB.save(filename1) # save the second generator model filename2 = &#39;/content/drive/My Drive/EIP3/session7/g_model_BtoA_%06d.h5&#39; % (step+1) g_model_BtoA.save(filename2) print(&#39;&gt;Saved: %s and %s&#39; % (filename1, filename2)) # save the generator models to file def save_models2(step, g_model_AtoB, g_model_BtoA,d_model_A,d_model_B): # save the first generator model filename1 = &#39;/content/drive/My Drive/EIP3/session7/g_model_AtoB_%06d.h5&#39; % (step+1) g_model_AtoB.save(filename1) # save the second generator model filename2 = &#39;/content/drive/My Drive/EIP3/session7/g_model_BtoA_%06d.h5&#39; % (step+1) g_model_BtoA.save(filename2) # save the first discriminator model A filename3 = &#39;/content/drive/My Drive/EIP3/session7/d_model_A_%06d.h5&#39; % (step+1) d_model_A.save(filename3) # save the first discriminator model B filename4 = &#39;/content/drive/My Drive/EIP3/session7/d_model_B_%06d.h5&#39; % (step+1) d_model_B.save(filename4) print(&#39;&gt;Saved: %s , %s , %s and %s&#39; % (filename1, filename2,filename3,filename4)) . def summarize_performance(step, g_model, trainX, name, n_samples=5): pyplot.figure( figsize=(15, 8), dpi=120) # select a sample of input images X_in, _ = generate_real_samples(trainX, n_samples, 0) # generate translated images X_out, _ = generate_fake_samples(g_model, X_in, 0) # scale all pixels from [-1,1] to [0,1] X_in = (X_in + 1) / 2.0 X_out = (X_out + 1) / 2.0 # plot real images for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(X_in[i]) # plot translated image for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + n_samples + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(X_out[i]) # save plot to file filename1 = &#39;/content/drive/My Drive/EIP3/session7/%s_generated_plot_%06d.png&#39; % (name, (step+1)) pyplot.savefig(filename1) pyplot.close() . Maintain a pool of 50 images as described in the paper . def update_image_pool(pool, images, max_size=50): selected = list() for image in images: if len(pool) &lt; max_size: # stock the pool pool.append(image) selected.append(image) elif random() &lt; 0.5: # use image, but don&#39;t add it to the pool selected.append(image) else: # replace an existing image and use replaced image ix = randint(0, len(pool)) selected.append(pool[ix]) pool[ix] = image return asarray(selected) . function to run the training . def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size,n_epochs): # define properties of the training run n_epochs, n_batch, = n_epochs, batch_size # determine the output square shape of the discriminator n_patch = d_model_A.output_shape[1] # unpack dataset trainA, trainB = get_subsample(dataset) # prepare image pool for fakes poolA, poolB = list(), list() # calculate the number of batches per training epoch bat_per_epo = int(len(trainA) / n_batch) # calculate the number of training iterations n_steps = bat_per_epo * n_epochs # manually enumerate epochs for i in range(n_steps): # select a batch of real samples X_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch) X_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch) # generate a batch of fake samples X_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch) X_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch) # update fakes from pool X_fakeA = update_image_pool(poolA, X_fakeA) X_fakeB = update_image_pool(poolB, X_fakeB) # update generator B-&gt;A via adversarial and cycle loss g_loss2, _, _, _, _ = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA]) # update discriminator for A -&gt; [real/fake] dA_loss1 = d_model_A.train_on_batch(X_realA, y_realA) dA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA) # update generator A-&gt;B via adversarial and cycle loss g_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB]) # update discriminator for B -&gt; [real/fake] dB_loss1 = d_model_B.train_on_batch(X_realB, y_realB) dB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB) # summarize performance print(&#39;&gt;%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]&#39; % (i+1, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2)) # evaluate the model performance every so often if (i+1) % (bat_per_epo * 1) == 0: # plot A-&gt;B translation summarize_performance(i, g_model_AtoB, trainA, &#39;AtoB&#39;) # plot B-&gt;A translation summarize_performance(i, g_model_BtoA, trainB, &#39;BtoA&#39;) if (i+1) % (bat_per_epo * 5) == 0: # save the models save_models2(i, g_model_AtoB, g_model_BtoA,d_model_A,d_model_B) . define the models and run training . from random import random from numpy import load from numpy import zeros from numpy import ones from numpy import asarray from numpy.random import randint from keras.optimizers import Adam # load image data dataset = load_real_samples(&#39;/content/drive/My Drive/EIP3/session7/utkface_128.npz&#39;) print(&#39;Loaded&#39;, dataset[0].shape, dataset[1].shape) # define input shape based on the loaded dataset image_shape = dataset[0].shape[1:] # generator: A -&gt; B g_model_AtoB = build_generator(image_shape) # generator: B -&gt; A g_model_BtoA = build_generator(image_shape) # discriminator: A -&gt; [real/fake] d_model_A = build_discriminator(image_shape) # discriminator: B -&gt; [real/fake] d_model_B = build_discriminator(image_shape) # composite: A -&gt; B -&gt; [real/fake, A] c_model_AtoB = build_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape) # composite: B -&gt; A -&gt; [real/fake, B] c_model_BtoA = build_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape) # train models train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=2,n_epochs=10) . # load image data dataset = load_real_samples(&#39;/content/drive/My Drive/EIP3/session7/utkface_128.npz&#39;) print(&#39;Loaded&#39;, dataset[0].shape, dataset[1].shape) # define input shape based on the loaded dataset image_shape = dataset[0].shape[1:] #load the previously trained model cust = {&#39;InstanceNormalization&#39;: InstanceNormalization, &#39;tf&#39;: tf} # generator: A -&gt; B g_model_AtoB = load_model(&#39;/content/drive/My Drive/EIP3/session7/g_model_AtoB_005625.h5&#39;, cust) # generator: B -&gt; A g_model_BtoA = load_model(&#39;/content/drive/My Drive/EIP3/session7/g_model_BtoA_005625.h5&#39;, cust) # discriminator: A -&gt; [real/fake] d_model_A = build_discriminator(image_shape) # discriminator: B -&gt; [real/fake] d_model_B = build_discriminator(image_shape) # composite: A -&gt; B -&gt; [real/fake, A] c_model_AtoB = build_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape) # composite: B -&gt; A -&gt; [real/fake, B] c_model_BtoA = build_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape) . dataset = load_real_samples2(&#39;/content/drive/My Drive/EIP3/session7/utkface_128_2.npz&#39;) . After training the model for slighly over 100 epochs (Although more epochs will give better results) , we will try out the results of this training using the show_results function. . def show_results( g_model, trainX, n_samples=5,title=&#39;A to B&#39;): pyplot.figure( figsize=(12, 6), dpi=120) # select a sample of input images X_in, _ = generate_real_samples(trainX, n_samples, 0) # generate translated images X_out, _ = generate_fake_samples(g_model, X_in, 0) # scale all pixels from [-1,1] to [0,1] X_in = (X_in + 1) / 2.0 X_out = (X_out + 1) / 2.0 # plot real images #pyplot.title(title) for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(X_in[i]) pyplot.show() print(&quot; ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ &quot;+title+&quot; ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓&quot;) # plot translated image pyplot.figure( figsize=(12, 6), dpi=120) for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + n_samples + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(X_out[i]) pyplot.show() . Here are some samples of generated images : . . . . . . . . Training the models for more epochs will make the results better, especially for Young to Old Translation . Also we used 128x128 images due to time and compute constraints . Training on the original 200x200 image size would have yielded better results .",
            "url": "https://ravindrabharathi.github.io/blog/dl/cyclegan/gan/2020/04/02/Face-Aging-Cycle-GANs.html",
            "relUrl": "/dl/cyclegan/gan/2020/04/02/Face-Aging-Cycle-GANs.html",
            "date": " • Apr 2, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Classify images in CIFAR-100 Dataset",
            "content": "In this post we will see how to use transfer learning (where we use what a model has learned in one task to solve another similar task) . We will use a pretrained ResNet model trained on ImageNet dataset to learn and classify images in the CIFAR-100 dataset. . We will use a ResNet34 pretrained model from https://github.com/qubvel/classification_models . We will use Resnet34 model to try and achieve 80% validation accuracy . Since pretrained weights are only available for imagenet and models expect a 224x224 image size , we will resize the cifar100 images to 224x224 while training . . In the pretrained model we will remove the top prediction layers and freeze the last 11 layers . We will add a GlobalAveragepooling2D layer , a dense layer and a softmax activation to form our prediction layer for cifar100. The first part will be to train with the frozen layers in base model . After training for about 30 epochs , we will unfreeze the layers and train further . . Install the required files from qubvel keras applications project in order to get the pretrained ResNet model . !pip install git+https://github.com/qubvel/classification_models.git . Collecting git+https://github.com/qubvel/classification_models.git Cloning https://github.com/qubvel/classification_models.git to /tmp/pip-req-build-5x6yx4oj Running command git clone -q https://github.com/qubvel/classification_models.git /tmp/pip-req-build-5x6yx4oj Running command git submodule update --init --recursive -q Requirement already satisfied: keras_applications&lt;=1.0.8,&gt;=1.0.7 in /usr/local/lib/python3.6/dist-packages (from image-classifiers==1.0.0) (1.0.8) Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras_applications&lt;=1.0.8,&gt;=1.0.7-&gt;image-classifiers==1.0.0) (2.8.0) Requirement already satisfied: numpy&gt;=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras_applications&lt;=1.0.8,&gt;=1.0.7-&gt;image-classifiers==1.0.0) (1.17.4) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py-&gt;keras_applications&lt;=1.0.8,&gt;=1.0.7-&gt;image-classifiers==1.0.0) (1.12.0) Building wheels for collected packages: image-classifiers Building wheel for image-classifiers (setup.py) ... done Created wheel for image-classifiers: filename=image_classifiers-1.0.0-cp36-none-any.whl size=19950 sha256=be0aa5db89758bc9c55b9e6009bebb1222b086b087e45a6c54c4fb8e56b48877 Stored in directory: /tmp/pip-ephem-wheel-cache-cw05kyqp/wheels/de/2b/fd/29a6d33edb8c28bc7d94e95ea1d39c9a218ac500a3cfb1b197 Successfully built image-classifiers Installing collected packages: image-classifiers Successfully installed image-classifiers-1.0.0 . Import necessary keras modules , numpy and matplotlib . from keras import backend as K import time import matplotlib.pyplot as plt import numpy as np % matplotlib inline np.random.seed(2017) #from keras.models import Sequential from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D from keras.layers import Activation, Flatten, Dropout from keras.layers import BatchNormalization from keras.utils import np_utils import os . import ResNet34 and image preprocessing from the project we installed earlier . import keras import cv2 #from classification_models.resnet import ResNet34, preprocess_input from classification_models.keras import Classifiers ResNet34, preprocess_input = Classifiers.get(&#39;resnet34&#39;) . get cifar100 dataset from keras datasets . from keras.datasets import cifar100 (train_features, train_labels), (test_features, test_labels) = cifar100.load_data() num_train, img_channels, img_rows, img_cols = train_features.shape num_test, _, _, _ = test_features.shape num_classes = len(np.unique(train_labels)) . Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz 169009152/169001437 [==============================] - 4s 0us/step . preprocess the images to make sure that they are in the format required by the pretrained model . train_features = preprocess_input(train_features) test_features = preprocess_input(test_features) . print max and min pixel values in the images which we can use in the ramdom-erase/cutout augmentation later . print(np.max(train_features),np.min(train_features)) . 255 0 . Store cifar100 train and test images in a local data folder. We will load these images using an imagedatagenerator and resize to 224x224 which is default size for Resnet-imagenet models . !rm -R ./data/ # remove old data direrctory to clean up . rm: cannot remove &#39;./data/&#39;: No such file or directory . sub_dir=&#39;train&#39; data_dir=&#39;./data&#39; if not os.path.exists(data_dir): os.mkdir(data_dir) image_dir=&#39;./data/&#39;+sub_dir+&#39;/&#39; if not os.path.exists(image_dir): os.mkdir(image_dir) . os.getcwd() . &#39;/content&#39; . os.path.exists(&#39;./data/train&#39;) . True . def save_img(images,sub_dir): c=0 os.chdir(&#39;/content/&#39;) curr_dir = os.getcwd() image_dir=&#39;./data/&#39;+sub_dir+&#39;/&#39; if not os.path.exists(image_dir): os.mkdir(image_dir) os.chdir(image_dir) print(&#39;current working directory is &#39;+os.getcwd()) for img in images: c +=1 filename=str(c)+&#39;.jpg&#39; cv2.imwrite(filename,img) print(&quot;files resized and saved to &quot;+image_dir) os.chdir(curr_dir) print(&#39;current working directory is &#39;+os.getcwd()) . save_img(train_features,&#39;train&#39;) . current working directory is /content/data/train files resized and saved to ./data/train/ current working directory is /content . save_img(test_features,&#39;test&#39;) . current working directory is /content/data/test files resized and saved to ./data/test/ current working directory is /content . !ls ./data . test train . Mount google drive to save best model while training . from google.colab import drive drive.mount(&#39;/gdrive&#39;,force_remount=True) . Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&amp;response_type=code&amp;scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly Enter your authorization code: ·········· Mounted at /gdrive . Import pandas and create a dataframe with image files and labels information. We will use this dataframe with Keras imagedatagenerator to load images for training and testing and calculate loss using the corresponding label values . import pandas as pd . def form_df(label_type=&#39;train&#39;): if label_type==&#39;train&#39;: labels=train_labels else: labels=test_labels file_name=[] class_label=[] for i in range(len(labels)): filename=str(i+1)+&#39;.jpg&#39; file_name.append(filename) class_label.append(str(labels[i][0])) df=pd.DataFrame({&#39;File&#39;:file_name,&#39;Class&#39;:class_label}) return df . train_df=form_df(&#39;train&#39;) print(train_df.head()) train_df.info() . File Class 0 1.jpg 19 1 2.jpg 29 2 3.jpg 0 3 4.jpg 11 4 5.jpg 1 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 50000 entries, 0 to 49999 Data columns (total 2 columns): File 50000 non-null object Class 50000 non-null object dtypes: object(2) memory usage: 781.4+ KB . train_df.tail() . File Class . 49995 49996.jpg | 80 | . 49996 49997.jpg | 7 | . 49997 49998.jpg | 3 | . 49998 49999.jpg | 7 | . 49999 50000.jpg | 73 | . test_df=form_df(&#39;test&#39;) print(test_df.head()) print(test_df.tail()) print(test_df.info()) . File Class 0 1.jpg 49 1 2.jpg 33 2 3.jpg 72 3 4.jpg 51 4 5.jpg 71 File Class 9995 9996.jpg 83 9996 9997.jpg 14 9997 9998.jpg 51 9998 9999.jpg 42 9999 10000.jpg 70 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 10000 entries, 0 to 9999 Data columns (total 2 columns): File 10000 non-null object Class 10000 non-null object dtypes: object(2) memory usage: 156.4+ KB None . Custom function for random-pad-crop augmentation . def pad4(img): pad_size=img.shape[1]//8 img=np.pad(img, [ (pad_size, pad_size), (pad_size, pad_size), (0, 0)], mode=&#39;reflect&#39;) return img def random_pad_crop_img(img,crop_size=224): crop_size=img.shape[1] img=pad4(img) pad=img.shape[1]-crop_size x1=np.random.randint(pad) x2=x1+crop_size y1=np.random.randint(pad) y2=y1+crop_size img=img[x1:x2,y1:y2,:] return img . We will now get the ResNet34 model weights for imagenet (Cifar is not available in this library). . input shape set to 224,224,3 . Add GlobalAveragePooling to convert these to 1D inputs suitable for the softmax prediction layer . Add a Dense Layer instead of the one we removed from the pretrained model . Add softmax prediction . for the first train run we will freeze the all layers of the pretrained model except the last 11 layers . # build model from keras.layers import GlobalAveragePooling2D, Add, Lambda, Dense, GlobalMaxPooling2D #base modek from REsnet34 base_model = ResNet34(input_shape=(224,224,3), weights=&#39;imagenet&#39;, include_top=False) #Freeze all but last 11 layers for layer in base_model.layers[:-11]: layer.trainable=False for layer in base_model.layers: print(layer, layer.trainable) #Add our own Top/Prediction layers x = GlobalAveragePooling2D()(base_model.output) x= Dense(num_classes,use_bias=False)(x) output = keras.layers.Activation(&#39;softmax&#39;)(x) model = keras.models.Model(inputs=[base_model.input], outputs=[output]) . Compile the model using Stochastic Gradient descent optimizer with momentum of 0.9 and lr of 0.015 . from keras.optimizers import SGD opt=SGD(lr=0.015, momentum=0.9, nesterov=True) model.compile(optimizer=opt, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead. . we want to get the model with best validation accuracy for the prediction task and so we will save the best model from the various epochs in Google Drive using ModelCheckpoint callback available in Keras . define a Modelcheckpoint to save the best Model . from keras.callbacks import ModelCheckpoint model_save_path=&#39;/gdrive/My Drive/EVA/session20/best_model2.h5&#39; chkpoint_model=ModelCheckpoint(model_save_path, monitor=&#39;val_acc&#39;, verbose=1, save_best_only=True, save_weights_only=False, mode=&#39;max&#39;) . Cutout Augmentation . Cutout was first presented as an effective augmentation technique in these two papers : . Improved Regularization of Convolutional Neural Networks with Cutout and Random Erasing Data Augmentation . The idea is to randomly cut away patches of information from images that a model is training on to force it to learn from more parts of the image. This would help the model learn more features about a class instead of depending on some simple assumptions using smaller areas within the image . This helps the model generalize better and make better predictions . . We will use python code for random erasing found at https://github.com/yu4u/cutout-random-erasing . !wget https://raw.githubusercontent.com/yu4u/cutout-random-erasing/master/random_eraser.py . --2019-12-04 02:57:37-- https://raw.githubusercontent.com/yu4u/cutout-random-erasing/master/random_eraser.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 888 [text/plain] Saving to: ‘random_eraser.py’ random_eraser.py 100%[===================&gt;] 888 --.-KB/s in 0s 2019-12-04 02:57:37 (191 MB/s) - ‘random_eraser.py’ saved [888/888] . Train the model for 100 epochs using a batch size of 128 . We will use a ImageDataGenerator to apply image augmentation of random-pad-crop, horizontal Flip and CutOut augmentation for the training . from random_eraser import get_random_eraser eraser = get_random_eraser(p=0.8, s_l=0.15, s_h=0.25,r_1=0.5, r_2=1/0.5,v_l=0,v_h=255,pixel_level=False) def img_aug1(img): img=random_pad_crop_img(img) img=eraser(img) return img . def scheduler(epoch): if epoch &lt; 30: return 0.01 elif 30 &lt; epoch &lt; 50: return 0.008 else: return 0.008 * tensorflow.math.exp(0.1 * (50 - epoch)) lr_callback = keras.callbacks.LearningRateScheduler(scheduler) . from tensorflow.keras.preprocessing.image import ImageDataGenerator EPOCHS=100 batch_size=128 train_datagen=ImageDataGenerator( preprocessing_function=img_aug1, horizontal_flip=True ) val_datagen= ImageDataGenerator( ) training_generator = train_datagen.flow_from_dataframe(train_df, directory=&#39;./data/train/&#39;, x_col=&#39;File&#39;, y_col=&#39;Class&#39;, target_size=(224, 224), color_mode=&#39;rgb&#39;, interpolation=&#39;bicubic&#39;, class_mode=&#39;categorical&#39;, batch_size=batch_size, shuffle=True, seed=42) validation_generator = val_datagen.flow_from_dataframe(test_df, directory=&#39;./data/test/&#39;, x_col=&#39;File&#39;, y_col=&#39;Class&#39;, target_size=(224, 224),interpolation=&#39;bicubic&#39;, color_mode=&#39;rgb&#39;, class_mode=&#39;categorical&#39;, batch_size=batch_size, shuffle=True, seed=42) . Found 50000 validated image filenames belonging to 100 classes. Found 10000 validated image filenames belonging to 100 classes. . def scheduler(epoch): if epoch &lt; 5: return 0.02 elif 5 &lt; epoch &lt; 12: return 0.015 elif 12 &lt; epoch &lt; 20: return 0.010 elif 20 &lt; epoch &lt; 25: return 0.007 else: return 0.003 lr_callback = keras.callbacks.LearningRateScheduler(scheduler) . model.fit_generator(training_generator, epochs=30, steps_per_epoch=np.ceil(train_features.shape[0]/batch_size), validation_steps=np.ceil(test_features.shape[0]/batch_size), validation_data=validation_generator, shuffle=True, callbacks=[chkpoint_model,lr_callback], verbose=1) . WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead. Epoch 1/30 391/391 [==============================] - 143s 367ms/step - loss: 3.0429 - acc: 0.2592 - val_loss: 3.3339 - val_acc: 0.2888 Epoch 00001: val_acc improved from -inf to 0.28880, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 2/30 391/391 [==============================] - 131s 334ms/step - loss: 2.4142 - acc: 0.3753 - val_loss: 2.7019 - val_acc: 0.3728 Epoch 00002: val_acc improved from 0.28880 to 0.37280, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 3/30 391/391 [==============================] - 130s 332ms/step - loss: 2.2485 - acc: 0.4119 - val_loss: 2.9838 - val_acc: 0.3559 Epoch 00003: val_acc did not improve from 0.37280 Epoch 4/30 391/391 [==============================] - 130s 332ms/step - loss: 2.1262 - acc: 0.4380 - val_loss: 2.9980 - val_acc: 0.3657 Epoch 00004: val_acc did not improve from 0.37280 Epoch 5/30 391/391 [==============================] - 130s 333ms/step - loss: 2.0500 - acc: 0.4526 - val_loss: 3.5103 - val_acc: 0.3361 Epoch 00005: val_acc did not improve from 0.37280 Epoch 6/30 391/391 [==============================] - 129s 331ms/step - loss: 1.8938 - acc: 0.4893 - val_loss: 3.0115 - val_acc: 0.3799 Epoch 00006: val_acc improved from 0.37280 to 0.37990, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 7/30 391/391 [==============================] - 129s 331ms/step - loss: 1.9202 - acc: 0.4823 - val_loss: 3.1336 - val_acc: 0.3733 Epoch 00007: val_acc did not improve from 0.37990 Epoch 8/30 391/391 [==============================] - 129s 330ms/step - loss: 1.8907 - acc: 0.4902 - val_loss: 3.0686 - val_acc: 0.3787 Epoch 00008: val_acc did not improve from 0.37990 Epoch 9/30 391/391 [==============================] - 130s 332ms/step - loss: 1.8420 - acc: 0.5018 - val_loss: 3.2207 - val_acc: 0.3743 Epoch 00009: val_acc did not improve from 0.37990 Epoch 10/30 391/391 [==============================] - 129s 331ms/step - loss: 1.8068 - acc: 0.5090 - val_loss: 3.5011 - val_acc: 0.3613 Epoch 00010: val_acc did not improve from 0.37990 Epoch 11/30 391/391 [==============================] - 129s 330ms/step - loss: 1.7793 - acc: 0.5141 - val_loss: 3.0006 - val_acc: 0.3945 Epoch 00011: val_acc improved from 0.37990 to 0.39450, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 12/30 391/391 [==============================] - 129s 331ms/step - loss: 1.7491 - acc: 0.5239 - val_loss: 3.0046 - val_acc: 0.4053 Epoch 00012: val_acc improved from 0.39450 to 0.40530, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 13/30 391/391 [==============================] - 130s 332ms/step - loss: 1.6473 - acc: 0.5471 - val_loss: 3.0496 - val_acc: 0.4013 Epoch 00013: val_acc did not improve from 0.40530 Epoch 14/30 391/391 [==============================] - 129s 331ms/step - loss: 1.6570 - acc: 0.5460 - val_loss: 3.2120 - val_acc: 0.3886 Epoch 00014: val_acc did not improve from 0.40530 Epoch 15/30 391/391 [==============================] - 129s 330ms/step - loss: 1.6426 - acc: 0.5465 - val_loss: 2.8877 - val_acc: 0.4152 Epoch 00015: val_acc improved from 0.40530 to 0.41520, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 16/30 391/391 [==============================] - 128s 328ms/step - loss: 1.6199 - acc: 0.5533 - val_loss: 3.2430 - val_acc: 0.3968 Epoch 00016: val_acc did not improve from 0.41520 Epoch 17/30 391/391 [==============================] - 122s 311ms/step - loss: 1.6093 - acc: 0.5553 - val_loss: 3.3633 - val_acc: 0.3853 Epoch 00017: val_acc did not improve from 0.41520 Epoch 18/30 391/391 [==============================] - 122s 312ms/step - loss: 1.5891 - acc: 0.5611 - val_loss: 3.5916 - val_acc: 0.3703 Epoch 00018: val_acc did not improve from 0.41520 Epoch 19/30 391/391 [==============================] - 121s 310ms/step - loss: 1.5707 - acc: 0.5665 - val_loss: 2.8044 - val_acc: 0.4265 Epoch 00019: val_acc improved from 0.41520 to 0.42650, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 20/30 391/391 [==============================] - 122s 311ms/step - loss: 1.5550 - acc: 0.5687 - val_loss: 2.9007 - val_acc: 0.4178 Epoch 00020: val_acc did not improve from 0.42650 Epoch 21/30 391/391 [==============================] - 122s 312ms/step - loss: 1.4868 - acc: 0.5854 - val_loss: 3.1454 - val_acc: 0.4037 Epoch 00021: val_acc did not improve from 0.42650 Epoch 22/30 391/391 [==============================] - 121s 310ms/step - loss: 1.5035 - acc: 0.5839 - val_loss: 3.3994 - val_acc: 0.3845 Epoch 00022: val_acc did not improve from 0.42650 Epoch 23/30 391/391 [==============================] - 121s 310ms/step - loss: 1.4860 - acc: 0.5860 - val_loss: 3.1657 - val_acc: 0.4043 Epoch 00023: val_acc did not improve from 0.42650 Epoch 24/30 391/391 [==============================] - 120s 308ms/step - loss: 1.4809 - acc: 0.5875 - val_loss: 3.0020 - val_acc: 0.4094 Epoch 00024: val_acc did not improve from 0.42650 Epoch 25/30 391/391 [==============================] - 121s 309ms/step - loss: 1.4730 - acc: 0.5906 - val_loss: 2.8147 - val_acc: 0.4342 Epoch 00025: val_acc improved from 0.42650 to 0.43420, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 26/30 391/391 [==============================] - 122s 312ms/step - loss: 1.4163 - acc: 0.6054 - val_loss: 2.9704 - val_acc: 0.4182 Epoch 00026: val_acc did not improve from 0.43420 Epoch 27/30 391/391 [==============================] - 121s 309ms/step - loss: 1.4087 - acc: 0.6058 - val_loss: 2.8172 - val_acc: 0.4336 Epoch 00027: val_acc did not improve from 0.43420 Epoch 28/30 391/391 [==============================] - 121s 309ms/step - loss: 1.3998 - acc: 0.6061 - val_loss: 2.9284 - val_acc: 0.4253 Epoch 00028: val_acc did not improve from 0.43420 Epoch 29/30 391/391 [==============================] - 120s 308ms/step - loss: 1.3893 - acc: 0.6134 - val_loss: 2.9667 - val_acc: 0.4245 Epoch 00029: val_acc did not improve from 0.43420 Epoch 30/30 391/391 [==============================] - 121s 309ms/step - loss: 1.3943 - acc: 0.6102 - val_loss: 2.9288 - val_acc: 0.4246 Epoch 00030: val_acc did not improve from 0.43420 . &lt;keras.callbacks.History at 0x7feeefb15898&gt; . After the initial 30 epochs of training the last few layers , now unfreeze all the layers and train again for 100 epochs . for layer in model.layers: layer.trainable=True . opt=SGD(lr=0.01, momentum=0.9, nesterov=True) model.compile(optimizer=opt, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . import math def scheduler1(epoch): if epoch &lt; 15: return 0.01 elif 15 &lt; epoch &lt; 30: return 0.008 else: return 0.008 * math.exp(0.1 * (30 - epoch)) lr_callback = keras.callbacks.LearningRateScheduler(scheduler1) . model.fit_generator(training_generator, epochs=EPOCHS, steps_per_epoch=np.ceil(train_features.shape[0]/batch_size), validation_steps=np.ceil(test_features.shape[0]/batch_size), validation_data=validation_generator, shuffle=True, callbacks=[chkpoint_model,lr_callback], verbose=1) . Epoch 1/100 391/391 [==============================] - 186s 476ms/step - loss: 1.2726 - acc: 0.6361 - val_loss: 1.2386 - val_acc: 0.6603 Epoch 00001: val_acc improved from 0.43420 to 0.66030, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 2/100 391/391 [==============================] - 178s 456ms/step - loss: 0.9419 - acc: 0.7234 - val_loss: 1.0315 - val_acc: 0.7018 Epoch 00002: val_acc improved from 0.66030 to 0.70180, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 3/100 391/391 [==============================] - 178s 456ms/step - loss: 0.8065 - acc: 0.7602 - val_loss: 0.9088 - val_acc: 0.7393 Epoch 00003: val_acc improved from 0.70180 to 0.73930, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 4/100 391/391 [==============================] - 178s 456ms/step - loss: 0.7124 - acc: 0.7846 - val_loss: 0.9617 - val_acc: 0.7312 Epoch 00004: val_acc did not improve from 0.73930 Epoch 5/100 391/391 [==============================] - 178s 455ms/step - loss: 0.6331 - acc: 0.8057 - val_loss: 0.9588 - val_acc: 0.7385 Epoch 00005: val_acc did not improve from 0.73930 Epoch 6/100 391/391 [==============================] - 179s 457ms/step - loss: 0.5726 - acc: 0.8238 - val_loss: 1.0137 - val_acc: 0.7265 Epoch 00006: val_acc did not improve from 0.73930 Epoch 7/100 391/391 [==============================] - 178s 455ms/step - loss: 0.5171 - acc: 0.8410 - val_loss: 0.9424 - val_acc: 0.7432 Epoch 00007: val_acc improved from 0.73930 to 0.74320, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 8/100 391/391 [==============================] - 179s 457ms/step - loss: 0.4767 - acc: 0.8544 - val_loss: 0.8891 - val_acc: 0.7581 Epoch 00008: val_acc improved from 0.74320 to 0.75810, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 9/100 391/391 [==============================] - 178s 455ms/step - loss: 0.4326 - acc: 0.8660 - val_loss: 0.9155 - val_acc: 0.7550 Epoch 00009: val_acc did not improve from 0.75810 Epoch 10/100 391/391 [==============================] - 178s 456ms/step - loss: 0.4012 - acc: 0.8755 - val_loss: 0.9171 - val_acc: 0.7544 Epoch 00010: val_acc did not improve from 0.75810 Epoch 11/100 391/391 [==============================] - 179s 457ms/step - loss: 0.3686 - acc: 0.8856 - val_loss: 0.9026 - val_acc: 0.7599 Epoch 00011: val_acc improved from 0.75810 to 0.75990, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 12/100 391/391 [==============================] - 179s 458ms/step - loss: 0.3393 - acc: 0.8954 - val_loss: 1.0303 - val_acc: 0.7399 Epoch 00012: val_acc did not improve from 0.75990 Epoch 13/100 391/391 [==============================] - 178s 456ms/step - loss: 0.3221 - acc: 0.9011 - val_loss: 0.9879 - val_acc: 0.7550 Epoch 00013: val_acc did not improve from 0.75990 Epoch 14/100 391/391 [==============================] - 179s 457ms/step - loss: 0.2955 - acc: 0.9081 - val_loss: 0.9063 - val_acc: 0.7609 Epoch 00014: val_acc improved from 0.75990 to 0.76090, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 15/100 391/391 [==============================] - 178s 456ms/step - loss: 0.2780 - acc: 0.9140 - val_loss: 0.9361 - val_acc: 0.7626 Epoch 00015: val_acc improved from 0.76090 to 0.76260, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 16/100 391/391 [==============================] - 179s 458ms/step - loss: 1.0478 - acc: 0.6974 - val_loss: 2.0535 - val_acc: 0.5705 Epoch 00016: val_acc did not improve from 0.76260 Epoch 17/100 391/391 [==============================] - 179s 457ms/step - loss: 0.5366 - acc: 0.8374 - val_loss: 0.8038 - val_acc: 0.7734 Epoch 00017: val_acc improved from 0.76260 to 0.77340, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 18/100 391/391 [==============================] - 179s 457ms/step - loss: 0.3469 - acc: 0.8952 - val_loss: 0.7776 - val_acc: 0.7876 Epoch 00018: val_acc improved from 0.77340 to 0.78760, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 19/100 391/391 [==============================] - 179s 457ms/step - loss: 0.2823 - acc: 0.9137 - val_loss: 0.7753 - val_acc: 0.7940 Epoch 00019: val_acc improved from 0.78760 to 0.79400, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 20/100 391/391 [==============================] - 178s 456ms/step - loss: 0.2419 - acc: 0.9262 - val_loss: 0.7797 - val_acc: 0.7936 Epoch 00020: val_acc did not improve from 0.79400 Epoch 21/100 391/391 [==============================] - 178s 456ms/step - loss: 0.2175 - acc: 0.9347 - val_loss: 0.8229 - val_acc: 0.7880 Epoch 00021: val_acc did not improve from 0.79400 Epoch 22/100 391/391 [==============================] - 178s 456ms/step - loss: 0.1976 - acc: 0.9403 - val_loss: 0.8246 - val_acc: 0.7911 Epoch 00022: val_acc did not improve from 0.79400 Epoch 23/100 391/391 [==============================] - 179s 457ms/step - loss: 0.1786 - acc: 0.9477 - val_loss: 0.8362 - val_acc: 0.7902 Epoch 00023: val_acc did not improve from 0.79400 Epoch 24/100 391/391 [==============================] - 178s 456ms/step - loss: 0.1649 - acc: 0.9520 - val_loss: 0.8262 - val_acc: 0.7909 Epoch 00024: val_acc did not improve from 0.79400 Epoch 25/100 391/391 [==============================] - 179s 457ms/step - loss: 0.1562 - acc: 0.9534 - val_loss: 0.8414 - val_acc: 0.7893 Epoch 00025: val_acc did not improve from 0.79400 Epoch 26/100 391/391 [==============================] - 178s 456ms/step - loss: 0.1447 - acc: 0.9576 - val_loss: 0.8416 - val_acc: 0.7896 Epoch 00026: val_acc did not improve from 0.79400 Epoch 27/100 391/391 [==============================] - 178s 456ms/step - loss: 0.1422 - acc: 0.9574 - val_loss: 0.8558 - val_acc: 0.7918 Epoch 00027: val_acc did not improve from 0.79400 Epoch 28/100 391/391 [==============================] - 178s 456ms/step - loss: 0.1323 - acc: 0.9607 - val_loss: 0.8532 - val_acc: 0.7924 Epoch 00028: val_acc did not improve from 0.79400 Epoch 29/100 391/391 [==============================] - 178s 456ms/step - loss: 0.1261 - acc: 0.9628 - val_loss: 0.8479 - val_acc: 0.7946 Epoch 00029: val_acc improved from 0.79400 to 0.79460, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 30/100 391/391 [==============================] - 178s 456ms/step - loss: 0.1132 - acc: 0.9682 - val_loss: 0.8551 - val_acc: 0.7941 Epoch 00030: val_acc did not improve from 0.79460 Epoch 31/100 391/391 [==============================] - 179s 458ms/step - loss: 0.1123 - acc: 0.9670 - val_loss: 0.8399 - val_acc: 0.7972 Epoch 00031: val_acc improved from 0.79460 to 0.79720, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 32/100 391/391 [==============================] - 178s 455ms/step - loss: 0.1035 - acc: 0.9703 - val_loss: 0.8720 - val_acc: 0.7908 Epoch 00032: val_acc did not improve from 0.79720 Epoch 33/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0928 - acc: 0.9738 - val_loss: 0.8403 - val_acc: 0.7952 Epoch 00033: val_acc did not improve from 0.79720 Epoch 34/100 391/391 [==============================] - 177s 454ms/step - loss: 0.0854 - acc: 0.9758 - val_loss: 0.8532 - val_acc: 0.7980 Epoch 00034: val_acc improved from 0.79720 to 0.79800, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 35/100 391/391 [==============================] - 177s 454ms/step - loss: 0.0800 - acc: 0.9777 - val_loss: 0.8207 - val_acc: 0.8023 Epoch 00035: val_acc improved from 0.79800 to 0.80230, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 36/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0731 - acc: 0.9793 - val_loss: 0.8210 - val_acc: 0.8059 Epoch 00036: val_acc improved from 0.80230 to 0.80590, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 37/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0691 - acc: 0.9804 - val_loss: 0.8296 - val_acc: 0.8046 Epoch 00037: val_acc did not improve from 0.80590 Epoch 38/100 391/391 [==============================] - 179s 457ms/step - loss: 0.0637 - acc: 0.9824 - val_loss: 0.8162 - val_acc: 0.8077 Epoch 00038: val_acc improved from 0.80590 to 0.80770, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 39/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0601 - acc: 0.9834 - val_loss: 0.8142 - val_acc: 0.8057 Epoch 00039: val_acc did not improve from 0.80770 Epoch 40/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0557 - acc: 0.9846 - val_loss: 0.8178 - val_acc: 0.8081 Epoch 00040: val_acc improved from 0.80770 to 0.80810, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 41/100 391/391 [==============================] - 179s 457ms/step - loss: 0.0529 - acc: 0.9859 - val_loss: 0.8090 - val_acc: 0.8095 Epoch 00041: val_acc improved from 0.80810 to 0.80950, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 42/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0487 - acc: 0.9872 - val_loss: 0.8091 - val_acc: 0.8043 Epoch 00042: val_acc did not improve from 0.80950 Epoch 43/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0520 - acc: 0.9862 - val_loss: 0.8063 - val_acc: 0.8071 Epoch 00043: val_acc did not improve from 0.80950 Epoch 44/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0459 - acc: 0.9884 - val_loss: 0.8085 - val_acc: 0.8090 Epoch 00044: val_acc did not improve from 0.80950 Epoch 45/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0463 - acc: 0.9879 - val_loss: 0.8072 - val_acc: 0.8082 Epoch 00045: val_acc did not improve from 0.80950 Epoch 46/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0439 - acc: 0.9880 - val_loss: 0.8002 - val_acc: 0.8131 Epoch 00046: val_acc improved from 0.80950 to 0.81310, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 47/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0416 - acc: 0.9891 - val_loss: 0.7974 - val_acc: 0.8099 Epoch 00047: val_acc did not improve from 0.81310 Epoch 48/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0422 - acc: 0.9888 - val_loss: 0.8042 - val_acc: 0.8090 Epoch 00048: val_acc did not improve from 0.81310 Epoch 49/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0381 - acc: 0.9901 - val_loss: 0.7994 - val_acc: 0.8109 Epoch 00049: val_acc did not improve from 0.81310 Epoch 50/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0410 - acc: 0.9899 - val_loss: 0.8007 - val_acc: 0.8108 Epoch 00050: val_acc did not improve from 0.81310 Epoch 51/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0404 - acc: 0.9896 - val_loss: 0.7964 - val_acc: 0.8119 Epoch 00051: val_acc did not improve from 0.81310 Epoch 52/100 391/391 [==============================] - 179s 457ms/step - loss: 0.0391 - acc: 0.9899 - val_loss: 0.8002 - val_acc: 0.8110 Epoch 00052: val_acc did not improve from 0.81310 Epoch 53/100 391/391 [==============================] - 179s 457ms/step - loss: 0.0389 - acc: 0.9898 - val_loss: 0.7957 - val_acc: 0.8115 Epoch 00053: val_acc did not improve from 0.81310 Epoch 54/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0381 - acc: 0.9907 - val_loss: 0.7948 - val_acc: 0.8119 Epoch 00054: val_acc did not improve from 0.81310 Epoch 55/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0374 - acc: 0.9907 - val_loss: 0.7925 - val_acc: 0.8108 Epoch 00055: val_acc did not improve from 0.81310 Epoch 56/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0358 - acc: 0.9916 - val_loss: 0.7959 - val_acc: 0.8118 Epoch 00056: val_acc did not improve from 0.81310 Epoch 57/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0373 - acc: 0.9906 - val_loss: 0.7964 - val_acc: 0.8115 Epoch 00057: val_acc did not improve from 0.81310 Epoch 58/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0355 - acc: 0.9908 - val_loss: 0.7961 - val_acc: 0.8110 Epoch 00058: val_acc did not improve from 0.81310 Epoch 59/100 391/391 [==============================] - 177s 453ms/step - loss: 0.0354 - acc: 0.9912 - val_loss: 0.7963 - val_acc: 0.8110 Epoch 00059: val_acc did not improve from 0.81310 Epoch 60/100 391/391 [==============================] - 177s 453ms/step - loss: 0.0374 - acc: 0.9904 - val_loss: 0.7965 - val_acc: 0.8107 Epoch 00060: val_acc did not improve from 0.81310 Epoch 61/100 391/391 [==============================] - 177s 453ms/step - loss: 0.0347 - acc: 0.9911 - val_loss: 0.7953 - val_acc: 0.8102 Epoch 00061: val_acc did not improve from 0.81310 Epoch 62/100 391/391 [==============================] - 177s 453ms/step - loss: 0.0352 - acc: 0.9910 - val_loss: 0.7961 - val_acc: 0.8093 Epoch 00062: val_acc did not improve from 0.81310 Epoch 63/100 391/391 [==============================] - 178s 454ms/step - loss: 0.0333 - acc: 0.9921 - val_loss: 0.7956 - val_acc: 0.8109 Epoch 00063: val_acc did not improve from 0.81310 Epoch 64/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0356 - acc: 0.9910 - val_loss: 0.7947 - val_acc: 0.8099 Epoch 00064: val_acc did not improve from 0.81310 Epoch 65/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0353 - acc: 0.9909 - val_loss: 0.7954 - val_acc: 0.8104 Epoch 00065: val_acc did not improve from 0.81310 Epoch 66/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0353 - acc: 0.9912 - val_loss: 0.7957 - val_acc: 0.8120 Epoch 00066: val_acc did not improve from 0.81310 Epoch 67/100 391/391 [==============================] - 179s 458ms/step - loss: 0.0337 - acc: 0.9920 - val_loss: 0.7941 - val_acc: 0.8112 Epoch 00067: val_acc did not improve from 0.81310 Epoch 68/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0336 - acc: 0.9913 - val_loss: 0.7948 - val_acc: 0.8115 Epoch 00068: val_acc did not improve from 0.81310 Epoch 69/100 391/391 [==============================] - 179s 457ms/step - loss: 0.0345 - acc: 0.9912 - val_loss: 0.7934 - val_acc: 0.8121 Epoch 00069: val_acc did not improve from 0.81310 Epoch 70/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0362 - acc: 0.9907 - val_loss: 0.7947 - val_acc: 0.8124 Epoch 00070: val_acc did not improve from 0.81310 Epoch 71/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0343 - acc: 0.9911 - val_loss: 0.7944 - val_acc: 0.8111 Epoch 00071: val_acc did not improve from 0.81310 Epoch 72/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0348 - acc: 0.9916 - val_loss: 0.7935 - val_acc: 0.8124 Epoch 00072: val_acc did not improve from 0.81310 Epoch 73/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0341 - acc: 0.9913 - val_loss: 0.7945 - val_acc: 0.8117 Epoch 00073: val_acc did not improve from 0.81310 Epoch 74/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0322 - acc: 0.9924 - val_loss: 0.7942 - val_acc: 0.8110 Epoch 00074: val_acc did not improve from 0.81310 Epoch 75/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0341 - acc: 0.9915 - val_loss: 0.7944 - val_acc: 0.8121 Epoch 00075: val_acc did not improve from 0.81310 Epoch 76/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0341 - acc: 0.9915 - val_loss: 0.7950 - val_acc: 0.8117 Epoch 00076: val_acc did not improve from 0.81310 Epoch 77/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0333 - acc: 0.9913 - val_loss: 0.7945 - val_acc: 0.8117 Epoch 00077: val_acc did not improve from 0.81310 Epoch 78/100 391/391 [==============================] - 179s 458ms/step - loss: 0.0346 - acc: 0.9914 - val_loss: 0.7945 - val_acc: 0.8115 Epoch 00078: val_acc did not improve from 0.81310 Epoch 79/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0332 - acc: 0.9916 - val_loss: 0.7950 - val_acc: 0.8112 Epoch 00079: val_acc did not improve from 0.81310 Epoch 80/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0326 - acc: 0.9923 - val_loss: 0.7945 - val_acc: 0.8120 Epoch 00080: val_acc did not improve from 0.81310 Epoch 81/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0310 - acc: 0.9927 - val_loss: 0.7949 - val_acc: 0.8116 Epoch 00081: val_acc did not improve from 0.81310 Epoch 82/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0329 - acc: 0.9918 - val_loss: 0.7958 - val_acc: 0.8121 Epoch 00082: val_acc did not improve from 0.81310 Epoch 83/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0332 - acc: 0.9914 - val_loss: 0.7946 - val_acc: 0.8115 Epoch 00083: val_acc did not improve from 0.81310 Epoch 84/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0336 - acc: 0.9918 - val_loss: 0.7943 - val_acc: 0.8119 Epoch 00084: val_acc did not improve from 0.81310 Epoch 85/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0334 - acc: 0.9915 - val_loss: 0.7951 - val_acc: 0.8117 Epoch 00085: val_acc did not improve from 0.81310 Epoch 86/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0330 - acc: 0.9918 - val_loss: 0.7950 - val_acc: 0.8121 Epoch 00086: val_acc did not improve from 0.81310 Epoch 87/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0330 - acc: 0.9916 - val_loss: 0.7951 - val_acc: 0.8113 Epoch 00087: val_acc did not improve from 0.81310 Epoch 88/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0326 - acc: 0.9920 - val_loss: 0.7945 - val_acc: 0.8122 Epoch 00088: val_acc did not improve from 0.81310 Epoch 89/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0321 - acc: 0.9920 - val_loss: 0.7942 - val_acc: 0.8114 Epoch 00089: val_acc did not improve from 0.81310 Epoch 90/100 391/391 [==============================] - 177s 453ms/step - loss: 0.0327 - acc: 0.9922 - val_loss: 0.7952 - val_acc: 0.8114 Epoch 00090: val_acc did not improve from 0.81310 Epoch 91/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0325 - acc: 0.9919 - val_loss: 0.7947 - val_acc: 0.8105 Epoch 00091: val_acc did not improve from 0.81310 Epoch 92/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0329 - acc: 0.9921 - val_loss: 0.7953 - val_acc: 0.8117 Epoch 00092: val_acc did not improve from 0.81310 Epoch 93/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0329 - acc: 0.9918 - val_loss: 0.7949 - val_acc: 0.8109 Epoch 00093: val_acc did not improve from 0.81310 Epoch 94/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0333 - acc: 0.9916 - val_loss: 0.7943 - val_acc: 0.8111 Epoch 00094: val_acc did not improve from 0.81310 Epoch 95/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0316 - acc: 0.9924 - val_loss: 0.7950 - val_acc: 0.8117 Epoch 00095: val_acc did not improve from 0.81310 Epoch 96/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0326 - acc: 0.9926 - val_loss: 0.7947 - val_acc: 0.8113 Epoch 00096: val_acc did not improve from 0.81310 Epoch 97/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0333 - acc: 0.9919 - val_loss: 0.7950 - val_acc: 0.8118 Epoch 00097: val_acc did not improve from 0.81310 Epoch 98/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0326 - acc: 0.9921 - val_loss: 0.7954 - val_acc: 0.8113 Epoch 00098: val_acc did not improve from 0.81310 Epoch 99/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0332 - acc: 0.9920 - val_loss: 0.7947 - val_acc: 0.8107 Epoch 00099: val_acc did not improve from 0.81310 Epoch 100/100 391/391 [==============================] - 179s 457ms/step - loss: 0.0330 - acc: 0.9921 - val_loss: 0.7945 - val_acc: 0.8110 Epoch 00100: val_acc did not improve from 0.81310 . &lt;keras.callbacks.History at 0x7feeee498358&gt; . Val accuracy reached 80.23 at the end of 35th epoch and 81.31 at the end of 100 epochs .We have aleady reached our target of 80% val accuracy . Let us train another 100 epochs to see how much further we can push this validation accuracy . def scheduler2(epoch): if epoch &lt; 15: return 0.002 elif 15 &lt; epoch &lt; 30: return 0.001 elif 13 &lt; epoch &lt; 50: return 0.0005 else: return 0.0005 * math.exp(0.5 * (50 - epoch)) lr_callback = keras.callbacks.LearningRateScheduler(scheduler2) . opt=SGD(lr=0.002, momentum=0.9, nesterov=True) model.compile(optimizer=opt, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . train_datagen=ImageDataGenerator( #preprocessing_function=img_aug2, horizontal_flip=True,width_shift_range=0.05, height_shift_range=0.05 ) val_datagen= ImageDataGenerator( ) training_generator = train_datagen.flow_from_dataframe(train_df, directory=&#39;./data/train/&#39;, x_col=&#39;File&#39;, y_col=&#39;Class&#39;, target_size=(224, 224), color_mode=&#39;rgb&#39;, interpolation=&#39;bicubic&#39;, class_mode=&#39;categorical&#39;, batch_size=batch_size, shuffle=True, seed=42) validation_generator = val_datagen.flow_from_dataframe(test_df, directory=&#39;./data/test/&#39;, x_col=&#39;File&#39;, y_col=&#39;Class&#39;, target_size=(224, 224),interpolation=&#39;bicubic&#39;, color_mode=&#39;rgb&#39;, class_mode=&#39;categorical&#39;, batch_size=batch_size, shuffle=True, seed=42) . Found 50000 validated image filenames belonging to 100 classes. Found 10000 validated image filenames belonging to 100 classes. . model.fit_generator(training_generator, epochs=EPOCHS, steps_per_epoch=np.ceil(train_features.shape[0]/batch_size), validation_steps=np.ceil(test_features.shape[0]/batch_size), validation_data=validation_generator, shuffle=True, callbacks=[chkpoint_model,lr_callback], verbose=1) . Epoch 1/100 391/391 [==============================] - 572s 1s/step - loss: 0.0033 - acc: 0.9996 - val_loss: 0.7933 - val_acc: 0.8118 Epoch 00001: val_acc did not improve from 0.81310 Epoch 2/100 391/391 [==============================] - 560s 1s/step - loss: 0.0027 - acc: 0.9997 - val_loss: 0.7897 - val_acc: 0.8113 Epoch 00002: val_acc did not improve from 0.81310 Epoch 3/100 391/391 [==============================] - 563s 1s/step - loss: 0.0022 - acc: 0.9997 - val_loss: 0.7869 - val_acc: 0.8114 Epoch 00003: val_acc did not improve from 0.81310 Epoch 4/100 391/391 [==============================] - 557s 1s/step - loss: 0.0022 - acc: 0.9997 - val_loss: 0.7868 - val_acc: 0.8123 Epoch 00004: val_acc did not improve from 0.81310 Epoch 5/100 391/391 [==============================] - 558s 1s/step - loss: 0.0020 - acc: 0.9998 - val_loss: 0.7862 - val_acc: 0.8109 Epoch 00005: val_acc did not improve from 0.81310 Epoch 6/100 391/391 [==============================] - 559s 1s/step - loss: 0.0020 - acc: 0.9998 - val_loss: 0.7859 - val_acc: 0.8126 Epoch 00006: val_acc did not improve from 0.81310 Epoch 7/100 391/391 [==============================] - 556s 1s/step - loss: 0.0020 - acc: 0.9998 - val_loss: 0.7855 - val_acc: 0.8132 Epoch 00007: val_acc improved from 0.81310 to 0.81320, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 8/100 391/391 [==============================] - 556s 1s/step - loss: 0.0019 - acc: 0.9998 - val_loss: 0.7852 - val_acc: 0.8131 Epoch 00008: val_acc did not improve from 0.81320 Epoch 9/100 391/391 [==============================] - 557s 1s/step - loss: 0.0019 - acc: 0.9997 - val_loss: 0.7850 - val_acc: 0.8131 Epoch 00009: val_acc did not improve from 0.81320 Epoch 10/100 391/391 [==============================] - 556s 1s/step - loss: 0.0018 - acc: 0.9998 - val_loss: 0.7852 - val_acc: 0.8137 Epoch 00010: val_acc improved from 0.81320 to 0.81370, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 11/100 391/391 [==============================] - 554s 1s/step - loss: 0.0019 - acc: 0.9997 - val_loss: 0.7851 - val_acc: 0.8135 Epoch 00011: val_acc did not improve from 0.81370 Epoch 12/100 391/391 [==============================] - 559s 1s/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.7853 - val_acc: 0.8138 Epoch 00012: val_acc improved from 0.81370 to 0.81380, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 13/100 391/391 [==============================] - 560s 1s/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.7848 - val_acc: 0.8145 Epoch 00013: val_acc improved from 0.81380 to 0.81450, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 14/100 391/391 [==============================] - 559s 1s/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.7845 - val_acc: 0.8145 Epoch 00014: val_acc did not improve from 0.81450 Epoch 15/100 391/391 [==============================] - 562s 1s/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.7855 - val_acc: 0.8138 Epoch 00015: val_acc did not improve from 0.81450 Epoch 16/100 391/391 [==============================] - 564s 1s/step - loss: 0.0016 - acc: 0.9998 - val_loss: 0.7857 - val_acc: 0.8139 Epoch 00016: val_acc did not improve from 0.81450 Epoch 17/100 391/391 [==============================] - 563s 1s/step - loss: 0.0016 - acc: 0.9998 - val_loss: 0.7855 - val_acc: 0.8147 Epoch 00017: val_acc improved from 0.81450 to 0.81470, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 18/100 391/391 [==============================] - 570s 1s/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.7842 - val_acc: 0.8146 Epoch 00018: val_acc did not improve from 0.81470 Epoch 19/100 391/391 [==============================] - 569s 1s/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.7848 - val_acc: 0.8147 Epoch 00019: val_acc did not improve from 0.81470 Epoch 20/100 391/391 [==============================] - 565s 1s/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.7859 - val_acc: 0.8149 Epoch 00020: val_acc improved from 0.81470 to 0.81490, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 21/100 391/391 [==============================] - 567s 1s/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.7853 - val_acc: 0.8148 Epoch 00021: val_acc did not improve from 0.81490 Epoch 22/100 391/391 [==============================] - 568s 1s/step - loss: 0.0014 - acc: 0.9999 - val_loss: 0.7847 - val_acc: 0.8144 Epoch 00022: val_acc did not improve from 0.81490 Epoch 23/100 391/391 [==============================] - 561s 1s/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.7857 - val_acc: 0.8144 Epoch 00023: val_acc did not improve from 0.81490 Epoch 24/100 391/391 [==============================] - 561s 1s/step - loss: 0.0015 - acc: 0.9997 - val_loss: 0.7852 - val_acc: 0.8145 Epoch 00024: val_acc did not improve from 0.81490 Epoch 25/100 391/391 [==============================] - 567s 1s/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.7851 - val_acc: 0.8148 Epoch 00025: val_acc did not improve from 0.81490 Epoch 26/100 391/391 [==============================] - 575s 1s/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.7857 - val_acc: 0.8144 Epoch 00026: val_acc did not improve from 0.81490 Epoch 27/100 391/391 [==============================] - 565s 1s/step - loss: 0.0015 - acc: 0.9997 - val_loss: 0.7848 - val_acc: 0.8152 Epoch 00027: val_acc improved from 0.81490 to 0.81520, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 28/100 371/391 [===========================&gt;..] - ETA: 28s - loss: 0.0015 - acc: 0.9997 . Runtime disconnected after 27 epochs . Val accuracy has reached 81.52 . We will stop here although we could load the model again and train for more epochs to see how much farther we could go. . Load the model saved best model from google drive . model= keras.models.load_model(&#39;/gdrive/My Drive/EVA/session20/best_model2.h5&#39;) . Evaluate and print validation loss and validation accuracy . score=model.evaluate_generator(validation_generator) . print(&#39;validation loss =&#39;,score[0] , &#39;, Validation accuracy =&#39;,score[1]) . validation loss = 0.7847665718078614 , Validation accuracy = 0.8152 . We used a pre-trained a ResNet34 model to classify images in the CIFAR100 dataset. We aded our own prediction layer on top of the base model and trained it to achieve 81.52 max validation accuracy .",
            "url": "https://ravindrabharathi.github.io/blog/resnet/cifar-100/image%20classification/cnn/transfer%20learning/2020/03/01/cifar-100.html",
            "relUrl": "/resnet/cifar-100/image%20classification/cnn/transfer%20learning/2020/03/01/cifar-100.html",
            "date": " • Mar 1, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Spam Classifier",
            "content": "We will use the files from &#39;full&#39; folder in this dataset. . | We will use Pandas to read the content and categorize them into spam:1 and ham:0 . | We will clean up the content by removing line endings , tabs , return characters . We will also remove email addresses, numbers and punctuations from the sentences and convert the text to lowercase. . | We will use NLTK library to remove stopwords like &#39;the&#39; &#39;had&#39; , etc . | We will will use NLTK for stemming which is to convert various forms of a root word to the root word itself e.g &#39;like&#39; is the root word for &#39;liked&#39; , &#39;likes&#39; , &#39;liking&#39; , etc . We will use NLTK SnowballStemmer which handles languages other than English. When handling English language alone Porter stemmer could be used. . | We will use TfidfVectorizer from Scikit Learn library to vectorize and create the train, validation samples . | We will train and test spam / ham classification using Support Vector Machines and NaiveBayes . | import Pandas and other modules for reading data . import pandas as pd import os from pathlib import Path . read email data . data=pd.read_csv(&quot;./full/index&quot;,sep=&#39; &#39;,header=None) data.head() . 0 1 . 0 spam | ../data/inmail.1 | . 1 ham | ../data/inmail.2 | . 2 spam | ../data/inmail.3 | . 3 spam | ../data/inmail.4 | . 4 spam | ../data/inmail.5 | . create class, filepath and contents columns . data.columns=[&#39;class&#39;,&#39;filepath&#39;] . data[&#39;contents&#39;]=None data.head() . class filepath contents . 0 spam | ../data/inmail.1 | None | . 1 ham | ../data/inmail.2 | None | . 2 spam | ../data/inmail.3 | None | . 3 spam | ../data/inmail.4 | None | . 4 spam | ../data/inmail.5 | None | . polulate the contents column with email text . import re import string for i,row in data.iterrows(): filepath=os.path.join(os.getcwd(),row[&#39;filepath&#39;].replace(&#39;../&#39;,&#39;&#39;)) with open(filepath, &#39;rb&#39;) as f: email_txt = f.read() email_text=str(email_txt) if i&lt;2: print(email_txt) data.at[i,&#39;contents&#39;]= email_txt print(data.head()) print(data.info()) . b&#39;From RickyAmes@aol.com Sun Apr 8 13:07:32 2007 nReturn-Path: &lt;RickyAmes@aol.com&gt; nReceived: from 129.97.78.23 ([211.202.101.74]) n tby speedy.uwaterloo.ca (8.12.8/8.12.5) with SMTP id l38H7G0I003017; n tSun, 8 Apr 2007 13:07:21 -0400 nReceived: from 0.144.152.6 by 211.202.101.74; Sun, 08 Apr 2007 19:04:48 +0100 nMessage-ID: &lt;WYADCKPDFWWTWTXNFVUE@yahoo.com&gt; nFrom: &#34;Tomas Jacobs&#34; &lt;RickyAmes@aol.com&gt; nReply-To: &#34;Tomas Jacobs&#34; &lt;RickyAmes@aol.com&gt; nTo: the00@speedy.uwaterloo.ca nSubject: Generic Cialis, branded quality@ nDate: Sun, 08 Apr 2007 21:00:48 +0300 nX-Mailer: Microsoft Outlook Express 6.00.2600.0000 nMIME-Version: 1.0 nContent-Type: multipart/alternative; n tboundary=&#34;--8896484051606557286&#34; nX-Priority: 3 nX-MSMail-Priority: Normal nStatus: RO nContent-Length: 988 nLines: 24 n n-8896484051606557286 nContent-Type: text/html; nContent-Transfer-Encoding: 7Bit n n&lt;html&gt; n&lt;body bgcolor=&#34;#ffffff&#34;&gt; n&lt;div style=&#34;border-color: #00FFFF; border-right-width: 0px; border-bottom-width: 0px; margin-bottom: 0px;&#34; align=&#34;center&#34;&gt; n&lt;table style=&#34;border: 1px; border-style: solid; border-color:#000000;&#34; cellpadding=&#34;5&#34; cellspacing=&#34;0&#34; bgcolor=&#34;#CCFFAA&#34;&gt; n&lt;tr&gt; n&lt;td style=&#34;border: 0px; border-bottom: 1px; border-style: solid; border-color:#000000;&#34;&gt; n&lt;center&gt; nDo you feel the pressure to perform and not rising to the occasion??&lt;br&gt; n&lt;/center&gt; n&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; n&lt;td bgcolor=#FFFF33 style=&#34;border: 0px; border-bottom: 1px; border-style: solid; border-color:#000000;&#34;&gt; n&lt;center&gt; n n&lt;b&gt;&lt;a href= &#39;http://excoriationtuh.com/?lzmfnrdkleks &#39;&gt;Try &lt;span&gt;V&lt;/span&gt;&lt;span&gt;ia&lt;span&gt;&lt;/span&gt;gr&lt;span&gt;a&lt;/span&gt;.....&lt;/a&gt;&lt;/b&gt;&lt;/center&gt; n&lt;/td&gt;&lt;/tr&gt;&lt;td&gt;&lt;center&gt;your anxiety will be a thing of the past and you will&lt;br&gt; nbe back to your old self. n&lt;/center&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; n n n-8896484051606557286-- n n&#39; b&#39;From bounce-debian-mirrors=ktwarwic=speedy.uwaterloo.ca@lists.debian.org Sun Apr 8 13:09:29 2007 nReturn-Path: &lt;bounce-debian-mirrors=ktwarwic=speedy.uwaterloo.ca@lists.debian.org&gt; nReceived: from murphy.debian.org (murphy.debian.org [70.103.162.31]) n tby speedy.uwaterloo.ca (8.12.8/8.12.5) with ESMTP id l38H9S0I003031 n tfor &lt;ktwarwic@speedy.uwaterloo.ca&gt;; Sun, 8 Apr 2007 13:09:28 -0400 nReceived: from localhost (localhost [127.0.0.1]) n tby murphy.debian.org (Postfix) with QMQP n tid 90C152E68E; Sun, 8 Apr 2007 12:09:05 -0500 (CDT) nOld-Return-Path: &lt;yan.morin@savoirfairelinux.com&gt; nX-Spam-Checker-Version: SpamAssassin 3.1.4 (2006-07-26) on murphy.debian.org nX-Spam-Level: nX-Spam-Status: No, score=-1.1 required=4.0 tests=BAYES_05 autolearn=no n tversion=3.1.4 nX-Original-To: debian-mirrors@lists.debian.org nReceived: from xenon.savoirfairelinux.net (savoirfairelinux.net [199.243.85.90]) n tby murphy.debian.org (Postfix) with ESMTP id 827432E3E5 n tfor &lt;debian-mirrors@lists.debian.org&gt;; Sun, 8 Apr 2007 11:52:35 -0500 (CDT) nReceived: from [192.168.0.101] (bas6-montreal28-1177925679.dsl.bell.ca [70.53.184.47]) n tby xenon.savoirfairelinux.net (Postfix) with ESMTP id C1223F69B7 n tfor &lt;debian-mirrors@lists.debian.org&gt;; Sun, 8 Apr 2007 12:52:34 -0400 (EDT) nMessage-ID: &lt;46191DCE.3020508@savoirfairelinux.com&gt; nDate: Sun, 08 Apr 2007 12:52:30 -0400 nFrom: Yan Morin &lt;yan.morin@savoirfairelinux.com&gt; nUser-Agent: Icedove 1.5.0.10 (X11/20070329) nMIME-Version: 1.0 nTo: debian-mirrors@lists.debian.org nSubject: Typo in /debian/README nX-Enigmail-Version: 0.94.2.0 nContent-Type: text/plain; charset=ISO-8859-1 nContent-Transfer-Encoding: 7bit nX-Rc-Spam: 2007-01-18_01 nX-Rc-Virus: 2006-10-25_01 nX-Rc-Spam: 2007-01-18_01 nResent-Message-ID: &lt;tHOiyB.A.jEC.xGSGGB@murphy&gt; nResent-From: debian-mirrors@lists.debian.org nX-Mailing-List: &lt;debian-mirrors@lists.debian.org&gt; nX-Loop: debian-mirrors@lists.debian.org nList-Id: &lt;debian-mirrors.lists.debian.org&gt; nList-Post: &lt;mailto:debian-mirrors@lists.debian.org&gt; nList-Help: &lt;mailto:debian-mirrors-request@lists.debian.org?subject=help&gt; nList-Subscribe: &lt;mailto:debian-mirrors-request@lists.debian.org?subject=subscribe&gt; nList-Unsubscribe: &lt;mailto:debian-mirrors-request@lists.debian.org?subject=unsubscribe&gt; nPrecedence: list nResent-Sender: debian-mirrors-request@lists.debian.org nResent-Date: Sun, 8 Apr 2007 12:09:05 -0500 (CDT) nStatus: RO nContent-Length: 729 nLines: 26 n nHi, i &#39;ve just updated from the gulus and I check on other mirrors. nIt seems there is a little typo in /debian/README file n nExample: nhttp://gulus.usherbrooke.ca/debian/README nftp://ftp.fr.debian.org/debian/README n n&#34;Testing, or lenny. Access this release through dists/testing. The ncurrent tested development snapshot is named etch. Packages which nhave been tested in unstable and passed automated tests propogate to nthis release.&#34; n netch should be replace by lenny like in the README.html n n n n-- nYan Morin nConsultant en logiciel libre nyan.morin@savoirfairelinux.com n514-994-1556 n n n-- nTo UNSUBSCRIBE, email to debian-mirrors-REQUEST@lists.debian.org nwith a subject of &#34;unsubscribe&#34;. Trouble? Contact listmaster@lists.debian.org n n&#39; class filepath contents 0 spam ../data/inmail.1 b&#39;From RickyAmes@aol.com Sun Apr 8 13:07:32 ... 1 ham ../data/inmail.2 b&#39;From bounce-debian-mirrors=ktwarwic=speedy.u... 2 spam ../data/inmail.3 b&#39;From 7stocknews@tractionmarketing.com Sun A... 3 spam ../data/inmail.4 b&#39;From vqucsmdfgvsg@ruraltek.com Sun Apr 8 1... 4 spam ../data/inmail.5 b&#39;From dcube@totalink.net Sun Apr 8 13:19:30... &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 75419 entries, 0 to 75418 Data columns (total 3 columns): class 75419 non-null object filepath 75419 non-null object contents 75419 non-null object dtypes: object(3) memory usage: 1.7+ MB None . categorize class as spam:1 and ham:0 . category={&#39;spam&#39;:1,&#39;ham&#39;:0} data[&#39;class&#39;]=[category[item] for item in data[&#39;class&#39;] ] data.head() . class filepath contents . 0 1 | ../data/inmail.1 | b&#39;From RickyAmes@aol.com Sun Apr 8 13:07:32 ... | . 1 0 | ../data/inmail.2 | b&#39;From bounce-debian-mirrors=ktwarwic=speedy.u... | . 2 1 | ../data/inmail.3 | b&#39;From 7stocknews@tractionmarketing.com Sun A... | . 3 1 | ../data/inmail.4 | b&#39;From vqucsmdfgvsg@ruraltek.com Sun Apr 8 1... | . 4 1 | ../data/inmail.5 | b&#39;From dcube@totalink.net Sun Apr 8 13:19:30... | . create train, test data from the full set . X_train=(data[&#39;contents&#39;][0:1999]).copy() . y_train=data[&#39;class&#39;][0:1999] . X_test=(data[&#39;contents&#39;][2000:2500]).copy() . y_test=data[&#39;class&#39;][2000:2500] . import nltk stowords . import nltk from nltk.corpus import stopwords nltk.download (&#39;stopwords&#39;) nltk.download (&#39;punkt&#39;) stop_words=set(stopwords.words(&quot;english&quot;)) . [nltk_data] Downloading package stopwords to [nltk_data] /Users/ravindra/nltk_data... [nltk_data] Package stopwords is already up-to-date! [nltk_data] Downloading package punkt to /Users/ravindra/nltk_data... [nltk_data] Package punkt is already up-to-date! . import nltk tokenizers, stemmers and add function definitions for processing the email text . from nltk.tokenize import word_tokenize from nltk.stem import PorterStemmer,SnowballStemmer def cleanText(email_txt): email_txt=str(email_txt).replace(&#39; n&#39;, &#39; &#39;).replace(&#39; r&#39;, &#39; &#39;).replace(&#39; t&#39;,&#39; &#39;) #print(email_txt) clean1 = re.compile(&#39;&lt;.*?&gt;&#39;) email_txt=re.sub(clean1, &#39;&#39;, str(email_txt)).lower() clean2=re.compile(&#39; S*@ S* s?&#39;) email_txt=re.sub(clean2,&#39;emailAddress&#39;,email_txt) email_txt=email_txt.translate(str.maketrans(&#39;&#39;,&#39;&#39;,string.punctuation)) email_txt=email_txt.translate(str.maketrans(&#39;&#39;,&#39;&#39;,&#39;1234567890&#39;)) return str(email_txt) def tokenizeText(text): return word_tokenize(text) def removeStopWords(text): result=[] for word in text: if word not in stop_words: result.append(word) return result def performStemming(text): result=&#39;&#39; stemr=SnowballStemmer(&#39;english&#39;) for word in text: result +=(stemr.stem(word))+&#39; &#39; return result def preprocessText(text): text0=cleanText(text) text1=tokenizeText(text0) text2=removeStopWords(text1) text3=performStemming(text2) #print(text3) return text3 . apply preprocessing on train , test set . X_train = X_train.apply(preprocessText) . X_test=X_test.apply(preprocessText) . use TfidVectorizer for feature extraction /vectorization and create train/validation set using train_test_split module . from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split vectorizer = TfidfVectorizer(&quot;english&quot;) features = vectorizer.fit_transform(X_train) features_train, features_test, labels_train, labels_test = train_test_split(features, y_train, test_size=0.3, random_state=42) . use SVM for training and then test spam / ham classification on validation set . from sklearn.metrics import accuracy_score from sklearn.svm import SVC svc = SVC(kernel=&#39;sigmoid&#39;, gamma=1.0) svc.fit(features_train, labels_train) prediction = svc.predict(features_test) accuracy_score(labels_test,prediction) . 0.9916666666666667 . use NaiveBayes to train and then test spam / ham classification on validation set . from sklearn.naive_bayes import MultinomialNB mnb = MultinomialNB(alpha=0.2) mnb.fit(features_train, labels_train) prediction = mnb.predict(features_test) accuracy_score(labels_test,prediction) . 0.9716666666666667 . Get the prediction of NaiveBayes classifier on the test set . features3 = vectorizer.transform(X_test) print(features3.shape) prediction = mnb.predict(features3) . (500, 82430) . print accuracy score by comparing predictions vs True values . accuracy_score(y_test,prediction) . 0.986 .",
            "url": "https://ravindrabharathi.github.io/blog/nlp/spam%20classification/svm/naivebayes/2020/03/01/Spam-classifier.html",
            "relUrl": "/nlp/spam%20classification/svm/naivebayes/2020/03/01/Spam-classifier.html",
            "date": " • Mar 1, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "LR Finder",
            "content": "We will use CIFAR-10 dataset for this exercise and build the model using Keras .. . import necessary modules . from keras import backend as K import time import matplotlib.pyplot as plt import numpy as np % matplotlib inline np.random.seed(2017) from keras import regularizers from keras.models import Sequential from keras.layers.convolutional import Convolution2D, MaxPooling2D,AveragePooling2D from keras.layers import Activation, Flatten, Dense, Dropout from keras.layers.normalization import BatchNormalization from keras.utils import np_utils from keras.preprocessing.image import ImageDataGenerator . Using TensorFlow backend. . get CIFAR10 dataset and set the train and test data . from keras.datasets import cifar10 (train_features, train_labels), (test_features, test_labels) = cifar10.load_data() num_train, img_rows, img_cols,img_channels = train_features.shape num_test, _, _, _ = test_features.shape num_classes = len(np.unique(train_labels)) . Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz 170500096/170498071 [==============================] - 9s 0us/step . print (num_classes) print (num_train) print (train_features.shape) . 10 50000 (50000, 32, 32, 3) . inspect some of the images from the dataset by printing . class_names = [&#39;airplane&#39;,&#39;automobile&#39;,&#39;bird&#39;,&#39;cat&#39;,&#39;deer&#39;, &#39;dog&#39;,&#39;frog&#39;,&#39;horse&#39;,&#39;ship&#39;,&#39;truck&#39;] fig = plt.figure(figsize=(8,3)) for i in range(num_classes): ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[]) idx = np.where(train_labels[:]==i)[0] features_idx = train_features[idx,::] img_num = np.random.randint(features_idx.shape[0]) im = features_idx[img_num] ax.set_title(class_names[i]) plt.imshow(im) plt.show() . function for plotting accuracy vs number of epochs . def plot_model_history(model_history): fig, axs = plt.subplots(1,2,figsize=(15,5)) # summarize history for accuracy axs[0].plot(range(1,len(model_history.history[&#39;acc&#39;])+1),model_history.history[&#39;acc&#39;]) axs[0].plot(range(1,len(model_history.history[&#39;val_acc&#39;])+1),model_history.history[&#39;val_acc&#39;]) axs[0].set_title(&#39;Model Accuracy&#39;) axs[0].set_ylabel(&#39;Accuracy&#39;) axs[0].set_xlabel(&#39;Epoch&#39;) axs[0].set_xticks(np.arange(1,len(model_history.history[&#39;acc&#39;])+1),len(model_history.history[&#39;acc&#39;])/10) axs[0].legend([&#39;train&#39;, &#39;val&#39;], loc=&#39;best&#39;) # summarize history for loss axs[1].plot(range(1,len(model_history.history[&#39;loss&#39;])+1),model_history.history[&#39;loss&#39;]) axs[1].plot(range(1,len(model_history.history[&#39;val_loss&#39;])+1),model_history.history[&#39;val_loss&#39;]) axs[1].set_title(&#39;Model Loss&#39;) axs[1].set_ylabel(&#39;Loss&#39;) axs[1].set_xlabel(&#39;Epoch&#39;) axs[1].set_xticks(np.arange(1,len(model_history.history[&#39;loss&#39;])+1),len(model_history.history[&#39;loss&#39;])/10) axs[1].legend([&#39;train&#39;, &#39;val&#39;], loc=&#39;best&#39;) plt.show() . function to calculate accuracy on test data . def accuracy(test_x, test_y, model): result = model.predict(test_x) predicted_class = np.argmax(result, axis=1) true_class = np.argmax(test_y, axis=1) num_correct = np.sum(predicted_class == true_class) accuracy = float(num_correct)/result.shape[0] return (accuracy * 100) . function to get max training accuracy from model history . def get_max_train_accuracy(model_info): train_acc=model_info.history[&#39;acc&#39;] max_train_acc=max(train_acc) return (max_train_acc * 100) . function to get max validation accuracy from model history . def get_max_val_accuracy(model_info): val_acc=model_info.history[&#39;val_acc&#39;] max_val_acc=max(val_acc) return (max_val_acc * 100) . standardize pixel values of train and test images and convert train and test labels to categorical one hot encoded vectors . train_features = train_features.astype(&#39;float32&#39;)/255 test_features = test_features.astype(&#39;float32&#39;)/255 # convert class labels to binary class labels train_labels = np_utils.to_categorical(train_labels, num_classes) test_labels = np_utils.to_categorical(test_labels, num_classes) . train_labels . array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 1.], [0., 0., 0., ..., 0., 0., 1.], ..., [0., 0., 0., ..., 0., 0., 1.], [0., 1., 0., ..., 0., 0., 0.], [0., 1., 0., ..., 0., 0., 0.]], dtype=float32) . train dataset stats :mean , standard deviation for whole dataset , for a batch of 128 images , etc . (trainX, trainy), (testX, testy) = cifar10.load_data() print(&#39;Statistics train=%.3f (%.3f), test=%.3f (%.3f)&#39; % (trainX.mean(), trainX.std(), testX.mean(), testX.std())) # create generator that centers pixel values datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True) # calculate the mean on the training dataset datagen.fit(trainX) #print(&#39;Data Generator mean=%.3f, std=%.3f&#39; % (datagen.mean, datagen.std)) # demonstrate effect on a single batch of samples iterator = datagen.flow(trainX, trainy, batch_size=128) # get a batch batchX, batchy = iterator.next() # pixel stats in the batch print(batchX.shape, batchX.mean(), batchX.std()) # demonstrate effect on entire training dataset iterator = datagen.flow(trainX, trainy, batch_size=len(trainX), shuffle=False) # get a batch batchX, batchy = iterator.next() # pixel stats in the batch print(batchX.shape, batchX.mean(), batchX.std()) . Statistics train=120.708 (64.150), test=121.529 (64.061) (128, 32, 32, 3) 0.01989002 1.0052702 (50000, 32, 32, 3) -1.6605131e-06 1.0000001 . iterator1 = datagen.flow(testX, testy, batch_size=len(testX), shuffle=False) batch_testX, batch_testy = iterator1.next() X_train = batchX X_test = batch_testX y_train=batchy y_test=batch_testy . Y_train = np_utils.to_categorical(y_train, 10) Y_test = np_utils.to_categorical(y_test, 10) . Use the following standardization/regularization techniques for the model . Using Image Normalization | Making use of Batch Normalization | Making use of L2 Regularizer | Properly using Dropout | model1 = Sequential() model1.add(Convolution2D(32, 3, 3, border_mode=&#39;same&#39;,kernel_regularizer=regularizers.l2(0.0001), input_shape=(32, 32, 3))) model1.add(Activation(&#39;relu&#39;)) model1.add(BatchNormalization()) model1.add(Convolution2D(64, 3, 3,kernel_regularizer=regularizers.l2(0.0001),border_mode=&#39;same&#39;)) model1.add(Activation(&#39;relu&#39;)) model1.add(BatchNormalization()) model1.add(MaxPooling2D(pool_size=(2, 2))) model1.add(Dropout(0.2)) model1.add(Convolution2D(32, 1, 1)) model1.add(Convolution2D(64, 3, 3,kernel_regularizer=regularizers.l2(0.0001),border_mode=&#39;same&#39;)) model1.add(Activation(&#39;relu&#39;)) model1.add(BatchNormalization()) model1.add(Convolution2D(128, 3, 3,kernel_regularizer=regularizers.l2(0.0001),border_mode=&#39;same&#39;)) model1.add(Activation(&#39;relu&#39;)) model1.add(BatchNormalization()) model1.add(MaxPooling2D(pool_size=(2, 2))) model1.add(Dropout(0.3)) model1.add(Convolution2D(32, 1, 1)) model1.add(Convolution2D(128, 3, 3,kernel_regularizer=regularizers.l2(0.0001), border_mode=&#39;same&#39;)) model1.add(Activation(&#39;relu&#39;)) model1.add(BatchNormalization()) model1.add(Convolution2D(256, 3, 3,kernel_regularizer=regularizers.l2(0.0001), border_mode=&#39;same&#39;, name=&#39;LC1&#39;)) model1.add(Activation(&#39;relu&#39;,name=&#39;R1&#39;)) model1.add(BatchNormalization(name=&#39;BN1&#39;)) model1.add(MaxPooling2D(pool_size=(2, 2))) model1.add(Dropout(0.5)) model1.add(Convolution2D(10, 1, 1, name=&quot;red1&quot;)) model1.add(AveragePooling2D(pool_size = (4,4))) model1.add(Flatten()) model1.add(Activation(&#39;softmax&#39;)) . WARNING: Logging before flag parsing goes to stderr. W0720 16:04:14.356874 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead. /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), kernel_regularizer=&lt;keras.reg..., input_shape=(32, 32, 3..., padding=&#34;same&#34;)` W0720 16:04:14.388915 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. W0720 16:04:14.394831 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead. W0720 16:04:14.438685 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead. W0720 16:04:14.439589 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead. W0720 16:04:16.704932 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead. /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), kernel_regularizer=&lt;keras.reg..., padding=&#34;same&#34;)` W0720 16:04:16.960443 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead. W0720 16:04:16.970278 140478930995072 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating: Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1))` del sys.path[0] /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), kernel_regularizer=&lt;keras.reg..., padding=&#34;same&#34;)` app.launch_new_instance() /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), kernel_regularizer=&lt;keras.reg..., padding=&#34;same&#34;)` /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1))` /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), kernel_regularizer=&lt;keras.reg..., padding=&#34;same&#34;)` /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), kernel_regularizer=&lt;keras.reg..., name=&#34;LC1&#34;, padding=&#34;same&#34;)` /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), name=&#34;red1&#34;)` W0720 16:04:17.392197 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead. . print model summary . model1.summary() . _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 32, 32, 32) 896 _________________________________________________________________ activation_1 (Activation) (None, 32, 32, 32) 0 _________________________________________________________________ batch_normalization_1 (Batch (None, 32, 32, 32) 128 _________________________________________________________________ conv2d_2 (Conv2D) (None, 32, 32, 64) 18496 _________________________________________________________________ activation_2 (Activation) (None, 32, 32, 64) 0 _________________________________________________________________ batch_normalization_2 (Batch (None, 32, 32, 64) 256 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 16, 16, 64) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 16, 16, 32) 2080 _________________________________________________________________ conv2d_4 (Conv2D) (None, 16, 16, 64) 18496 _________________________________________________________________ activation_3 (Activation) (None, 16, 16, 64) 0 _________________________________________________________________ batch_normalization_3 (Batch (None, 16, 16, 64) 256 _________________________________________________________________ conv2d_5 (Conv2D) (None, 16, 16, 128) 73856 _________________________________________________________________ activation_4 (Activation) (None, 16, 16, 128) 0 _________________________________________________________________ batch_normalization_4 (Batch (None, 16, 16, 128) 512 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 8, 8, 128) 0 _________________________________________________________________ dropout_2 (Dropout) (None, 8, 8, 128) 0 _________________________________________________________________ conv2d_6 (Conv2D) (None, 8, 8, 32) 4128 _________________________________________________________________ conv2d_7 (Conv2D) (None, 8, 8, 128) 36992 _________________________________________________________________ activation_5 (Activation) (None, 8, 8, 128) 0 _________________________________________________________________ batch_normalization_5 (Batch (None, 8, 8, 128) 512 _________________________________________________________________ LC1 (Conv2D) (None, 8, 8, 256) 295168 _________________________________________________________________ R1 (Activation) (None, 8, 8, 256) 0 _________________________________________________________________ BN1 (BatchNormalization) (None, 8, 8, 256) 1024 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 4, 4, 256) 0 _________________________________________________________________ dropout_3 (Dropout) (None, 4, 4, 256) 0 _________________________________________________________________ red1 (Conv2D) (None, 4, 4, 10) 2570 _________________________________________________________________ average_pooling2d_1 (Average (None, 1, 1, 10) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 10) 0 _________________________________________________________________ activation_6 (Activation) (None, 10) 0 ================================================================= Total params: 455,370 Trainable params: 454,026 Non-trainable params: 1,344 _________________________________________________________________ . Total params: 455,370 . LR Finder . Leslie N. Smith&#39;s 2015 paper “Cyclical Learning Rates for Training Neural Networks&quot; describes an effective technique of finding a range of learning rates for neural network where loss descends steeply and where the training diverges due to training rate being too large. . This page http://puzzlemusa.com/2018/05/14/learning-rate-finder-using-keras/ describes how to use this technique of LR Finder . We will use LR Finder to fix our initial learning rate to train the model . In order to use the model with LR Finder compile the model with SGD optimizer . model1.compile(optimizer=&#39;sgd&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . W0720 16:04:17.436660 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. . Get LR finder callback from this reference http://puzzlemusa.com/2018/05/14/learning-rate-finder-using-keras/ . Add functions to print LR at min loss and min smoothed loss . from keras.callbacks import Callback class LR_Finder(Callback): def __init__(self, start_lr=1e-5, end_lr=10, step_size=None, beta=.98): super().__init__() self.start_lr = start_lr self.end_lr = end_lr self.step_size = step_size self.beta = beta self.lr_mult = (end_lr / start_lr) ** (1 / step_size) #print(&quot;lr mult : &quot;+str(self.lr_mult)) def on_train_begin(self, logs=None): self.best_loss = 1e9 self.avg_loss = 0 self.losses, self.smoothed_losses, self.lrs, self.iterations = [], [], [], [] self.iteration = 0 logs = logs or {} K.set_value(self.model.optimizer.lr, self.start_lr) def on_batch_end(self, epoch, logs=None): logs = logs or {} loss = logs.get(&#39;loss&#39;) self.iteration += 1 self.avg_loss = self.beta * self.avg_loss + (1 - self.beta) * loss smoothed_loss = self.avg_loss / (1 - self.beta ** self.iteration) # Check if the loss is not exploding if self.iteration &gt; 1 and smoothed_loss &gt; self.best_loss * 4: self.model.stop_training = True return if smoothed_loss &lt; self.best_loss or self.iteration == 1: self.best_loss = smoothed_loss lr = self.start_lr * (self.lr_mult ** self.iteration) #print(&quot;lr = &quot;+str(lr)) self.losses.append(loss) self.smoothed_losses.append(smoothed_loss) self.lrs.append(lr) self.iterations.append(self.iteration) K.set_value(self.model.optimizer.lr, lr) def plot_lr(self): plt.figure(figsize=(18,12)) plt.xlabel(&#39;Iterations&#39;) plt.ylabel(&#39;Learning rate&#39;) plt.plot(self.iterations, self.lrs) def plot(self, n_skip=10): plt.figure(figsize=(18,12)) plt.ylabel(&#39;Loss&#39;) plt.xlabel(&#39;Learning rate (log scale)&#39;) plt.plot(self.lrs[n_skip:-5], self.losses[n_skip:-5]) plt.xscale(&#39;log&#39;) def plot_smoothed_loss(self, n_skip=10): plt.figure(figsize=(18,12)) plt.ylabel(&#39;Smoothed Losses&#39;) plt.xlabel(&#39;Learning rate (log scale)&#39;) plt.plot(self.lrs[n_skip:-5], self.smoothed_losses[n_skip:-5]) plt.xscale(&#39;log&#39;) def plot_loss(self): plt.figure(figsize=(18,12)) plt.ylabel(&#39;Losses&#39;) plt.xlabel(&#39;Iterations&#39;) plt.plot(self.iterations[10:], self.losses[10:]) def get_best_loss(self): return self.best_loss def find_lr_at_best_loss(self): print(&quot;====================================================================&quot;) print(&quot;LR at min loss &quot;) print(&quot;LR at min loss : &quot;+str(self.lrs[np.argmin(self.losses)])) print(&quot;LR at min smoothed loss : &quot;+str(self.lrs[np.argmin(self.smoothed_losses)])) print(&quot;====================================================================&quot;) . Run LR_finder for 1 epoch with start_lr=1e-5 , end_lr=10 . batch_size=128 lr_finder = LR_Finder(start_lr=1e-5, end_lr=10, step_size=np.ceil(X_train.shape[0]/batch_size)) model1.fit(X_train, Y_train, epochs=1, batch_size=batch_size,callbacks=[lr_finder] ) . W0720 16:04:17.724291 140478930995072 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where . Epoch 1/1 50000/50000 [==============================] - 14s 283us/step - loss: 3.2580 - acc: 0.1908 . &lt;keras.callbacks.History at 0x7fc3a9a1d588&gt; . plot the following . 1. LR vs iterations . 2. Loss vs LR(log scale) . 3. Smoothed Loss vs LR(log scale) . From the smoothed loss vs LR plot we can see that the max descent for loss is between lr of 0.01 and 0.1 . lr_finder.plot_lr() . lr_finder.plot() . lr_finder.plot_smoothed_loss() . lr_finder.find_lr_at_best_loss() . ==================================================================== LR at min loss LR at min loss : 0.04489251258218551 LR at min smoothed loss : 0.04489251258218551 ==================================================================== . K.eval(lr_finder.model.optimizer.lr) . 10.0 . From the loss vs lr plots ,The max rate of descent seems to be between 0.01 and 0.1 and it looks like min loss is between lr values of 0.1 and 0.01 . printing the lr corresponding to the min loss and min smoothed loss confirms this observation . Let us pick an initial learning rate of 0.045 since that is where the smoothened loss curve starts going up . we will use SGD optimizer with lr=0.045 and momentum =0.9 to compile the model . from keras import optimizers opt= optimizers.SGD(lr=0.05,momentum=0.9) model1.compile(optimizer=opt, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . Cutout Augmentation . Cutout was first presented as an effective augmentation technique in these two papers : Improved Regularization of Convolutional Neural Networks with Cutout and Random Erasing Data Augmentation The idea is to randomly cut away patches of information from images that a model is training on to force it to learn from more parts of the image. This would help the model learn more features about a class instead of depending on some simple assumptions using smaller areas within the image . This helps the model generalize better and make better predictions . We will use python code for random erasing found at https://github.com/yu4u/cutout-random-erasing . !wget https://raw.githubusercontent.com/yu4u/cutout-random-erasing/master/random_eraser.py . --2019-07-20 16:05:13-- https://raw.githubusercontent.com/yu4u/cutout-random-erasing/master/random_eraser.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 888 [text/plain] Saving to: ‘random_eraser.py’ random_eraser.py 100%[===================&gt;] 888 --.-KB/s in 0s 2019-07-20 16:05:13 (158 MB/s) - ‘random_eraser.py’ saved [888/888] . train the model for 100 epochs . Use image augmentation of random cutout , horizontal flip . plot accuracy vs epochs , print accuracy and max val accuracy . from keras.preprocessing.image import ImageDataGenerator from random_eraser import get_random_eraser batch_size=128 train_datagen=ImageDataGenerator( featurewise_center=True, # set input mean to 0 over the dataset #samplewise_center=False, # set each sample mean to 0 featurewise_std_normalization=True, # divide inputs by std of the dataset #samplewise_std_normalization=False, # divide each input by its std preprocessing_function=get_random_eraser(v_l=0, v_h=1), horizontal_flip=True ) val_datagen= ImageDataGenerator( featurewise_center=True, # set input mean to 0 over the dataset featurewise_std_normalization=True, # divide inputs by std of the dataset ) train_datagen.fit(X_train) val_datagen.fit(X_test) training_generator=train_datagen.flow(X_train, Y_train, batch_size=batch_size,shuffle=True,seed=42) validation_generator=val_datagen.flow(X_test, Y_test, batch_size=batch_size,shuffle=True,seed=42) # train the model start = time.time() # Train the model model_info = model1.fit_generator(training_generator, epochs=100, steps_per_epoch=np.ceil(X_train.shape[0]/batch_size), validation_steps=np.ceil(X_test.shape[0]/batch_size), validation_data=validation_generator, shuffle=True, verbose=0) end = time.time() print (&quot;Model took %0.2f seconds to train&quot;%(end - start)) # plot model history plot_model_history(model_info) # compute test accuracy print (&quot;Accuracy on test data is: %0.2f&quot;%accuracy(X_test, Y_test, model1)) print (&quot;Max training accuracy is: %0.2f&quot;%get_max_train_accuracy(model_info)) print (&quot;Max validation accuracy is: %0.2f&quot;%get_max_val_accuracy(model_info)) . Model took 1151.59 seconds to train . Accuracy on test data is: 86.24 Max training accuracy is: 87.53 Max validation accuracy is: 87.81 . the model was trained for 100 epochs and reached a max val accuracy of 87.81 .We also notice that it almost the same as the max training accuracy . Training more epochs would yield better accuracy values . We will stop at 100 epochs and prepare to print Grad-CAM visualization on some misclassified images from this model&#39;s prediction on the test data . Grad-CAM . Now let us define the function for Grad-CAM visualization . This function named gradcam takes as input the model , the set of images , the labels for each image and the layer to be used for calculating gradients . It returns a list of dictionaries containing original image , the heatmap, the titles to display during visualization . import cv2 from mpl_toolkits.axes_grid1 import ImageGrid from google.colab.patches import cv2_imshow from IPython.core.display import display, HTML #select test images and corresponding labels to print heatmap #x=np.array([test_features[41],test_features[410],test_features[222],test_features[950]]) #y=[test_labels[41],test_labels[410],test_labels[222],test_labels[950]] def gradcam(model1,x,y,which_layer): # results=[] #make prediction for these 4 images preds = model1.predict(x) for j in range(x.shape[0]): #get class id from the prediction values class_idx = np.argmax(preds[j]) class_output = model1.output[:, class_idx] ## choose the layer nearest to prediction that has a size of about 7x7 or 8x8 #in this case it is the layer being sent to the gradcam function last_conv_layer = model1.get_layer(which_layer) # compute gradients and from heatmap grads = K.gradients(class_output, last_conv_layer.output)[0] pooled_grads = K.mean(grads, axis=(0, 1, 2)) iterate = K.function([model1.input], [pooled_grads, last_conv_layer.output[0]]) pooled_grads_value, conv_layer_output_value = iterate([x]) #apply the pooled grad value to the conv layer channels for i in range(256): conv_layer_output_value[:, :, i] *= pooled_grads_value[i] #get the mean of the weighted values and assign to heatmap heatmap = np.mean(conv_layer_output_value, axis=-1) #retain only positive values (or 0) in heatmap heatmap = np.maximum(heatmap, 0) #convert values between 0 and 1 using divide by max value heatmap /= np.max(heatmap) #we now have a heatmap with size equal to the output size of the layer we chose #img is the image we are running gradcam on img = x[j] #resize heatmap 8x8 to image size of 32x32 heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0])) #convert pixel values to be between 0 and 255 heatmap = np.uint8(255 * heatmap) #apply suitable cv2 colormap . In this case colormap_JET heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET) # convert from BGR to RGB if we want to display using matplotlib heatmap1 = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB) # create superimposed image if we want to print using cv2 (cv2_imshow supported in colab) superimposed_img = cv2.addWeighted(img, 0.5, heatmap1, 0.5, 0,dtype=5) #create a dictionary object with details of image, heatmap, its title title1=str(j+1)+&quot;: &quot;+ class_names[np.argmax(y[j])]+&quot; predicted as &quot;+str(class_names[class_idx]) title2=&#39;superimposed heatmap&#39; image1=img image2=heatmap1 image3=superimposed_img imageObj={&#39;image1&#39;:image1,&#39;image2&#39;:image2,&#39;image3&#39;:image3,&#39;title1&#39;:title1,&#39;title2&#39;:title2} #append the image dict object to results list results.append(imageObj) #print(j) #return grad-cam results as a list of dictionary objects , each containing an image and its heatmap return results . Define the function to display the Grad-CAM visualizations . This function displays a set of two images with heatmap visuals per row . def displayRow(images): # we will plot 2 images in a row # cv.imshow does not work in jupyter notebooks and colab # cv2_imshow patch works on colab but matplotlib gives us a little more flexibility in formatting the display # we will use matplotlib to print the image and its heatmap fig = plt.figure(1, (13,13)) grid = ImageGrid(fig, 111, nrows_ncols=(1,5), axes_pad=1,label_mode=&quot;1&quot; ) #horizontal spacer #grid[0].imshow(np.ones((32, 10)),alpha=0) #grid[0].axis(&#39;off&#39;) #first image #print(&quot; original class is :&quot;+class_names[np.argmax(y[j])]+&quot; and predicted class is :&quot;+str(class_names[class_idx])) grid[0].imshow(images[0][&#39;image1&#39;]) grid[0].set_title(images[0][&#39;title1&#39;]) grid[0].axis(&#39;off&#39;) #print the original image and on top of it place the heat map at 60% transparency grid[1].imshow(images[0][&#39;image1&#39;],alpha=0.9) grid[1].imshow(images[0][&#39;image2&#39;],alpha=0.6) grid[1].set_title(images[0][&#39;title2&#39;]) grid[1].axis(&#39;off&#39;) #vertical separator grid[2].imshow(np.ones((32, 1))) grid[2].axis(&#39;off&#39;) #second image #print(&quot; original class is :&quot;+class_names[np.argmax(y[j])]+&quot; and predicted class is :&quot;+str(class_names[class_idx])) grid[3].imshow(images[1][&#39;image1&#39;]) grid[3].set_title(images[1][&#39;title1&#39;]) grid[3].axis(&#39;off&#39;) #print the original image and on top of it place the heat map at 60% transparency grid[4].imshow(images[1][&#39;image1&#39;],alpha=0.9) grid[4].imshow(images[1][&#39;image2&#39;],alpha=0.6) grid[4].set_title(images[1][&#39;title2&#39;]) grid[4].axis(&#39;off&#39;) plt.show() display(HTML(&quot;&lt;hr size=&#39;5&#39; color=&#39;black&#39; width=&#39;100%&#39; align=&#39;center&#39; /&gt;&quot;)) . Make predictions using the model and collect all the images that were classified wrongly . pred=model1.predict(X_test) pred2=np.argmax(pred,axis=1) wrong_set=[] correct_set=[] wrong_labels=[] true_labels=[] wrong_indices=[] for i in range(X_test.shape[0]): if (pred2[i]==np.argmax(test_labels[i])): correct_set.append(X_test[i]) else: wrong_indices.append(i) wrong_labels.append(class_names[pred2[i]]) true_labels.append(class_names[np.argmax(test_labels[i])]) wrong_set.append(X_test[i]) . Now take the first 26 images and the corresponding labels to create the data for Grad-CAM visualization . w_list=wrong_indices[:26] x=[] y=[] for i in range(len(w_list)): x.append(test_features[w_list[i]]) y.append(test_labels[w_list[i]]) #convert the image list to numpy array x=np.array(x) . Obtain results from the gradcam function . results=gradcam(model1,x,y,&#39;R1&#39;) # we choose this layer as the layer nearest to prediction having a size of 8x8 . display the results from gradcam function with images and corresponding heatmap visuals . display(HTML(&quot;&lt;h2 align=&#39;center&#39;&gt;First 26 misclassified images with Grad-CAM heatmap &lt;/h2&gt;&lt;hr size=&#39;5&#39; color=&#39;black&#39; width=&#39;100%&#39; align=&#39;center&#39; /&gt;&quot;)) for i in range(0,len(results),2): images=[] images.append(results[i]) images.append(results[i+1]) displayRow(images) . First 26 misclassified images with Grad-CAM heatmap . . . . . . . . . . . . . . We used LR finder to fix an optimum learning rate of 0.045 for training the model on cifar 10 dataset . The model reached a max val accuracy of 87.81 and was almost the same as the max training accuracy indicating that this model would reach even higher accuracies with more epochs . We then used Grad-CAM to visualize the heatmaps for a set of 26 images that this model misclassifed. .",
            "url": "https://ravindrabharathi.github.io/blog/lr%20finder/grad-cam/heatmap%20visualization/optimimum%20lr/2020/02/10/LR-Finder.html",
            "relUrl": "/lr%20finder/grad-cam/heatmap%20visualization/optimimum%20lr/2020/02/10/LR-Finder.html",
            "date": " • Feb 10, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Gradient-weighted Class Activation Mapping (Grad-CAM)",
            "content": "We will classify images in CIFAR 10 dataset and integrate Grad-CAM visualization. We will also use Cutout Image Augmentation for training the model . Import necessary Modules . from keras import backend as K import time import matplotlib.pyplot as plt import numpy as np % matplotlib inline np.random.seed(2017) from keras.models import Sequential from keras.layers.convolutional import Convolution2D, MaxPooling2D from keras.layers import Activation, Flatten, Dropout from keras.layers.normalization import BatchNormalization from keras.utils import np_utils . create train and test data using cifar10 dataset in Keras . from keras.datasets import cifar10 (train_features, train_labels), (test_features, test_labels) = cifar10.load_data() num_train, img_channels, img_rows, img_cols = train_features.shape num_test, _, _, _ = test_features.shape num_classes = len(np.unique(train_labels)) . Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz 170500096/170498071 [==============================] - 11s 0us/step . Plot some of the images in the dataset along with class label . class_names = [&#39;airplane&#39;,&#39;automobile&#39;,&#39;bird&#39;,&#39;cat&#39;,&#39;deer&#39;, &#39;dog&#39;,&#39;frog&#39;,&#39;horse&#39;,&#39;ship&#39;,&#39;truck&#39;] fig = plt.figure(figsize=(8,3)) for i in range(num_classes): ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[]) idx = np.where(train_labels[:]==i)[0] features_idx = train_features[idx,::] img_num = np.random.randint(features_idx.shape[0]) im = features_idx[img_num] ax.set_title(class_names[i]) plt.imshow(im) plt.show() . Scale the input features to be within 0 and 1 . convert the train and test labels to 10 class category format . train_features = train_features.astype(&#39;float32&#39;)/255 test_features = test_features.astype(&#39;float32&#39;)/255 # convert class labels to binary class labels train_labels = np_utils.to_categorical(train_labels, num_classes) test_labels = np_utils.to_categorical(test_labels, num_classes) . Define Model for image classification . from keras.layers import Conv2D,BatchNormalization,MaxPooling2D,Activation,Flatten # Define the model #RF model = Sequential() model.add(Conv2D(32, 3, border_mode=&#39;same&#39;, name=&#39;layer1&#39;, input_shape=(32, 32, 3))) #3 model.add(BatchNormalization(name=&#39;BN1&#39;)) model.add(Activation(&#39;relu&#39;,name=&#39;rl1&#39;)) #Conv block 1 model.add(Conv2D(64, 3,name=&#39;layer2&#39;,border_mode=&#39;same&#39;)) #5 model.add(BatchNormalization(name=&#39;BN2&#39;)) model.add(Activation(&#39;relu&#39;,name=&#39;rl2&#39;)) model.add(Conv2D(128, 3,name=&#39;layer3&#39;)) #7 model.add(BatchNormalization(name=&#39;BN3&#39;)) model.add(Activation(&#39;relu&#39;,name=&#39;rl3&#39;)) #dropout after conv block1 model.add(Dropout(0.1,name=&#39;drp1&#39;)) #Transition Block 1 model.add(Conv2D(32,1,name=&#39;tb1&#39;)) model.add(BatchNormalization(name=&#39;tb-BN1&#39;)) model.add(Activation(&#39;relu&#39;,name=&#39;tb-rl1&#39;)) model.add(MaxPooling2D(pool_size=(2, 2),name=&#39;mp1&#39;)) #14 #Conv Block 2 model.add(Conv2D(64, 3, name=&#39;layer4&#39;,border_mode=&#39;same&#39;)) #16 model.add(BatchNormalization(name=&#39;BN4&#39;)) model.add(Activation(&#39;relu&#39;,name=&#39;rl4&#39;)) model.add(Conv2D(128, 3,name=&#39;layer5&#39;,border_mode=&#39;same&#39;)) #18 model.add(BatchNormalization(name=&#39;BN5&#39;)) model.add(Activation(&#39;relu&#39;,name=&#39;rl5&#39;)) #dropout after conv block2 model.add(Dropout(0.1,name=&#39;drp2&#39;)) #Transition Block 2 model.add(Conv2D(32,1,name=&#39;tb2&#39;)) model.add(BatchNormalization(name=&#39;tb-BN2&#39;)) model.add(Activation(&#39;relu&#39;,name=&#39;tb-rl2&#39;)) model.add(MaxPooling2D(pool_size=(2, 2),name=&#39;mp2&#39;)) #36 - we have reached the image size here #final conv Block model.add(Conv2D(64, 3, name=&#39;layer6&#39;,border_mode=&#39;same&#39;)) #38 model.add(BatchNormalization(name=&#39;BN6&#39;)) model.add(Activation(&#39;relu&#39;,name=&#39;rl6&#39;)) model.add(Conv2D(128, 3,name=&#39;layer7&#39;,border_mode=&#39;same&#39;)) #40 model.add(BatchNormalization(name=&#39;BN7&#39;)) model.add(Activation(&#39;relu&#39;,name=&#39;rl7&#39;)) #dropout after final conv block model.add(Dropout(0.1,name=&#39;d3&#39;)) #Pointwise convolution to squash 128 channels to 10 output channels model.add(Conv2D(10,1,name=&#39;red1&#39;)) model.add(BatchNormalization(name=&#39;red-BN1&#39;)) model.add(Activation(&#39;relu&#39;,name=&#39;rrl1&#39;)) #last conv layer - No ReLU activation, No Batch Normalization model.add(Conv2D(10,7,name=&#39;layer8&#39;)) #47 #Flatten the output model.add(Flatten()) #Softmax activation to output likelihood values for classes model.add(Activation(&#39;softmax&#39;)) #Print model summary model.summary() . WARNING: Logging before flag parsing goes to stderr. W0627 19:25:18.179095 139858174785408 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead. /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, 3, name=&#34;layer1&#34;, input_shape=(32, 32, 3..., padding=&#34;same&#34;)` &#34;&#34;&#34; W0627 19:25:18.194814 139858174785408 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. W0627 19:25:18.197712 139858174785408 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead. W0627 19:25:18.224598 139858174785408 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead. W0627 19:25:18.225610 139858174785408 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead. W0627 19:25:18.964834 139858174785408 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead. /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, 3, name=&#34;layer2&#34;, padding=&#34;same&#34;)` # This is added back by InteractiveShellApp.init_path() W0627 19:25:19.196393 139858174785408 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating: Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. W0627 19:25:19.384407 139858174785408 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead. /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, 3, name=&#34;layer4&#34;, padding=&#34;same&#34;)` /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, 3, name=&#34;layer5&#34;, padding=&#34;same&#34;)` /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, 3, name=&#34;layer6&#34;, padding=&#34;same&#34;)` /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, 3, name=&#34;layer7&#34;, padding=&#34;same&#34;)` . _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= layer1 (Conv2D) (None, 32, 32, 32) 896 _________________________________________________________________ BN1 (BatchNormalization) (None, 32, 32, 32) 128 _________________________________________________________________ rl1 (Activation) (None, 32, 32, 32) 0 _________________________________________________________________ layer2 (Conv2D) (None, 32, 32, 64) 18496 _________________________________________________________________ BN2 (BatchNormalization) (None, 32, 32, 64) 256 _________________________________________________________________ rl2 (Activation) (None, 32, 32, 64) 0 _________________________________________________________________ layer3 (Conv2D) (None, 30, 30, 128) 73856 _________________________________________________________________ BN3 (BatchNormalization) (None, 30, 30, 128) 512 _________________________________________________________________ rl3 (Activation) (None, 30, 30, 128) 0 _________________________________________________________________ drp1 (Dropout) (None, 30, 30, 128) 0 _________________________________________________________________ tb1 (Conv2D) (None, 30, 30, 32) 4128 _________________________________________________________________ tb-BN1 (BatchNormalization) (None, 30, 30, 32) 128 _________________________________________________________________ tb-rl1 (Activation) (None, 30, 30, 32) 0 _________________________________________________________________ mp1 (MaxPooling2D) (None, 15, 15, 32) 0 _________________________________________________________________ layer4 (Conv2D) (None, 15, 15, 64) 18496 _________________________________________________________________ BN4 (BatchNormalization) (None, 15, 15, 64) 256 _________________________________________________________________ rl4 (Activation) (None, 15, 15, 64) 0 _________________________________________________________________ layer5 (Conv2D) (None, 15, 15, 128) 73856 _________________________________________________________________ BN5 (BatchNormalization) (None, 15, 15, 128) 512 _________________________________________________________________ rl5 (Activation) (None, 15, 15, 128) 0 _________________________________________________________________ drp2 (Dropout) (None, 15, 15, 128) 0 _________________________________________________________________ tb2 (Conv2D) (None, 15, 15, 32) 4128 _________________________________________________________________ tb-BN2 (BatchNormalization) (None, 15, 15, 32) 128 _________________________________________________________________ tb-rl2 (Activation) (None, 15, 15, 32) 0 _________________________________________________________________ mp2 (MaxPooling2D) (None, 7, 7, 32) 0 _________________________________________________________________ layer6 (Conv2D) (None, 7, 7, 64) 18496 _________________________________________________________________ BN6 (BatchNormalization) (None, 7, 7, 64) 256 _________________________________________________________________ rl6 (Activation) (None, 7, 7, 64) 0 _________________________________________________________________ layer7 (Conv2D) (None, 7, 7, 128) 73856 _________________________________________________________________ BN7 (BatchNormalization) (None, 7, 7, 128) 512 _________________________________________________________________ rl7 (Activation) (None, 7, 7, 128) 0 _________________________________________________________________ d3 (Dropout) (None, 7, 7, 128) 0 _________________________________________________________________ red1 (Conv2D) (None, 7, 7, 10) 1290 _________________________________________________________________ red-BN1 (BatchNormalization) (None, 7, 7, 10) 40 _________________________________________________________________ rrl1 (Activation) (None, 7, 7, 10) 0 _________________________________________________________________ layer8 (Conv2D) (None, 1, 1, 10) 4910 _________________________________________________________________ flatten_1 (Flatten) (None, 10) 0 _________________________________________________________________ activation_1 (Activation) (None, 10) 0 ================================================================= Total params: 295,136 Trainable params: 293,772 Non-trainable params: 1,364 _________________________________________________________________ . Learning Rate Scheduler : We will add a custom learning rate scheduler that reduces the rate every 3rd epoch sugject to a min of 0.0005. We will also start with a slightly larger lr of 0.003 compared to default of 0.001 for Adam optimizer . from keras.optimizers import Adam from keras.callbacks import LearningRateScheduler def scheduler(epoch, lr): if (epoch%3==0 and epoch): new_lr = max(0.9*lr,0.0005) else: new_lr=lr return round(new_lr, 10) lr_scheduler=LearningRateScheduler(scheduler,verbose=1) . model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=Adam(lr=0.003), metrics=[&#39;accuracy&#39;]) . W0627 19:25:19.959195 139858174785408 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. . mount google drive so that you can save the model with best validation accuracy and use it later for prediction tasks . from google.colab import drive def mount_drive(): drive.mount(&#39;/gdrive&#39;,force_remount=True) mount_drive() . Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&amp;response_type=code Enter your authorization code: ·········· Mounted at /gdrive . create a modelcheckpoint callback to chack validation accuracy at the end of each epoch and save the model with best validation accuracy . from keras.callbacks import ModelCheckpoint chkpoint_model=ModelCheckpoint(&quot;/gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5&quot;, monitor=&#39;val_acc&#39;, verbose=1, save_best_only=True, save_weights_only=False, mode=&#39;max&#39;) . Data Augmentation : Define datagenerator with horizontal flip set to True ,zoom range of 0.15 . Train the model for 100 epochs . from keras.preprocessing.image import ImageDataGenerator datagen = ImageDataGenerator(zoom_range=0.15, horizontal_flip=True) # train the model start = time.time() # Train the model model_info = model.fit_generator(datagen.flow(train_features, train_labels, batch_size = 128), samples_per_epoch = train_features.shape[0], nb_epoch = 100, validation_data = (test_features, test_labels), callbacks=[chkpoint_model,lr_scheduler],verbose=1) end = time.time() print (&quot;Model took %0.2f seconds to train n&quot;%(end - start)) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`-&gt;`validation_steps` and `val_samples`-&gt;`steps` arguments have changed. Update your method calls accordingly. /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(&lt;keras_pre..., validation_data=(array([[[..., callbacks=[&lt;keras.ca..., verbose=1, steps_per_epoch=390, epochs=100)` W0627 19:25:22.494500 139858174785408 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where . Epoch 1/100 Epoch 00001: LearningRateScheduler setting learning rate to 0.003. 390/390 [==============================] - 28s 73ms/step - loss: 1.3386 - acc: 0.5150 - val_loss: 2.5504 - val_acc: 0.3937 Epoch 00001: val_acc improved from -inf to 0.39370, saving model to /gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5 Epoch 2/100 Epoch 00002: LearningRateScheduler setting learning rate to 0.003. 390/390 [==============================] - 24s 63ms/step - loss: 0.9163 - acc: 0.6763 - val_loss: 1.0819 - val_acc: 0.6341 Epoch 00002: val_acc improved from 0.39370 to 0.63410, saving model to /gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5 Epoch 3/100 Epoch 00003: LearningRateScheduler setting learning rate to 0.003. 390/390 [==============================] - 25s 63ms/step - loss: 0.7828 - acc: 0.7260 - val_loss: 0.8384 - val_acc: 0.7091 Epoch 00003: val_acc improved from 0.63410 to 0.70910, saving model to /gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5 Epoch 4/100 Epoch 00004: LearningRateScheduler setting learning rate to 0.0027. 390/390 [==============================] - 24s 62ms/step - loss: 0.6944 - acc: 0.7559 - val_loss: 0.9584 - val_acc: 0.6864 Epoch 00004: val_acc did not improve from 0.70910 Epoch 5/100 Epoch 00005: LearningRateScheduler setting learning rate to 0.0027000001. 390/390 [==============================] - 24s 62ms/step - loss: 0.6330 - acc: 0.7791 - val_loss: 0.8523 - val_acc: 0.7208 Epoch 00005: val_acc improved from 0.70910 to 0.72080, saving model to /gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5 Epoch 6/100 Epoch 00006: LearningRateScheduler setting learning rate to 0.0027000001. 390/390 [==============================] - 24s 62ms/step - loss: 0.5904 - acc: 0.7929 - val_loss: 1.2239 - val_acc: 0.6212 Epoch 00006: val_acc did not improve from 0.72080 Epoch 7/100 Epoch 00007: LearningRateScheduler setting learning rate to 0.0024300001. 390/390 [==============================] - 24s 62ms/step - loss: 0.5478 - acc: 0.8092 - val_loss: 0.7544 - val_acc: 0.7380 Epoch 00007: val_acc improved from 0.72080 to 0.73800, saving model to /gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5 Epoch 8/100 Epoch 00008: LearningRateScheduler setting learning rate to 0.0024300001. 390/390 [==============================] - 24s 62ms/step - loss: 0.5171 - acc: 0.8191 - val_loss: 0.6991 - val_acc: 0.7669 Epoch 00008: val_acc improved from 0.73800 to 0.76690, saving model to /gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5 Epoch 9/100 Epoch 00009: LearningRateScheduler setting learning rate to 0.0024300001. 390/390 [==============================] - 25s 63ms/step - loss: 0.4938 - acc: 0.8291 - val_loss: 0.8762 - val_acc: 0.7299 Epoch 00009: val_acc did not improve from 0.76690 Epoch 10/100 Epoch 00010: LearningRateScheduler setting learning rate to 0.0021870001. 390/390 [==============================] - 24s 62ms/step - loss: 0.4619 - acc: 0.8379 - val_loss: 0.5646 - val_acc: 0.8077 Epoch 00010: val_acc improved from 0.76690 to 0.80770, saving model to /gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5 Epoch 11/100 Epoch 00011: LearningRateScheduler setting learning rate to 0.0021870001. 390/390 [==============================] - 24s 62ms/step - loss: 0.4408 - acc: 0.8472 - val_loss: 0.5457 - val_acc: 0.8105 Epoch 00011: val_acc improved from 0.80770 to 0.81050, saving model to /gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5 Epoch 12/100 Epoch 00012: LearningRateScheduler setting learning rate to 0.0021870001. 390/390 [==============================] - 24s 62ms/step - loss: 0.4312 - acc: 0.8495 - val_loss: 0.5831 - val_acc: 0.8048 Epoch 00012: val_acc did not improve from 0.81050 Epoch 13/100 Epoch 00013: LearningRateScheduler setting learning rate to 0.0019683001. 390/390 [==============================] - 24s 62ms/step - loss: 0.4065 - acc: 0.8580 - val_loss: 0.5741 - val_acc: 0.8071 Epoch 00013: val_acc did not improve from 0.81050 Epoch 14/100 Epoch 00014: LearningRateScheduler setting learning rate to 0.0019683002. 390/390 [==============================] - 24s 62ms/step - loss: 0.3854 - acc: 0.8644 - val_loss: 0.6685 - val_acc: 0.7760 Epoch 00014: val_acc did not improve from 0.81050 Epoch 15/100 Epoch 00015: LearningRateScheduler setting learning rate to 0.0019683002. 390/390 [==============================] - 24s 62ms/step - loss: 0.3822 - acc: 0.8669 - val_loss: 0.6907 - val_acc: 0.7848 Epoch 00015: val_acc did not improve from 0.81050 Epoch 16/100 Epoch 00016: LearningRateScheduler setting learning rate to 0.0017714702. 390/390 [==============================] - 24s 63ms/step - loss: 0.3550 - acc: 0.8768 - val_loss: 0.5103 - val_acc: 0.8281 Epoch 00016: val_acc improved from 0.81050 to 0.82810, saving model to /gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5 Epoch 17/100 Epoch 00017: LearningRateScheduler setting learning rate to 0.0017714702. 390/390 [==============================] - 24s 62ms/step - loss: 0.3466 - acc: 0.8783 - val_loss: 0.5340 - val_acc: 0.8252 Epoch 00017: val_acc did not improve from 0.82810 Epoch 18/100 Epoch 00018: LearningRateScheduler setting learning rate to 0.0017714702. 390/390 [==============================] - 24s 62ms/step - loss: 0.3386 - acc: 0.8829 - val_loss: 0.6789 - val_acc: 0.7971 Epoch 00018: val_acc did not improve from 0.82810 Epoch 19/100 Epoch 00019: LearningRateScheduler setting learning rate to 0.0015943232. 390/390 [==============================] - 24s 63ms/step - loss: 0.3179 - acc: 0.8891 - val_loss: 0.5386 - val_acc: 0.8302 Epoch 00019: val_acc improved from 0.82810 to 0.83020, saving model to /gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5 Epoch 20/100 Epoch 00020: LearningRateScheduler setting learning rate to 0.0015943232. 390/390 [==============================] - 25s 64ms/step - loss: 0.3111 - acc: 0.8895 - val_loss: 0.5236 - val_acc: 0.8371 Epoch 00020: val_acc improved from 0.83020 to 0.83710, saving model to /gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5 Epoch 21/100 Epoch 00021: LearningRateScheduler setting learning rate to 0.0015943232. 390/390 [==============================] - 26s 66ms/step - loss: 0.3061 - acc: 0.8916 - val_loss: 0.6673 - val_acc: 0.7944 Epoch 00021: val_acc did not improve from 0.83710 Epoch 22/100 Epoch 00022: LearningRateScheduler setting learning rate to 0.0014348909. 390/390 [==============================] - 25s 65ms/step - loss: 0.2902 - acc: 0.8979 - val_loss: 0.4576 - val_acc: 0.8463 Epoch 00022: val_acc improved from 0.83710 to 0.84630, saving model to /gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5 Epoch 23/100 Epoch 00023: LearningRateScheduler setting learning rate to 0.0014348909. 390/390 [==============================] - 24s 62ms/step - loss: 0.2780 - acc: 0.9021 - val_loss: 0.4518 - val_acc: 0.8531 Epoch 00023: val_acc improved from 0.84630 to 0.85310, saving model to /gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5 Epoch 24/100 Epoch 00024: LearningRateScheduler setting learning rate to 0.0014348909. 390/390 [==============================] - 24s 62ms/step - loss: 0.2798 - acc: 0.9014 - val_loss: 0.5083 - val_acc: 0.8384 Epoch 00024: val_acc did not improve from 0.85310 Epoch 25/100 Epoch 00025: LearningRateScheduler setting learning rate to 0.0012914018. 390/390 [==============================] - 24s 62ms/step - loss: 0.2666 - acc: 0.9061 - val_loss: 0.5021 - val_acc: 0.8413 Epoch 00025: val_acc did not improve from 0.85310 Epoch 26/100 Epoch 00026: LearningRateScheduler setting learning rate to 0.0012914018. 390/390 [==============================] - 24s 62ms/step - loss: 0.2578 - acc: 0.9074 - val_loss: 0.4619 - val_acc: 0.8531 Epoch 00026: val_acc did not improve from 0.85310 Epoch 27/100 Epoch 00027: LearningRateScheduler setting learning rate to 0.0012914018. 390/390 [==============================] - 25s 64ms/step - loss: 0.2526 - acc: 0.9107 - val_loss: 0.6129 - val_acc: 0.8177 Epoch 00027: val_acc did not improve from 0.85310 Epoch 28/100 Epoch 00028: LearningRateScheduler setting learning rate to 0.0011622616. 390/390 [==============================] - 25s 65ms/step - loss: 0.2402 - acc: 0.9149 - val_loss: 0.4123 - val_acc: 0.8653 Epoch 00028: val_acc improved from 0.85310 to 0.86530, saving model to /gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5 Epoch 29/100 Epoch 00029: LearningRateScheduler setting learning rate to 0.0011622616. 390/390 [==============================] - 24s 62ms/step - loss: 0.2329 - acc: 0.9179 - val_loss: 0.4629 - val_acc: 0.8522 Epoch 00029: val_acc did not improve from 0.86530 Epoch 30/100 Epoch 00030: LearningRateScheduler setting learning rate to 0.0011622616. 390/390 [==============================] - 24s 62ms/step - loss: 0.2277 - acc: 0.9195 - val_loss: 0.4833 - val_acc: 0.8522 Epoch 00030: val_acc did not improve from 0.86530 Epoch 31/100 Epoch 00031: LearningRateScheduler setting learning rate to 0.0010460354. 390/390 [==============================] - 24s 62ms/step - loss: 0.2199 - acc: 0.9222 - val_loss: 0.5000 - val_acc: 0.8466 Epoch 00031: val_acc did not improve from 0.86530 Epoch 32/100 Epoch 00032: LearningRateScheduler setting learning rate to 0.0010460354. 390/390 [==============================] - 25s 63ms/step - loss: 0.2136 - acc: 0.9246 - val_loss: 0.4482 - val_acc: 0.8652 Epoch 00032: val_acc did not improve from 0.86530 Epoch 33/100 Epoch 00033: LearningRateScheduler setting learning rate to 0.0010460354. 390/390 [==============================] - 24s 62ms/step - loss: 0.2123 - acc: 0.9238 - val_loss: 0.4620 - val_acc: 0.8548 Epoch 00033: val_acc did not improve from 0.86530 Epoch 34/100 Epoch 00034: LearningRateScheduler setting learning rate to 0.0009414319. 390/390 [==============================] - 24s 62ms/step - loss: 0.2061 - acc: 0.9273 - val_loss: 0.5046 - val_acc: 0.8520 Epoch 00034: val_acc did not improve from 0.86530 Epoch 35/100 Epoch 00035: LearningRateScheduler setting learning rate to 0.0009414319. 390/390 [==============================] - 24s 62ms/step - loss: 0.1968 - acc: 0.9313 - val_loss: 0.4320 - val_acc: 0.8698 Epoch 00035: val_acc improved from 0.86530 to 0.86980, saving model to /gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5 Epoch 36/100 Epoch 00036: LearningRateScheduler setting learning rate to 0.0009414319. 390/390 [==============================] - 24s 62ms/step - loss: 0.1941 - acc: 0.9319 - val_loss: 0.4695 - val_acc: 0.8561 Epoch 00036: val_acc did not improve from 0.86980 Epoch 37/100 Epoch 00037: LearningRateScheduler setting learning rate to 0.0008472887. 390/390 [==============================] - 24s 62ms/step - loss: 0.1857 - acc: 0.9343 - val_loss: 0.4558 - val_acc: 0.8654 Epoch 00037: val_acc did not improve from 0.86980 Epoch 38/100 Epoch 00038: LearningRateScheduler setting learning rate to 0.0008472887. 390/390 [==============================] - 24s 62ms/step - loss: 0.1797 - acc: 0.9355 - val_loss: 0.4444 - val_acc: 0.8648 Epoch 00038: val_acc did not improve from 0.86980 Epoch 39/100 Epoch 00039: LearningRateScheduler setting learning rate to 0.0008472887. 390/390 [==============================] - 24s 62ms/step - loss: 0.1814 - acc: 0.9363 - val_loss: 0.4368 - val_acc: 0.8718 Epoch 00039: val_acc improved from 0.86980 to 0.87180, saving model to /gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5 Epoch 40/100 Epoch 00040: LearningRateScheduler setting learning rate to 0.0007625598. 390/390 [==============================] - 24s 63ms/step - loss: 0.1749 - acc: 0.9379 - val_loss: 0.4507 - val_acc: 0.8658 Epoch 00040: val_acc did not improve from 0.87180 Epoch 41/100 Epoch 00041: LearningRateScheduler setting learning rate to 0.0007625598. 390/390 [==============================] - 24s 62ms/step - loss: 0.1742 - acc: 0.9386 - val_loss: 0.5242 - val_acc: 0.8468 Epoch 00041: val_acc did not improve from 0.87180 Epoch 42/100 Epoch 00042: LearningRateScheduler setting learning rate to 0.0007625598. 390/390 [==============================] - 24s 63ms/step - loss: 0.1680 - acc: 0.9404 - val_loss: 0.4621 - val_acc: 0.8630 Epoch 00042: val_acc did not improve from 0.87180 Epoch 43/100 Epoch 00043: LearningRateScheduler setting learning rate to 0.0006863038. 390/390 [==============================] - 24s 62ms/step - loss: 0.1635 - acc: 0.9414 - val_loss: 0.4745 - val_acc: 0.8608 Epoch 00043: val_acc did not improve from 0.87180 Epoch 44/100 Epoch 00044: LearningRateScheduler setting learning rate to 0.0006863038. 390/390 [==============================] - 24s 62ms/step - loss: 0.1610 - acc: 0.9428 - val_loss: 0.4373 - val_acc: 0.8727 Epoch 00044: val_acc improved from 0.87180 to 0.87270, saving model to /gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5 Epoch 45/100 Epoch 00045: LearningRateScheduler setting learning rate to 0.0006863038. 390/390 [==============================] - 25s 63ms/step - loss: 0.1583 - acc: 0.9431 - val_loss: 0.4971 - val_acc: 0.8579 Epoch 00045: val_acc did not improve from 0.87270 Epoch 46/100 Epoch 00046: LearningRateScheduler setting learning rate to 0.0006176734. 390/390 [==============================] - 25s 63ms/step - loss: 0.1480 - acc: 0.9476 - val_loss: 0.4316 - val_acc: 0.8757 Epoch 00046: val_acc improved from 0.87270 to 0.87570, saving model to /gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5 Epoch 47/100 Epoch 00047: LearningRateScheduler setting learning rate to 0.0006176734. 390/390 [==============================] - 24s 62ms/step - loss: 0.1508 - acc: 0.9453 - val_loss: 0.4261 - val_acc: 0.8745 Epoch 00047: val_acc did not improve from 0.87570 Epoch 48/100 Epoch 00048: LearningRateScheduler setting learning rate to 0.0006176734. 390/390 [==============================] - 24s 62ms/step - loss: 0.1458 - acc: 0.9484 - val_loss: 0.4629 - val_acc: 0.8689 Epoch 00048: val_acc did not improve from 0.87570 Epoch 49/100 Epoch 00049: LearningRateScheduler setting learning rate to 0.000555906. 390/390 [==============================] - 24s 62ms/step - loss: 0.1400 - acc: 0.9503 - val_loss: 0.4630 - val_acc: 0.8732 Epoch 00049: val_acc did not improve from 0.87570 Epoch 50/100 Epoch 00050: LearningRateScheduler setting learning rate to 0.000555906. 390/390 [==============================] - 24s 62ms/step - loss: 0.1383 - acc: 0.9500 - val_loss: 0.4634 - val_acc: 0.8687 Epoch 00050: val_acc did not improve from 0.87570 Epoch 51/100 Epoch 00051: LearningRateScheduler setting learning rate to 0.000555906. 390/390 [==============================] - 24s 62ms/step - loss: 0.1368 - acc: 0.9507 - val_loss: 0.4463 - val_acc: 0.8740 Epoch 00051: val_acc did not improve from 0.87570 Epoch 52/100 Epoch 00052: LearningRateScheduler setting learning rate to 0.0005003154. 390/390 [==============================] - 24s 62ms/step - loss: 0.1378 - acc: 0.9505 - val_loss: 0.4428 - val_acc: 0.8726 Epoch 00052: val_acc did not improve from 0.87570 Epoch 53/100 Epoch 00053: LearningRateScheduler setting learning rate to 0.0005003154. 390/390 [==============================] - 24s 62ms/step - loss: 0.1332 - acc: 0.9532 - val_loss: 0.4270 - val_acc: 0.8782 Epoch 00053: val_acc improved from 0.87570 to 0.87820, saving model to /gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5 Epoch 54/100 Epoch 00054: LearningRateScheduler setting learning rate to 0.0005003154. 390/390 [==============================] - 24s 62ms/step - loss: 0.1310 - acc: 0.9540 - val_loss: 0.4473 - val_acc: 0.8751 Epoch 00054: val_acc did not improve from 0.87820 Epoch 55/100 Epoch 00055: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1303 - acc: 0.9542 - val_loss: 0.4489 - val_acc: 0.8768 Epoch 00055: val_acc did not improve from 0.87820 Epoch 56/100 Epoch 00056: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1285 - acc: 0.9548 - val_loss: 0.4645 - val_acc: 0.8727 Epoch 00056: val_acc did not improve from 0.87820 Epoch 57/100 Epoch 00057: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1240 - acc: 0.9556 - val_loss: 0.4666 - val_acc: 0.8736 Epoch 00057: val_acc did not improve from 0.87820 Epoch 58/100 Epoch 00058: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 63ms/step - loss: 0.1262 - acc: 0.9548 - val_loss: 0.4472 - val_acc: 0.8776 Epoch 00058: val_acc did not improve from 0.87820 Epoch 59/100 Epoch 00059: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1245 - acc: 0.9565 - val_loss: 0.4428 - val_acc: 0.8781 Epoch 00059: val_acc did not improve from 0.87820 Epoch 60/100 Epoch 00060: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1213 - acc: 0.9560 - val_loss: 0.4795 - val_acc: 0.8718 Epoch 00060: val_acc did not improve from 0.87820 Epoch 61/100 Epoch 00061: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1207 - acc: 0.9571 - val_loss: 0.4990 - val_acc: 0.8662 Epoch 00061: val_acc did not improve from 0.87820 Epoch 62/100 Epoch 00062: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1221 - acc: 0.9564 - val_loss: 0.4456 - val_acc: 0.8805 Epoch 00062: val_acc improved from 0.87820 to 0.88050, saving model to /gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5 Epoch 63/100 Epoch 00063: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1231 - acc: 0.9562 - val_loss: 0.4609 - val_acc: 0.8778 Epoch 00063: val_acc did not improve from 0.88050 Epoch 64/100 Epoch 00064: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1190 - acc: 0.9573 - val_loss: 0.4852 - val_acc: 0.8721 Epoch 00064: val_acc did not improve from 0.88050 Epoch 65/100 Epoch 00065: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1207 - acc: 0.9558 - val_loss: 0.5250 - val_acc: 0.8629 Epoch 00065: val_acc did not improve from 0.88050 Epoch 66/100 Epoch 00066: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1163 - acc: 0.9588 - val_loss: 0.4776 - val_acc: 0.8730 Epoch 00066: val_acc did not improve from 0.88050 Epoch 67/100 Epoch 00067: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1151 - acc: 0.9589 - val_loss: 0.4753 - val_acc: 0.8762 Epoch 00067: val_acc did not improve from 0.88050 Epoch 68/100 Epoch 00068: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1153 - acc: 0.9591 - val_loss: 0.4765 - val_acc: 0.8739 Epoch 00068: val_acc did not improve from 0.88050 Epoch 69/100 Epoch 00069: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1144 - acc: 0.9602 - val_loss: 0.4949 - val_acc: 0.8691 Epoch 00069: val_acc did not improve from 0.88050 Epoch 70/100 Epoch 00070: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 63ms/step - loss: 0.1126 - acc: 0.9599 - val_loss: 0.4724 - val_acc: 0.8735 Epoch 00070: val_acc did not improve from 0.88050 Epoch 71/100 Epoch 00071: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1124 - acc: 0.9594 - val_loss: 0.4844 - val_acc: 0.8792 Epoch 00071: val_acc did not improve from 0.88050 Epoch 72/100 Epoch 00072: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1103 - acc: 0.9613 - val_loss: 0.4847 - val_acc: 0.8714 Epoch 00072: val_acc did not improve from 0.88050 Epoch 73/100 Epoch 00073: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1116 - acc: 0.9592 - val_loss: 0.4904 - val_acc: 0.8733 Epoch 00073: val_acc did not improve from 0.88050 Epoch 74/100 Epoch 00074: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1065 - acc: 0.9619 - val_loss: 0.4946 - val_acc: 0.8747 Epoch 00074: val_acc did not improve from 0.88050 Epoch 75/100 Epoch 00075: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1094 - acc: 0.9611 - val_loss: 0.5064 - val_acc: 0.8711 Epoch 00075: val_acc did not improve from 0.88050 Epoch 76/100 Epoch 00076: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1103 - acc: 0.9608 - val_loss: 0.5367 - val_acc: 0.8621 Epoch 00076: val_acc did not improve from 0.88050 Epoch 77/100 Epoch 00077: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1083 - acc: 0.9609 - val_loss: 0.4939 - val_acc: 0.8716 Epoch 00077: val_acc did not improve from 0.88050 Epoch 78/100 Epoch 00078: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 63ms/step - loss: 0.1025 - acc: 0.9634 - val_loss: 0.4985 - val_acc: 0.8733 Epoch 00078: val_acc did not improve from 0.88050 Epoch 79/100 Epoch 00079: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1069 - acc: 0.9614 - val_loss: 0.4908 - val_acc: 0.8796 Epoch 00079: val_acc did not improve from 0.88050 Epoch 80/100 Epoch 00080: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1050 - acc: 0.9613 - val_loss: 0.5295 - val_acc: 0.8701 Epoch 00080: val_acc did not improve from 0.88050 Epoch 81/100 Epoch 00081: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1032 - acc: 0.9629 - val_loss: 0.5254 - val_acc: 0.8702 Epoch 00081: val_acc did not improve from 0.88050 Epoch 82/100 Epoch 00082: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1017 - acc: 0.9634 - val_loss: 0.5113 - val_acc: 0.8715 Epoch 00082: val_acc did not improve from 0.88050 Epoch 83/100 Epoch 00083: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 25s 63ms/step - loss: 0.1033 - acc: 0.9624 - val_loss: 0.4798 - val_acc: 0.8771 Epoch 00083: val_acc did not improve from 0.88050 Epoch 84/100 Epoch 00084: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1022 - acc: 0.9629 - val_loss: 0.5231 - val_acc: 0.8737 Epoch 00084: val_acc did not improve from 0.88050 Epoch 85/100 Epoch 00085: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.0996 - acc: 0.9645 - val_loss: 0.5399 - val_acc: 0.8667 Epoch 00085: val_acc did not improve from 0.88050 Epoch 86/100 Epoch 00086: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.0997 - acc: 0.9636 - val_loss: 0.5004 - val_acc: 0.8740 Epoch 00086: val_acc did not improve from 0.88050 Epoch 87/100 Epoch 00087: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.1024 - acc: 0.9625 - val_loss: 0.5529 - val_acc: 0.8685 Epoch 00087: val_acc did not improve from 0.88050 Epoch 88/100 Epoch 00088: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 63ms/step - loss: 0.0958 - acc: 0.9645 - val_loss: 0.5266 - val_acc: 0.8693 Epoch 00088: val_acc did not improve from 0.88050 Epoch 89/100 Epoch 00089: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.0992 - acc: 0.9644 - val_loss: 0.5109 - val_acc: 0.8759 Epoch 00089: val_acc did not improve from 0.88050 Epoch 90/100 Epoch 00090: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.0979 - acc: 0.9654 - val_loss: 0.4919 - val_acc: 0.8765 Epoch 00090: val_acc did not improve from 0.88050 Epoch 91/100 Epoch 00091: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.0995 - acc: 0.9641 - val_loss: 0.5299 - val_acc: 0.8655 Epoch 00091: val_acc did not improve from 0.88050 Epoch 92/100 Epoch 00092: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.0975 - acc: 0.9650 - val_loss: 0.5312 - val_acc: 0.8695 Epoch 00092: val_acc did not improve from 0.88050 Epoch 93/100 Epoch 00093: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.0945 - acc: 0.9656 - val_loss: 0.5487 - val_acc: 0.8670 Epoch 00093: val_acc did not improve from 0.88050 Epoch 94/100 Epoch 00094: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.0981 - acc: 0.9651 - val_loss: 0.4930 - val_acc: 0.8800 Epoch 00094: val_acc did not improve from 0.88050 Epoch 95/100 Epoch 00095: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.0927 - acc: 0.9667 - val_loss: 0.5421 - val_acc: 0.8685 Epoch 00095: val_acc did not improve from 0.88050 Epoch 96/100 Epoch 00096: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 63ms/step - loss: 0.0952 - acc: 0.9659 - val_loss: 0.5243 - val_acc: 0.8693 Epoch 00096: val_acc did not improve from 0.88050 Epoch 97/100 Epoch 00097: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.0907 - acc: 0.9678 - val_loss: 0.5187 - val_acc: 0.8711 Epoch 00097: val_acc did not improve from 0.88050 Epoch 98/100 Epoch 00098: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.0943 - acc: 0.9667 - val_loss: 0.5180 - val_acc: 0.8752 Epoch 00098: val_acc did not improve from 0.88050 Epoch 99/100 Epoch 00099: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.0939 - acc: 0.9655 - val_loss: 0.5127 - val_acc: 0.8742 Epoch 00099: val_acc did not improve from 0.88050 Epoch 100/100 Epoch 00100: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 24s 62ms/step - loss: 0.0963 - acc: 0.9658 - val_loss: 0.5101 - val_acc: 0.8774 Epoch 00100: val_acc did not improve from 0.88050 Model took 2453.54 seconds to train . Model trained for 100 epochs and reached a max validation accuracy of 88.05. Model with best validation accuracy was saved in google drive . load the model with best validation accuracy . from keras.models import load_model model1=load_model(&#39;/gdrive/My Drive/EVA/Session9/model_customv1_cifar10_best.h5&#39;) . Gradient-weighted Class Activation Mapping (Grad-CAM) : . Grad-CAM is a technique to visually represent where amodel is looking at and why it has made a certain prediction and was first present in this paper Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization . We will integrate Grad-CAM to visualize where the network is looking at or which pixels in the image contribute most to the prediction being made . Choose 4 images from the test dataset , predict their classes and print GradCam heatmap visualization for these 4 images . import cv2 from mpl_toolkits.axes_grid1 import ImageGrid from google.colab.patches import cv2_imshow #select test images and corresponding labels to print heatmap x=np.array([test_features[41],test_features[410],test_features[222],test_features[950]]) y=[test_labels[41],test_labels[410],test_labels[222],test_labels[950]] #make prediction for these 4 images preds = model1.predict(x) for j in range(4): #get class id from the prediction values class_idx = np.argmax(preds[j]) class_output = model1.output[:, class_idx] ## choose the layer before last 7x7 layer last_conv_layer = model1.get_layer(&quot;rrl1&quot;) # compute gradients and from it heatmap grads = K.gradients(class_output, last_conv_layer.output)[0] pooled_grads = K.mean(grads, axis=(0, 1, 2)) iterate = K.function([model1.input], [pooled_grads, last_conv_layer.output[0]]) pooled_grads_value, conv_layer_output_value = iterate([x]) for i in range(10): conv_layer_output_value[:, :, i] *= pooled_grads_value[i] heatmap = np.mean(conv_layer_output_value, axis=-1) heatmap = np.maximum(heatmap, 0) heatmap /= np.max(heatmap) img = x[j] #resize heatmap 7x7 to image size of 32x32 heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0])) heatmap = np.uint8(255 * heatmap) heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET) # convert from BGR to RGB heatmap1 = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB) # create superimposed image if we want to print using cv2 (cv2_imshow supported in colab) superimposed_img = cv2.addWeighted(img, 0.8, heatmap, 0.2, 0,dtype=5) # since cv.imshow does not work in jupyter notebooks and colab # we will use matplotlib to print the image and its heatmap fig = plt.figure(1, (5,5)) grid = ImageGrid(fig, 111, nrows_ncols=(1,2), axes_pad=0.3, ) print(&quot; original class is :&quot;+class_names[np.argmax(y[j])]+&quot; and predicted class is :&quot;+str(class_names[class_idx])) grid[0].imshow(img) grid[0].set_title(&#39;Original&#39;) #print the original image and on top of it place the heat map at 70% transparency grid[1].imshow(img,alpha=1) grid[1].imshow(heatmap1,alpha=0.7) grid[1].set_title(&#39;superimposed heatmap&#39;) plt.show() . original class is :frog and predicted class is :frog . original class is :horse and predicted class is :horse . original class is :truck and predicted class is :truck . original class is :cat and predicted class is :cat . How about Misclassified Images ? . Let us also choose 4 misclassified images to visualize their Grad-CAM heatmap . pred=model1.predict(test_features) pred2=np.argmax(pred,axis=1) wrong_set=[] correct_set=[] wrong_labels=[] true_labels=[] wrong_indices=[] for i in range(10000): if (pred2[i]==np.argmax(test_labels[i])): correct_set.append(test_features[i]) else: wrong_indices.append(i) wrong_labels.append(class_names[pred2[i]]) true_labels.append(class_names[np.argmax(test_labels[i])]) wrong_set.append(test_features[i]) . A selection of 4 misclassiifed images . print(&#39; Selection of 4 misclassified images n _________________________________ n&#39;) from mpl_toolkits.axes_grid1 import ImageGrid fig = plt.figure(1, (12, 12)) grid = ImageGrid(fig, 111, nrows_ncols=(1, 4), axes_pad=1, ) for i in range(5,9): grid[i-5].imshow(wrong_set[i].reshape(32,32,3)) grid[i-5].set_title(&#39;{2}: {0}, predicted: {1}&#39;.format(true_labels[i],wrong_labels[i],wrong_indices[i])) plt.show() . Selection of 4 misclassified images _________________________________ . print Grad-CAM heatmap for the misclassifed images . w_list=wrong_indices[5:9] x=[] y=[] for i in range(len(w_list)): x.append(test_features[w_list[i]]) y.append(test_labels[w_list[i]]) #convert the image list to numpy array x=np.array(x) #make prediction for these 4 images preds = model1.predict(x) for j in range(len(x)): #get class id from the prediction values class_idx = np.argmax(preds[j]) class_output = model1.output[:, class_idx] ## choose the layer before last 7x7 layer last_conv_layer = model1.get_layer(&quot;rrl1&quot;) # compute gradients and from it heatmap grads = K.gradients(class_output, last_conv_layer.output)[0] pooled_grads = K.mean(grads, axis=(0, 1, 2)) iterate = K.function([model1.input], [pooled_grads, last_conv_layer.output[0]]) pooled_grads_value, conv_layer_output_value = iterate([x]) for i in range(10): conv_layer_output_value[:, :, i] *= pooled_grads_value[i] heatmap = np.mean(conv_layer_output_value, axis=-1) heatmap = np.maximum(heatmap, 0) heatmap /= np.max(heatmap) img = x[j] #resize heatmap 7x7 to image size of 32x32 heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0])) heatmap = np.uint8(255 * heatmap) heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET) # convert from BGR to RGB heatmap1 = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB) # create superimposed image if we want to print using cv2 (cv2_imshow supported in colab) superimposed_img = cv2.addWeighted(img, 0.8, heatmap, 0.2, 0,dtype=5) # since cv.imshow does not work in jupyter notebooks and colab # we will use matplotlib to print the image and its heatmap fig = plt.figure(1, (5,5)) grid = ImageGrid(fig, 111, nrows_ncols=(1,2), axes_pad=0.3, ) print(&quot; original class is :&quot;+class_names[np.argmax(y[j])]+&quot; and predicted class is :&quot;+str(class_names[class_idx])) grid[0].imshow(img) grid[0].set_title(&#39;Original&#39;) #print the original image and on top of it place the heat map at 70% transparency grid[1].imshow(img,alpha=1) grid[1].imshow(heatmap1,alpha=0.7) grid[1].set_title(&#39;superimposed heatmap&#39;) plt.show() . original class is :cat and predicted class is :dog . original class is :frog and predicted class is :dog . original class is :frog and predicted class is :bird . original class is :horse and predicted class is :dog . We trained the model with some basic data augmentation techniques available in Keras and visualized the Grad-CAM heatmaps for a selection 4 correctly classified images and 4 misclassifed images . Let us now use another augmentation technique called cutout to train the model and see if it improves the prediction of these misclassified images and also visualize where the model looks at when making the prediction . Cutout Augmentation . Cutout was first presented as an effective augmentation technique in these two papers : . Improved Regularization of Convolutional Neural Networks with Cutout and Random Erasing Data Augmentation . The idea is to randomly cut away patches of information from images that a model is training on to force it to learn from more parts of the image. This would help the model learn more features about a class instead of depending on some simple assumptions using smaller areas within the image . This helps the model generalize better and make better predictions . . We will use python code for cutout /random erasing found at https://github.com/yu4u/cutout-random-erasing . !wget https://raw.githubusercontent.com/yu4u/cutout-random-erasing/master/random_eraser.py . --2019-06-27 20:06:44-- https://raw.githubusercontent.com/yu4u/cutout-random-erasing/master/random_eraser.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 888 [text/plain] Saving to: ‘random_eraser.py.2’ random_eraser.py.2 100%[===================&gt;] 888 --.-KB/s in 0s 2019-06-27 20:06:45 (170 MB/s) - ‘random_eraser.py.2’ saved [888/888] . define model (It is the same as above) . from keras.layers import Conv2D,BatchNormalization,MaxPooling2D,Activation,Flatten # Define the model #RF model = Sequential() model.add(Conv2D(32, 3, border_mode=&#39;same&#39;, name=&#39;layer1&#39;, input_shape=(32, 32, 3))) #3 model.add(BatchNormalization(name=&#39;BN1&#39;)) model.add(Activation(&#39;relu&#39;,name=&#39;rl1&#39;)) #Conv block 1 model.add(Conv2D(64, 3,name=&#39;layer2&#39;,border_mode=&#39;same&#39;)) #5 model.add(BatchNormalization(name=&#39;BN2&#39;)) model.add(Activation(&#39;relu&#39;,name=&#39;rl2&#39;)) model.add(Conv2D(128, 3,name=&#39;layer3&#39;)) #7 model.add(BatchNormalization(name=&#39;BN3&#39;)) model.add(Activation(&#39;relu&#39;,name=&#39;rl3&#39;)) #dropout after conv block1 model.add(Dropout(0.1,name=&#39;drp1&#39;)) #Transition Block 1 model.add(Conv2D(32,1,name=&#39;tb1&#39;)) model.add(BatchNormalization(name=&#39;tb-BN1&#39;)) model.add(Activation(&#39;relu&#39;,name=&#39;tb-rl1&#39;)) model.add(MaxPooling2D(pool_size=(2, 2),name=&#39;mp1&#39;)) #14 #Conv Block 2 model.add(Conv2D(64, 3, name=&#39;layer4&#39;,border_mode=&#39;same&#39;)) #16 model.add(BatchNormalization(name=&#39;BN4&#39;)) model.add(Activation(&#39;relu&#39;,name=&#39;rl4&#39;)) model.add(Conv2D(128, 3,name=&#39;layer5&#39;,border_mode=&#39;same&#39;)) #18 model.add(BatchNormalization(name=&#39;BN5&#39;)) model.add(Activation(&#39;relu&#39;,name=&#39;rl5&#39;)) #dropout after conv block2 model.add(Dropout(0.1,name=&#39;drp2&#39;)) #Transition Block 2 model.add(Conv2D(32,1,name=&#39;tb2&#39;)) model.add(BatchNormalization(name=&#39;tb-BN2&#39;)) model.add(Activation(&#39;relu&#39;,name=&#39;tb-rl2&#39;)) model.add(MaxPooling2D(pool_size=(2, 2),name=&#39;mp2&#39;)) #36 - we have reached the image size here #final conv Block model.add(Conv2D(64, 3, name=&#39;layer6&#39;,border_mode=&#39;same&#39;)) #38 model.add(BatchNormalization(name=&#39;BN6&#39;)) model.add(Activation(&#39;relu&#39;,name=&#39;rl6&#39;)) model.add(Conv2D(128, 3,name=&#39;layer7&#39;,border_mode=&#39;same&#39;)) #40 model.add(BatchNormalization(name=&#39;BN7&#39;)) model.add(Activation(&#39;relu&#39;,name=&#39;rl7&#39;)) #dropout after final conv block model.add(Dropout(0.1,name=&#39;d3&#39;)) #Pointwise convolution to squash 128 channels to 10 output channels model.add(Conv2D(10,1,name=&#39;red1&#39;)) model.add(BatchNormalization(name=&#39;red-BN1&#39;)) model.add(Activation(&#39;relu&#39;,name=&#39;rrl1&#39;)) #last conv layer - No ReLU activation, No Batch Normalization model.add(Conv2D(10,7,name=&#39;layer8&#39;)) #47 #Flatten the output model.add(Flatten()) #Softmax activation to output likelihood values for classes model.add(Activation(&#39;softmax&#39;)) #Print model summary model.summary() . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, 3, name=&#34;layer1&#34;, input_shape=(32, 32, 3..., padding=&#34;same&#34;)` &#34;&#34;&#34; /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, 3, name=&#34;layer2&#34;, padding=&#34;same&#34;)` # This is added back by InteractiveShellApp.init_path() /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, 3, name=&#34;layer4&#34;, padding=&#34;same&#34;)` /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, 3, name=&#34;layer5&#34;, padding=&#34;same&#34;)` /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, 3, name=&#34;layer6&#34;, padding=&#34;same&#34;)` /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, 3, name=&#34;layer7&#34;, padding=&#34;same&#34;)` . _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= layer1 (Conv2D) (None, 32, 32, 32) 896 _________________________________________________________________ BN1 (BatchNormalization) (None, 32, 32, 32) 128 _________________________________________________________________ rl1 (Activation) (None, 32, 32, 32) 0 _________________________________________________________________ layer2 (Conv2D) (None, 32, 32, 64) 18496 _________________________________________________________________ BN2 (BatchNormalization) (None, 32, 32, 64) 256 _________________________________________________________________ rl2 (Activation) (None, 32, 32, 64) 0 _________________________________________________________________ layer3 (Conv2D) (None, 30, 30, 128) 73856 _________________________________________________________________ BN3 (BatchNormalization) (None, 30, 30, 128) 512 _________________________________________________________________ rl3 (Activation) (None, 30, 30, 128) 0 _________________________________________________________________ drp1 (Dropout) (None, 30, 30, 128) 0 _________________________________________________________________ tb1 (Conv2D) (None, 30, 30, 32) 4128 _________________________________________________________________ tb-BN1 (BatchNormalization) (None, 30, 30, 32) 128 _________________________________________________________________ tb-rl1 (Activation) (None, 30, 30, 32) 0 _________________________________________________________________ mp1 (MaxPooling2D) (None, 15, 15, 32) 0 _________________________________________________________________ layer4 (Conv2D) (None, 15, 15, 64) 18496 _________________________________________________________________ BN4 (BatchNormalization) (None, 15, 15, 64) 256 _________________________________________________________________ rl4 (Activation) (None, 15, 15, 64) 0 _________________________________________________________________ layer5 (Conv2D) (None, 15, 15, 128) 73856 _________________________________________________________________ BN5 (BatchNormalization) (None, 15, 15, 128) 512 _________________________________________________________________ rl5 (Activation) (None, 15, 15, 128) 0 _________________________________________________________________ drp2 (Dropout) (None, 15, 15, 128) 0 _________________________________________________________________ tb2 (Conv2D) (None, 15, 15, 32) 4128 _________________________________________________________________ tb-BN2 (BatchNormalization) (None, 15, 15, 32) 128 _________________________________________________________________ tb-rl2 (Activation) (None, 15, 15, 32) 0 _________________________________________________________________ mp2 (MaxPooling2D) (None, 7, 7, 32) 0 _________________________________________________________________ layer6 (Conv2D) (None, 7, 7, 64) 18496 _________________________________________________________________ BN6 (BatchNormalization) (None, 7, 7, 64) 256 _________________________________________________________________ rl6 (Activation) (None, 7, 7, 64) 0 _________________________________________________________________ layer7 (Conv2D) (None, 7, 7, 128) 73856 _________________________________________________________________ BN7 (BatchNormalization) (None, 7, 7, 128) 512 _________________________________________________________________ rl7 (Activation) (None, 7, 7, 128) 0 _________________________________________________________________ d3 (Dropout) (None, 7, 7, 128) 0 _________________________________________________________________ red1 (Conv2D) (None, 7, 7, 10) 1290 _________________________________________________________________ red-BN1 (BatchNormalization) (None, 7, 7, 10) 40 _________________________________________________________________ rrl1 (Activation) (None, 7, 7, 10) 0 _________________________________________________________________ layer8 (Conv2D) (None, 1, 1, 10) 4910 _________________________________________________________________ flatten_2 (Flatten) (None, 10) 0 _________________________________________________________________ activation_2 (Activation) (None, 10) 0 ================================================================= Total params: 295,136 Trainable params: 293,772 Non-trainable params: 1,364 _________________________________________________________________ . compile the model with Adam optimizer and initial learning rate of 0.003 . model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=Adam(lr=0.003), metrics=[&#39;accuracy&#39;]) . define a new modelcheckpoint to save this model trained with cutout augmentaion in a separate path on drive . chkpoint_model=ModelCheckpoint(&quot;/gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5&quot;, monitor=&#39;val_acc&#39;, verbose=1, save_best_only=True, save_weights_only=False, mode=&#39;max&#39;) . Data Augmentation : Define datagenerator with horizontal flip set to True ,zoom range of 0.15 . Add random erasing or cutout as a preprocessing step . Use the default parameters from the random eraser code . train the model for 100 epochs . from random_eraser import get_random_eraser from keras.preprocessing.image import ImageDataGenerator datagen = ImageDataGenerator(preprocessing_function=get_random_eraser(v_l=0, v_h=1), zoom_range=0.15, horizontal_flip=True) # train the model start = time.time() # Train the model model_info = model.fit_generator(datagen.flow(train_features, train_labels, batch_size = 128), samples_per_epoch = train_features.shape[0], nb_epoch = 100, validation_data = (test_features, test_labels), callbacks=[chkpoint_model,lr_scheduler],verbose=1) end = time.time() print (&quot;Model took %0.2f seconds to train n&quot;%(end - start)) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`-&gt;`validation_steps` and `val_samples`-&gt;`steps` arguments have changed. Update your method calls accordingly. app.launch_new_instance() /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(&lt;keras_pre..., validation_data=(array([[[..., callbacks=[&lt;keras.ca..., verbose=1, steps_per_epoch=390, epochs=100)` app.launch_new_instance() . Epoch 1/100 Epoch 00001: LearningRateScheduler setting learning rate to 0.003. 390/390 [==============================] - 29s 75ms/step - loss: 1.5322 - acc: 0.4402 - val_loss: 3.3519 - val_acc: 0.2329 Epoch 00001: val_acc improved from -inf to 0.23290, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 2/100 Epoch 00002: LearningRateScheduler setting learning rate to 0.003. 390/390 [==============================] - 26s 66ms/step - loss: 1.1131 - acc: 0.6023 - val_loss: 1.5691 - val_acc: 0.5297 Epoch 00002: val_acc improved from 0.23290 to 0.52970, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 3/100 Epoch 00003: LearningRateScheduler setting learning rate to 0.003. 390/390 [==============================] - 26s 66ms/step - loss: 0.9333 - acc: 0.6715 - val_loss: 0.9520 - val_acc: 0.6649 Epoch 00003: val_acc improved from 0.52970 to 0.66490, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 4/100 Epoch 00004: LearningRateScheduler setting learning rate to 0.0027. 390/390 [==============================] - 25s 65ms/step - loss: 0.8355 - acc: 0.7038 - val_loss: 0.9289 - val_acc: 0.6874 Epoch 00004: val_acc improved from 0.66490 to 0.68740, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 5/100 Epoch 00005: LearningRateScheduler setting learning rate to 0.0027000001. 390/390 [==============================] - 26s 66ms/step - loss: 0.7836 - acc: 0.7238 - val_loss: 1.3172 - val_acc: 0.5953 Epoch 00005: val_acc did not improve from 0.68740 Epoch 6/100 Epoch 00006: LearningRateScheduler setting learning rate to 0.0027000001. 390/390 [==============================] - 26s 67ms/step - loss: 0.7284 - acc: 0.7460 - val_loss: 0.7911 - val_acc: 0.7291 Epoch 00006: val_acc improved from 0.68740 to 0.72910, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 7/100 Epoch 00007: LearningRateScheduler setting learning rate to 0.0024300001. 390/390 [==============================] - 26s 67ms/step - loss: 0.6801 - acc: 0.7626 - val_loss: 0.9140 - val_acc: 0.6871 Epoch 00007: val_acc did not improve from 0.72910 Epoch 8/100 Epoch 00008: LearningRateScheduler setting learning rate to 0.0024300001. 390/390 [==============================] - 26s 66ms/step - loss: 0.6549 - acc: 0.7695 - val_loss: 0.8354 - val_acc: 0.7252 Epoch 00008: val_acc did not improve from 0.72910 Epoch 9/100 Epoch 00009: LearningRateScheduler setting learning rate to 0.0024300001. 390/390 [==============================] - 26s 66ms/step - loss: 0.6327 - acc: 0.7804 - val_loss: 1.1267 - val_acc: 0.6456 Epoch 00009: val_acc did not improve from 0.72910 Epoch 10/100 Epoch 00010: LearningRateScheduler setting learning rate to 0.0021870001. 390/390 [==============================] - 26s 66ms/step - loss: 0.5910 - acc: 0.7927 - val_loss: 0.6778 - val_acc: 0.7741 Epoch 00010: val_acc improved from 0.72910 to 0.77410, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 11/100 Epoch 00011: LearningRateScheduler setting learning rate to 0.0021870001. 390/390 [==============================] - 26s 65ms/step - loss: 0.5741 - acc: 0.7989 - val_loss: 0.7411 - val_acc: 0.7662 Epoch 00011: val_acc did not improve from 0.77410 Epoch 12/100 Epoch 00012: LearningRateScheduler setting learning rate to 0.0021870001. 390/390 [==============================] - 26s 66ms/step - loss: 0.5576 - acc: 0.8048 - val_loss: 0.5754 - val_acc: 0.8074 Epoch 00012: val_acc improved from 0.77410 to 0.80740, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 13/100 Epoch 00013: LearningRateScheduler setting learning rate to 0.0019683001. 390/390 [==============================] - 26s 66ms/step - loss: 0.5293 - acc: 0.8151 - val_loss: 0.6968 - val_acc: 0.7761 Epoch 00013: val_acc did not improve from 0.80740 Epoch 14/100 Epoch 00014: LearningRateScheduler setting learning rate to 0.0019683002. 390/390 [==============================] - 26s 66ms/step - loss: 0.5248 - acc: 0.8168 - val_loss: 0.5338 - val_acc: 0.8204 Epoch 00014: val_acc improved from 0.80740 to 0.82040, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 15/100 Epoch 00015: LearningRateScheduler setting learning rate to 0.0019683002. 390/390 [==============================] - 26s 66ms/step - loss: 0.5070 - acc: 0.8228 - val_loss: 0.6144 - val_acc: 0.7963 Epoch 00015: val_acc did not improve from 0.82040 Epoch 16/100 Epoch 00016: LearningRateScheduler setting learning rate to 0.0017714702. 390/390 [==============================] - 26s 66ms/step - loss: 0.4913 - acc: 0.8273 - val_loss: 0.5712 - val_acc: 0.8100 Epoch 00016: val_acc did not improve from 0.82040 Epoch 17/100 Epoch 00017: LearningRateScheduler setting learning rate to 0.0017714702. 390/390 [==============================] - 26s 66ms/step - loss: 0.4729 - acc: 0.8338 - val_loss: 0.5323 - val_acc: 0.8166 Epoch 00017: val_acc did not improve from 0.82040 Epoch 18/100 Epoch 00018: LearningRateScheduler setting learning rate to 0.0017714702. 390/390 [==============================] - 26s 66ms/step - loss: 0.4688 - acc: 0.8359 - val_loss: 0.5806 - val_acc: 0.8087 Epoch 00018: val_acc did not improve from 0.82040 Epoch 19/100 Epoch 00019: LearningRateScheduler setting learning rate to 0.0015943232. 390/390 [==============================] - 26s 66ms/step - loss: 0.4522 - acc: 0.8428 - val_loss: 0.5282 - val_acc: 0.8245 Epoch 00019: val_acc improved from 0.82040 to 0.82450, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 20/100 Epoch 00020: LearningRateScheduler setting learning rate to 0.0015943232. 390/390 [==============================] - 26s 65ms/step - loss: 0.4392 - acc: 0.8462 - val_loss: 0.6150 - val_acc: 0.7979 Epoch 00020: val_acc did not improve from 0.82450 Epoch 21/100 Epoch 00021: LearningRateScheduler setting learning rate to 0.0015943232. 390/390 [==============================] - 26s 66ms/step - loss: 0.4311 - acc: 0.8486 - val_loss: 0.5373 - val_acc: 0.8256 Epoch 00021: val_acc improved from 0.82450 to 0.82560, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 22/100 Epoch 00022: LearningRateScheduler setting learning rate to 0.0014348909. 390/390 [==============================] - 26s 66ms/step - loss: 0.4207 - acc: 0.8529 - val_loss: 0.4758 - val_acc: 0.8430 Epoch 00022: val_acc improved from 0.82560 to 0.84300, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 23/100 Epoch 00023: LearningRateScheduler setting learning rate to 0.0014348909. 390/390 [==============================] - 26s 66ms/step - loss: 0.4063 - acc: 0.8570 - val_loss: 0.5675 - val_acc: 0.8191 Epoch 00023: val_acc did not improve from 0.84300 Epoch 24/100 Epoch 00024: LearningRateScheduler setting learning rate to 0.0014348909. 390/390 [==============================] - 26s 66ms/step - loss: 0.4058 - acc: 0.8573 - val_loss: 0.4889 - val_acc: 0.8437 Epoch 00024: val_acc improved from 0.84300 to 0.84370, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 25/100 Epoch 00025: LearningRateScheduler setting learning rate to 0.0012914018. 390/390 [==============================] - 26s 66ms/step - loss: 0.3919 - acc: 0.8615 - val_loss: 0.4795 - val_acc: 0.8444 Epoch 00025: val_acc improved from 0.84370 to 0.84440, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 26/100 Epoch 00026: LearningRateScheduler setting learning rate to 0.0012914018. 390/390 [==============================] - 26s 66ms/step - loss: 0.3881 - acc: 0.8635 - val_loss: 0.4842 - val_acc: 0.8409 Epoch 00026: val_acc did not improve from 0.84440 Epoch 27/100 Epoch 00027: LearningRateScheduler setting learning rate to 0.0012914018. 390/390 [==============================] - 26s 66ms/step - loss: 0.3855 - acc: 0.8669 - val_loss: 0.4941 - val_acc: 0.8378 Epoch 00027: val_acc did not improve from 0.84440 Epoch 28/100 Epoch 00028: LearningRateScheduler setting learning rate to 0.0011622616. 390/390 [==============================] - 26s 66ms/step - loss: 0.3746 - acc: 0.8662 - val_loss: 0.4461 - val_acc: 0.8546 Epoch 00028: val_acc improved from 0.84440 to 0.85460, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 29/100 Epoch 00029: LearningRateScheduler setting learning rate to 0.0011622616. 390/390 [==============================] - 26s 66ms/step - loss: 0.3657 - acc: 0.8729 - val_loss: 0.4188 - val_acc: 0.8614 Epoch 00029: val_acc improved from 0.85460 to 0.86140, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 30/100 Epoch 00030: LearningRateScheduler setting learning rate to 0.0011622616. 390/390 [==============================] - 26s 66ms/step - loss: 0.3640 - acc: 0.8722 - val_loss: 0.5085 - val_acc: 0.8368 Epoch 00030: val_acc did not improve from 0.86140 Epoch 31/100 Epoch 00031: LearningRateScheduler setting learning rate to 0.0010460354. 390/390 [==============================] - 26s 67ms/step - loss: 0.3548 - acc: 0.8730 - val_loss: 0.4365 - val_acc: 0.8546 Epoch 00031: val_acc did not improve from 0.86140 Epoch 32/100 Epoch 00032: LearningRateScheduler setting learning rate to 0.0010460354. 390/390 [==============================] - 26s 66ms/step - loss: 0.3539 - acc: 0.8769 - val_loss: 0.4414 - val_acc: 0.8537 Epoch 00032: val_acc did not improve from 0.86140 Epoch 33/100 Epoch 00033: LearningRateScheduler setting learning rate to 0.0010460354. 390/390 [==============================] - 26s 66ms/step - loss: 0.3486 - acc: 0.8777 - val_loss: 0.4799 - val_acc: 0.8419 Epoch 00033: val_acc did not improve from 0.86140 Epoch 34/100 Epoch 00034: LearningRateScheduler setting learning rate to 0.0009414319. 390/390 [==============================] - 26s 66ms/step - loss: 0.3361 - acc: 0.8815 - val_loss: 0.4599 - val_acc: 0.8531 Epoch 00034: val_acc did not improve from 0.86140 Epoch 35/100 Epoch 00035: LearningRateScheduler setting learning rate to 0.0009414319. 390/390 [==============================] - 26s 66ms/step - loss: 0.3352 - acc: 0.8813 - val_loss: 0.5155 - val_acc: 0.8393 Epoch 00035: val_acc did not improve from 0.86140 Epoch 36/100 Epoch 00036: LearningRateScheduler setting learning rate to 0.0009414319. 390/390 [==============================] - 26s 66ms/step - loss: 0.3296 - acc: 0.8834 - val_loss: 0.4463 - val_acc: 0.8567 Epoch 00036: val_acc did not improve from 0.86140 Epoch 37/100 Epoch 00037: LearningRateScheduler setting learning rate to 0.0008472887. 390/390 [==============================] - 26s 66ms/step - loss: 0.3230 - acc: 0.8866 - val_loss: 0.5017 - val_acc: 0.8433 Epoch 00037: val_acc did not improve from 0.86140 Epoch 38/100 Epoch 00038: LearningRateScheduler setting learning rate to 0.0008472887. 390/390 [==============================] - 26s 66ms/step - loss: 0.3199 - acc: 0.8868 - val_loss: 0.4903 - val_acc: 0.8466 Epoch 00038: val_acc did not improve from 0.86140 Epoch 39/100 Epoch 00039: LearningRateScheduler setting learning rate to 0.0008472887. 390/390 [==============================] - 25s 65ms/step - loss: 0.3150 - acc: 0.8884 - val_loss: 0.3957 - val_acc: 0.8695 Epoch 00039: val_acc improved from 0.86140 to 0.86950, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 40/100 Epoch 00040: LearningRateScheduler setting learning rate to 0.0007625598. 390/390 [==============================] - 26s 66ms/step - loss: 0.3131 - acc: 0.8886 - val_loss: 0.4554 - val_acc: 0.8572 Epoch 00040: val_acc did not improve from 0.86950 Epoch 41/100 Epoch 00041: LearningRateScheduler setting learning rate to 0.0007625598. 390/390 [==============================] - 26s 65ms/step - loss: 0.3031 - acc: 0.8933 - val_loss: 0.4364 - val_acc: 0.8606 Epoch 00041: val_acc did not improve from 0.86950 Epoch 42/100 Epoch 00042: LearningRateScheduler setting learning rate to 0.0007625598. 390/390 [==============================] - 26s 66ms/step - loss: 0.3048 - acc: 0.8925 - val_loss: 0.4380 - val_acc: 0.8655 Epoch 00042: val_acc did not improve from 0.86950 Epoch 43/100 Epoch 00043: LearningRateScheduler setting learning rate to 0.0006863038. 390/390 [==============================] - 26s 67ms/step - loss: 0.3002 - acc: 0.8935 - val_loss: 0.4392 - val_acc: 0.8628 Epoch 00043: val_acc did not improve from 0.86950 Epoch 44/100 Epoch 00044: LearningRateScheduler setting learning rate to 0.0006863038. 390/390 [==============================] - 26s 66ms/step - loss: 0.2948 - acc: 0.8965 - val_loss: 0.4094 - val_acc: 0.8698 Epoch 00044: val_acc improved from 0.86950 to 0.86980, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 45/100 Epoch 00045: LearningRateScheduler setting learning rate to 0.0006863038. 390/390 [==============================] - 26s 66ms/step - loss: 0.2962 - acc: 0.8957 - val_loss: 0.4280 - val_acc: 0.8671 Epoch 00045: val_acc did not improve from 0.86980 Epoch 46/100 Epoch 00046: LearningRateScheduler setting learning rate to 0.0006176734. 390/390 [==============================] - 26s 66ms/step - loss: 0.2876 - acc: 0.8992 - val_loss: 0.4203 - val_acc: 0.8645 Epoch 00046: val_acc did not improve from 0.86980 Epoch 47/100 Epoch 00047: LearningRateScheduler setting learning rate to 0.0006176734. 390/390 [==============================] - 26s 66ms/step - loss: 0.2816 - acc: 0.9007 - val_loss: 0.3989 - val_acc: 0.8756 Epoch 00047: val_acc improved from 0.86980 to 0.87560, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 48/100 Epoch 00048: LearningRateScheduler setting learning rate to 0.0006176734. 390/390 [==============================] - 26s 66ms/step - loss: 0.2820 - acc: 0.9002 - val_loss: 0.4139 - val_acc: 0.8705 Epoch 00048: val_acc did not improve from 0.87560 Epoch 49/100 Epoch 00049: LearningRateScheduler setting learning rate to 0.000555906. 390/390 [==============================] - 26s 66ms/step - loss: 0.2756 - acc: 0.9028 - val_loss: 0.3932 - val_acc: 0.8732 Epoch 00049: val_acc did not improve from 0.87560 Epoch 50/100 Epoch 00050: LearningRateScheduler setting learning rate to 0.000555906. 390/390 [==============================] - 26s 65ms/step - loss: 0.2723 - acc: 0.9030 - val_loss: 0.3817 - val_acc: 0.8782 Epoch 00050: val_acc improved from 0.87560 to 0.87820, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 51/100 Epoch 00051: LearningRateScheduler setting learning rate to 0.000555906. 390/390 [==============================] - 25s 65ms/step - loss: 0.2719 - acc: 0.9035 - val_loss: 0.4292 - val_acc: 0.8674 Epoch 00051: val_acc did not improve from 0.87820 Epoch 52/100 Epoch 00052: LearningRateScheduler setting learning rate to 0.0005003154. 390/390 [==============================] - 26s 66ms/step - loss: 0.2757 - acc: 0.9021 - val_loss: 0.4165 - val_acc: 0.8700 Epoch 00052: val_acc did not improve from 0.87820 Epoch 53/100 Epoch 00053: LearningRateScheduler setting learning rate to 0.0005003154. 390/390 [==============================] - 26s 66ms/step - loss: 0.2659 - acc: 0.9051 - val_loss: 0.4131 - val_acc: 0.8727 Epoch 00053: val_acc did not improve from 0.87820 Epoch 54/100 Epoch 00054: LearningRateScheduler setting learning rate to 0.0005003154. 390/390 [==============================] - 26s 66ms/step - loss: 0.2654 - acc: 0.9067 - val_loss: 0.3955 - val_acc: 0.8794 Epoch 00054: val_acc improved from 0.87820 to 0.87940, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 55/100 Epoch 00055: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2690 - acc: 0.9032 - val_loss: 0.4341 - val_acc: 0.8642 Epoch 00055: val_acc did not improve from 0.87940 Epoch 56/100 Epoch 00056: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2632 - acc: 0.9066 - val_loss: 0.4070 - val_acc: 0.8717 Epoch 00056: val_acc did not improve from 0.87940 Epoch 57/100 Epoch 00057: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2543 - acc: 0.9096 - val_loss: 0.3858 - val_acc: 0.8794 Epoch 00057: val_acc did not improve from 0.87940 Epoch 58/100 Epoch 00058: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2633 - acc: 0.9076 - val_loss: 0.3999 - val_acc: 0.8760 Epoch 00058: val_acc did not improve from 0.87940 Epoch 59/100 Epoch 00059: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2561 - acc: 0.9091 - val_loss: 0.3971 - val_acc: 0.8780 Epoch 00059: val_acc did not improve from 0.87940 Epoch 60/100 Epoch 00060: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2595 - acc: 0.9091 - val_loss: 0.4161 - val_acc: 0.8714 Epoch 00060: val_acc did not improve from 0.87940 Epoch 61/100 Epoch 00061: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2579 - acc: 0.9084 - val_loss: 0.4031 - val_acc: 0.8760 Epoch 00061: val_acc did not improve from 0.87940 Epoch 62/100 Epoch 00062: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2589 - acc: 0.9073 - val_loss: 0.4104 - val_acc: 0.8708 Epoch 00062: val_acc did not improve from 0.87940 Epoch 63/100 Epoch 00063: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2523 - acc: 0.9121 - val_loss: 0.4117 - val_acc: 0.8753 Epoch 00063: val_acc did not improve from 0.87940 Epoch 64/100 Epoch 00064: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2525 - acc: 0.9114 - val_loss: 0.4070 - val_acc: 0.8743 Epoch 00064: val_acc did not improve from 0.87940 Epoch 65/100 Epoch 00065: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2550 - acc: 0.9093 - val_loss: 0.4059 - val_acc: 0.8722 Epoch 00065: val_acc did not improve from 0.87940 Epoch 66/100 Epoch 00066: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 25s 65ms/step - loss: 0.2471 - acc: 0.9115 - val_loss: 0.4236 - val_acc: 0.8738 Epoch 00066: val_acc did not improve from 0.87940 Epoch 67/100 Epoch 00067: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2502 - acc: 0.9120 - val_loss: 0.3889 - val_acc: 0.8775 Epoch 00067: val_acc did not improve from 0.87940 Epoch 68/100 Epoch 00068: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2485 - acc: 0.9120 - val_loss: 0.4276 - val_acc: 0.8705 Epoch 00068: val_acc did not improve from 0.87940 Epoch 69/100 Epoch 00069: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 65ms/step - loss: 0.2469 - acc: 0.9121 - val_loss: 0.4338 - val_acc: 0.8706 Epoch 00069: val_acc did not improve from 0.87940 Epoch 70/100 Epoch 00070: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 25s 65ms/step - loss: 0.2462 - acc: 0.9128 - val_loss: 0.3875 - val_acc: 0.8826 Epoch 00070: val_acc improved from 0.87940 to 0.88260, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 71/100 Epoch 00071: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2469 - acc: 0.9109 - val_loss: 0.4063 - val_acc: 0.8783 Epoch 00071: val_acc did not improve from 0.88260 Epoch 72/100 Epoch 00072: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2438 - acc: 0.9142 - val_loss: 0.3963 - val_acc: 0.8790 Epoch 00072: val_acc did not improve from 0.88260 Epoch 73/100 Epoch 00073: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 65ms/step - loss: 0.2404 - acc: 0.9142 - val_loss: 0.4025 - val_acc: 0.8762 Epoch 00073: val_acc did not improve from 0.88260 Epoch 74/100 Epoch 00074: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 25s 65ms/step - loss: 0.2440 - acc: 0.9133 - val_loss: 0.4124 - val_acc: 0.8751 Epoch 00074: val_acc did not improve from 0.88260 Epoch 75/100 Epoch 00075: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2427 - acc: 0.9150 - val_loss: 0.4031 - val_acc: 0.8769 Epoch 00075: val_acc did not improve from 0.88260 Epoch 76/100 Epoch 00076: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 25s 65ms/step - loss: 0.2386 - acc: 0.9161 - val_loss: 0.4553 - val_acc: 0.8661 Epoch 00076: val_acc did not improve from 0.88260 Epoch 77/100 Epoch 00077: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 25s 65ms/step - loss: 0.2394 - acc: 0.9150 - val_loss: 0.4006 - val_acc: 0.8809 Epoch 00077: val_acc did not improve from 0.88260 Epoch 78/100 Epoch 00078: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2366 - acc: 0.9158 - val_loss: 0.3951 - val_acc: 0.8815 Epoch 00078: val_acc did not improve from 0.88260 Epoch 79/100 Epoch 00079: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2335 - acc: 0.9176 - val_loss: 0.4210 - val_acc: 0.8756 Epoch 00079: val_acc did not improve from 0.88260 Epoch 80/100 Epoch 00080: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 65ms/step - loss: 0.2320 - acc: 0.9174 - val_loss: 0.4421 - val_acc: 0.8696 Epoch 00080: val_acc did not improve from 0.88260 Epoch 81/100 Epoch 00081: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2375 - acc: 0.9155 - val_loss: 0.4112 - val_acc: 0.8757 Epoch 00081: val_acc did not improve from 0.88260 Epoch 82/100 Epoch 00082: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2370 - acc: 0.9162 - val_loss: 0.4010 - val_acc: 0.8799 Epoch 00082: val_acc did not improve from 0.88260 Epoch 83/100 Epoch 00083: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 25s 65ms/step - loss: 0.2368 - acc: 0.9168 - val_loss: 0.4298 - val_acc: 0.8701 Epoch 00083: val_acc did not improve from 0.88260 Epoch 84/100 Epoch 00084: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2317 - acc: 0.9170 - val_loss: 0.4563 - val_acc: 0.8660 Epoch 00084: val_acc did not improve from 0.88260 Epoch 85/100 Epoch 00085: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 65ms/step - loss: 0.2355 - acc: 0.9181 - val_loss: 0.4113 - val_acc: 0.8769 Epoch 00085: val_acc did not improve from 0.88260 Epoch 86/100 Epoch 00086: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 25s 65ms/step - loss: 0.2314 - acc: 0.9177 - val_loss: 0.4147 - val_acc: 0.8776 Epoch 00086: val_acc did not improve from 0.88260 Epoch 87/100 Epoch 00087: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2336 - acc: 0.9167 - val_loss: 0.4029 - val_acc: 0.8826 Epoch 00087: val_acc did not improve from 0.88260 Epoch 88/100 Epoch 00088: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2299 - acc: 0.9178 - val_loss: 0.3895 - val_acc: 0.8805 Epoch 00088: val_acc did not improve from 0.88260 Epoch 89/100 Epoch 00089: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2278 - acc: 0.9193 - val_loss: 0.4378 - val_acc: 0.8704 Epoch 00089: val_acc did not improve from 0.88260 Epoch 90/100 Epoch 00090: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2295 - acc: 0.9186 - val_loss: 0.4316 - val_acc: 0.8705 Epoch 00090: val_acc did not improve from 0.88260 Epoch 91/100 Epoch 00091: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2334 - acc: 0.9162 - val_loss: 0.3876 - val_acc: 0.8828 Epoch 00091: val_acc improved from 0.88260 to 0.88280, saving model to /gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5 Epoch 92/100 Epoch 00092: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 25s 65ms/step - loss: 0.2264 - acc: 0.9201 - val_loss: 0.4117 - val_acc: 0.8776 Epoch 00092: val_acc did not improve from 0.88280 Epoch 93/100 Epoch 00093: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2252 - acc: 0.9211 - val_loss: 0.4250 - val_acc: 0.8751 Epoch 00093: val_acc did not improve from 0.88280 Epoch 94/100 Epoch 00094: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2226 - acc: 0.9224 - val_loss: 0.4000 - val_acc: 0.8807 Epoch 00094: val_acc did not improve from 0.88280 Epoch 95/100 Epoch 00095: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 65ms/step - loss: 0.2234 - acc: 0.9202 - val_loss: 0.4133 - val_acc: 0.8767 Epoch 00095: val_acc did not improve from 0.88280 Epoch 96/100 Epoch 00096: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2244 - acc: 0.9198 - val_loss: 0.4352 - val_acc: 0.8733 Epoch 00096: val_acc did not improve from 0.88280 Epoch 97/100 Epoch 00097: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2251 - acc: 0.9205 - val_loss: 0.4062 - val_acc: 0.8804 Epoch 00097: val_acc did not improve from 0.88280 Epoch 98/100 Epoch 00098: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 25s 65ms/step - loss: 0.2252 - acc: 0.9210 - val_loss: 0.4075 - val_acc: 0.8799 Epoch 00098: val_acc did not improve from 0.88280 Epoch 99/100 Epoch 00099: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2212 - acc: 0.9203 - val_loss: 0.4001 - val_acc: 0.8828 Epoch 00099: val_acc did not improve from 0.88280 Epoch 100/100 Epoch 00100: LearningRateScheduler setting learning rate to 0.0005. 390/390 [==============================] - 26s 66ms/step - loss: 0.2247 - acc: 0.9198 - val_loss: 0.4151 - val_acc: 0.8799 Epoch 00100: val_acc did not improve from 0.88280 Model took 2587.95 seconds to train . load the new model trained with cutout augmentation . model1=load_model(&#39;/gdrive/My Drive/EVA/Session9/model3_with_cutout_cifar10_best.h5&#39;) . visualize the same 4 images using Grad-CAM heatmap . x=np.array([test_features[41],test_features[410],test_features[222],test_features[950]]) y=[test_labels[41],test_labels[410],test_labels[222],test_labels[950]] #make prediction for these 4 images preds = model1.predict(x) for j in range(4): #get class id from the prediction values class_idx = np.argmax(preds[j]) class_output = model1.output[:, class_idx] ## choose the layer before last 7x7 layer last_conv_layer = model1.get_layer(&quot;rrl1&quot;) # compute gradients and from it heatmap grads = K.gradients(class_output, last_conv_layer.output)[0] pooled_grads = K.mean(grads, axis=(0, 1, 2)) iterate = K.function([model1.input], [pooled_grads, last_conv_layer.output[0]]) pooled_grads_value, conv_layer_output_value = iterate([x]) for i in range(10): conv_layer_output_value[:, :, i] *= pooled_grads_value[i] heatmap = np.mean(conv_layer_output_value, axis=-1) heatmap = np.maximum(heatmap, 0) heatmap /= np.max(heatmap) img = x[j] #resize heatmap 7x7 to image size of 32x32 heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0])) heatmap = np.uint8(255 * heatmap) heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET) # convert from BGR to RGB heatmap1 = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB) # create superimposed image if we want to print using cv2 (cv2_imshow supported in colab) superimposed_img = cv2.addWeighted(img, 0.8, heatmap, 0.2, 0,dtype=5) # since cv.imshow does not work in jupyter notebooks and colab # we will use matplotlib to print the image and its heatmap fig = plt.figure(1, (5,5)) grid = ImageGrid(fig, 111, nrows_ncols=(1,2), axes_pad=0.3, ) print(&quot; original class is :&quot;+class_names[np.argmax(y[j])]+&quot; and predicted class is :&quot;+str(class_names[class_idx])) grid[0].imshow(img) grid[0].set_title(&#39;Original&#39;) #print the original image and on top of it place the heat map at 70% transparency grid[1].imshow(img,alpha=1) grid[1].imshow(heatmap1,alpha=0.7) grid[1].set_title(&#39;superimposed heatmap&#39;) plt.show() . original class is :frog and predicted class is :frog . original class is :horse and predicted class is :horse . original class is :truck and predicted class is :truck . original class is :cat and predicted class is :cat . Let us see what happened to the 4 misclassified images after cutout augmenation - if the prediction changed and if the heatmap pattern changed too . w_list=wrong_indices[5:9] x=[] y=[] for i in range(len(w_list)): x.append(test_features[w_list[i]]) y.append(test_labels[w_list[i]]) #convert the image list to numpy array x=np.array(x) #make prediction for these 4 images preds = model1.predict(x) for j in range(len(x)): #get class id from the prediction values class_idx = np.argmax(preds[j]) class_output = model1.output[:, class_idx] ## choose the layer before last 7x7 layer last_conv_layer = model1.get_layer(&quot;rrl1&quot;) # compute gradients and from it heatmap grads = K.gradients(class_output, last_conv_layer.output)[0] pooled_grads = K.mean(grads, axis=(0, 1, 2)) iterate = K.function([model1.input], [pooled_grads, last_conv_layer.output[0]]) pooled_grads_value, conv_layer_output_value = iterate([x]) for i in range(10): conv_layer_output_value[:, :, i] *= pooled_grads_value[i] heatmap = np.mean(conv_layer_output_value, axis=-1) heatmap = np.maximum(heatmap, 0) heatmap /= np.max(heatmap) img = x[j] #resize heatmap 7x7 to image size of 32x32 heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0])) heatmap = np.uint8(255 * heatmap) heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET) # convert from BGR to RGB heatmap1 = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB) # create superimposed image if we want to print using cv2 (cv2_imshow supported in colab) superimposed_img = cv2.addWeighted(img, 0.8, heatmap, 0.2, 0,dtype=5) # since cv.imshow does not work in jupyter notebooks and colab # we will use matplotlib to print the image and its heatmap fig = plt.figure(1, (5,5)) grid = ImageGrid(fig, 111, nrows_ncols=(1,2), axes_pad=0.3, ) print(&quot; original class is :&quot;+class_names[np.argmax(y[j])]+&quot; and predicted class is :&quot;+str(class_names[class_idx])) grid[0].imshow(img) grid[0].set_title(&#39;Original&#39;) #print the original image and on top of it place the heat map at 70% transparency grid[1].imshow(img,alpha=1) grid[1].imshow(heatmap1,alpha=0.7) grid[1].set_title(&#39;superimposed heatmap&#39;) plt.show() . original class is :cat and predicted class is :cat . original class is :frog and predicted class is :frog . original class is :frog and predicted class is :frog . original class is :horse and predicted class is :horse . We can see that cutout augmenation forced the model to look at different parts of the image than it was looking at earlier and it helped in getting the classification of this set of 4 previously misclassified images right . It is also to be noted that the validation accuracy is still only at 88.28 even with cutout and we should train the network for more epochs and with different combinations of augmentations to get better results .",
            "url": "https://ravindrabharathi.github.io/blog/grad-cam/gradient%20heat%20map%20visualization/image%20classification/cifar-10/cutout%20augmentation/2020/02/10/Grad-CAM-visualization.html",
            "relUrl": "/grad-cam/gradient%20heat%20map%20visualization/image%20classification/cifar-10/cutout%20augmentation/2020/02/10/Grad-CAM-visualization.html",
            "date": " • Feb 10, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Experienced Software Engineer/Leader. . Extensive experience in developing and delivering solutions for multiple domains . Home automation / IOT / Analytics, Enterprise Search, Media /Advertising, Education /Learning Management solutions. | . Experience working with Agile / SCRUM Methodologies. .",
          "url": "https://ravindrabharathi.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ravindrabharathi.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}