{
  
    
        "post0": {
            "title": "Face Aging using CycleGANs",
            "content": ". CycleGANs were introduced in this paper titled Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks where the authors presented an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. . For the images of faces with various ages we will be using the UTKFace dataset wich has a cropped image set of only faces marked with age , gender , race , etc. . We will be using following two good references that use CycleGAN in order to build and train our models . https://github.com/sungnam0/Face-Aging-with-CycleGAN 2.https://machinelearningmastery.com/cyclegan-tutorial-with-keras/ | import necessary modules . import numpy as np import keras import tensorflow as tf from keras.layers import Input,Conv2D,MaxPooling2D,Flatten,Activation,BatchNormalization,K,UpSampling2D from keras.layers import Dropout,GlobalAveragePooling2D,LeakyReLU,Dense,Reshape, concatenate,Conv2DTranspose from keras.models import Model,load_model import matplotlib.pyplot as plt #import keras.backend as K import os import time from datetime import datetime from keras.applications import InceptionResNetV2 from keras.callbacks import TensorBoard from keras.optimizers import Adam from keras.utils import to_categorical from keras_preprocessing import image from numpy import asarray from numpy import vstack from keras.preprocessing.image import img_to_array from keras.preprocessing.image import load_img from numpy import savez_compressed import pandas as pd import os from matplotlib import pyplot from numpy import load from random import random from numpy import load from numpy import zeros from numpy import ones from numpy import asarray from numpy.random import randint from keras.optimizers import Adam . set tf backend config to allocate memory as needed instead of pre-allocating . config = tf.ConfigProto() config.gpu_options.allow_growth = True # Create a session with the above options specified. keras.backend.tensorflow_backend.set_session(tf.Session(config=config)) . mount google drive . from google.colab import drive drive.mount(&#39;/content/drive&#39;, force_remount=True) . Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&amp;response_type=code Enter your authorization code: ·········· Mounted at /content/drive . !ls -al &#39;/content/drive/My Drive/FaceGAN/&#39; . total 105354 drwx 2 root root 4096 Aug 22 10:59 results -rw- 1 root root 1239467 Aug 22 09:56 utk_data.csv drwx 2 root root 4096 Aug 21 08:41 UTKFace -rw- 1 root root 106634631 Aug 21 07:15 UTKFace.tar.gz . get the UTKFace dataset . !tar zxf &#39;/content/drive/My Drive/FaceGAN/UTKFace.tar.gz&#39; UTKFace . Parse the data . The labels of each face image is embedded in the file name, formated like [age][gender][race]_[date&amp;time].jpg . [age] is an integer from 0 to 116, indicating the age [gender] is either 0 (male) or 1 (female) [race] is an integer from 0 to 4, denoting White, Black, Asian, Indian, and Others (like Hispanic, Latino, Middle Eastern). [date&amp;time] is in the format of yyyymmddHHMMSSFFF, showing the date and time an image was collected to UTKFace . data=[] for filename in os.listdir(&#39;./UTKFace&#39;): parts=filename.split(&#39;_&#39;) #print(parts[3]) item={} item[&#39;image&#39;]=filename item[&#39;age&#39;]=parts[0] item[&#39;gender&#39;]=parts[1] item[&#39;race&#39;]=parts[2] if (len(parts)==4): item[&#39;date_time&#39;]=parts[3] data.append(item) utk_data=pd.DataFrame(data) utk_data.describe() . age date_time gender image race . count 23708 | 23705 | 23708 | 23708 | 23708 | . unique 104 | 23479 | 2 | 23708 | 8 | . top 26 | 20170110173815028.jpg.chip.jpg | 0 | 26_1_0_20170112213001988.jpg.chip.jpg | 0 | . freq 2197 | 7 | 12391 | 1 | 10078 | . utk_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 23708 entries, 0 to 23707 Data columns (total 5 columns): age 23708 non-null object date_time 23705 non-null object gender 23708 non-null object image 23708 non-null object race 23708 non-null object dtypes: object(5) memory usage: 926.2+ KB . utk_data.head() . age date_time gender image race . 0 25 | 20170119172104288.jpg.chip.jpg | 1 | 25_1_3_20170119172104288.jpg.chip.jpg | 3 | . 1 25 | 20170117141726361.jpg.chip.jpg | 1 | 25_1_0_20170117141726361.jpg.chip.jpg | 0 | . 2 27 | 20170116001407357.jpg.chip.jpg | 0 | 27_0_1_20170116001407357.jpg.chip.jpg | 1 | . 3 10 | 20170103200501766.jpg.chip.jpg | 0 | 10_0_4_20170103200501766.jpg.chip.jpg | 4 | . 4 26 | 20170116184024662.jpg.chip.jpg | 1 | 26_1_0_20170116184024662.jpg.chip.jpg | 0 | . we do not need date time , so delete it . del utk_data[&#39;date_time&#39;] . define a category for ages and apply it to the dataset . def age_cat_fn(age): age=int(age) if (0&lt;age&lt;18): return 0 elif(18&lt;=age&lt;=25): return 1 elif (25&lt;age&lt;=39): return 2 elif (39&lt; age &lt;=49): return 3 elif (49 &lt; age &lt;=60): return 4 elif age&gt;60: return 5 . utk_data[&#39;age_cat&#39;]=utk_data.age.map(age_cat_fn) . utk_data.to_csv(&#39;utk_data.csv&#39;,sep=&#39;,&#39;) !cp &#39;utk_data.csv&#39; &#39;/content/drive/My Drive/EIP3/session7&#39; . data with age category . utk_data.head() . age gender image race age_cat . 0 25 | 1 | 25_1_3_20170119172104288.jpg.chip.jpg | 3 | 1 | . 1 25 | 1 | 25_1_0_20170117141726361.jpg.chip.jpg | 0 | 1 | . 2 27 | 0 | 27_0_1_20170116001407357.jpg.chip.jpg | 1 | 2 | . 3 10 | 0 | 10_0_4_20170103200501766.jpg.chip.jpg | 4 | 0 | . 4 26 | 1 | 26_1_0_20170116184024662.jpg.chip.jpg | 0 | 2 | . split our data into two Domains . Young : age category 1 | Old : Age category 4 | data_A=utk_data[utk_data.age_cat==1] data_B=utk_data[utk_data.age_cat==4] . print(data_A[&#39;age_cat&#39;].count()) print(data_B[&#39;age_cat&#39;].count()) . 3404 2592 . #import os #utk_data=pd.read_csv(&#39;/content/drive/My Drive/EIP3/session7/utk_data.csv&#39;) . get the iames belonging to the two Domains and save as a compressed numpy array so that we can load them when necesary instead of processing the UTKFace dataset multiple times . image_dir=&#39;./UTKFace/&#39; image_paths_A = data_A[&#39;image&#39;].tolist() image_paths_B = data_B[&#39;image&#39;].tolist() #print(image_paths[:10]) . images_A=None images_B=None #store 2000 images for A for i, image_path in enumerate(image_paths_A): if (i&lt;2000): if (i%1000==0): print(&quot;processing set A image num &quot;+str(i)) try: # Load image loaded_image = image.load_img(image_dir+image_path, target_size=(128,128,3)) # Convert PIL image to numpy ndarray loaded_image = image.img_to_array(loaded_image) # Add another dimension (Add batch dimension) loaded_image = np.expand_dims(loaded_image, axis=0) # Concatenate all images into one tensor if images_A is None: images_A = loaded_image else: images_A = np.concatenate([images_A, loaded_image], axis=0) except Exception as e: print(&quot;Error:&quot;, i, e) #store 2000 images for B for i, image_path in enumerate(image_paths_B): if (i&lt;2000): if (i%999==0): print(&quot;processing set B image num &quot;+str(i)) try: # Load image loaded_image = image.load_img(image_dir+image_path, target_size=(128,128,3)) # Convert PIL image to numpy ndarray loaded_image = image.img_to_array(loaded_image) # Add another dimension (Add batch dimension) loaded_image = np.expand_dims(loaded_image, axis=0) # Concatenate all images into one tensor if images_B is None: images_B = loaded_image else: images_B = np.concatenate([images_B, loaded_image], axis=0) except Exception as e: print(&quot;Error:&quot;, i, e) . processing set A image num 0 processing set A image num 1000 processing set B image num 0 processing set B image num 999 processing set B image num 1998 . print(&#39;images_A :&#39;) print(images_A.shape) print(&#39;images_B :&#39;) print(images_B.shape) . images_A : (2000, 128, 128, 3) images_B : (2000, 128, 128, 3) . filename = &#39;/content/drive/My Drive/EIP3/session7/utkface_128.npz&#39; savez_compressed(filename, images_A, images_B) print(&#39;Saved dataset: &#39;, filename) . Saved dataset: /content/drive/My Drive/EIP3/session7/utkface_128.npz . load the saved numpy arrays and plot some images from either domain . from numpy import load from matplotlib import pyplot # load the dataset data = load(&#39;/content/drive/My Drive/EIP3/session7/utkface_128.npz&#39;) dataA, dataB = data[&#39;arr_0&#39;], data[&#39;arr_1&#39;] print(&#39;Loaded: &#39;, dataA.shape, dataB.shape) # plot source images n_samples = 3 for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(dataA[i].astype(&#39;uint8&#39;)) # plot target image for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + n_samples + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(dataB[i].astype(&#39;uint8&#39;)) pyplot.show() . Loaded: (2000, 128, 128, 3) (2000, 128, 128, 3) . install keras-contrib so that we can use InstanceNormalization instead of BatchNormalization . !pip install git+https://www.github.com/keras-team/keras-contrib.git . Collecting git+https://www.github.com/keras-team/keras-contrib.git Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-225cusg3 Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-225cusg3 Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-contrib==2.0.8) (2.2.4) Requirement already satisfied: keras-preprocessing&gt;=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras-&gt;keras-contrib==2.0.8) (1.1.0) Requirement already satisfied: keras-applications&gt;=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras-&gt;keras-contrib==2.0.8) (1.0.8) Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras-&gt;keras-contrib==2.0.8) (3.13) Requirement already satisfied: six&gt;=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras-&gt;keras-contrib==2.0.8) (1.12.0) Requirement already satisfied: scipy&gt;=0.14 in /usr/local/lib/python3.6/dist-packages (from keras-&gt;keras-contrib==2.0.8) (1.3.1) Requirement already satisfied: numpy&gt;=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras-&gt;keras-contrib==2.0.8) (1.16.4) Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-&gt;keras-contrib==2.0.8) (2.8.0) Building wheels for collected packages: keras-contrib Building wheel for keras-contrib (setup.py) ... done Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101066 sha256=4c2686ed2595b2fbcc1fd6df0f014cb70df688e3b37ff72e3fb6ef31dd35e615 Stored in directory: /tmp/pip-ephem-wheel-cache-rkeooafj/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba Successfully built keras-contrib Installing collected packages: keras-contrib Successfully installed keras-contrib-2.0.8 . from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization from keras.initializers import RandomNormal . define helper functions for the various components of the Model that we are going to build . Conv layers . def conv2d(input_, output_dim, ks=4, s=2, stddev=0.02, padding=&#39;SAME&#39;,name=&#39;c2d&#39;): return Conv2D(output_dim,kernel_size=ks,strides=s,padding=padding,kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),name=name)(input_) . Leaky Relu . def lrelu(input_,name=&#39;lr&#39;): return LeakyReLU(alpha=0.2,name=name)(input_) . InstanceNormalization . def iNorm(input_,name=&#39;iNorm&#39;): return InstanceNormalization(axis=-1,name=name)(input_) . Discriminator Model . def build_discriminator(image_shape): # weight initialization #init = RandomNormal(stddev=0.02) # source image input in_image = Input(shape=image_shape) #C1 d1 = lrelu(conv2d(in_image,64,4,name=&#39;d_c1&#39;),&#39;lr1&#39; ) # C2 d2 = lrelu(iNorm(conv2d(d1,128,4,name=&#39;d_c2&#39;),&#39;iN2&#39;),&#39;lr2&#39;) # C3 d3 = lrelu(iNorm(conv2d(d1,256,4,name=&#39;d_c3&#39;),&#39;iN3&#39;),&#39;lr3&#39;) # C4 d4 = lrelu(iNorm(conv2d(d3,512,4,name=&#39;d_c4&#39;),&#39;iN4&#39;),&#39;lr4&#39;) &#39;&#39;&#39; # second last output layer d = conv2d(in_image,128,3,1) d = iNorm(d) d = lrelu(d) &#39;&#39;&#39; # output d5 = conv2d(d4,1,4,1,name=&#39;d_c5&#39;) #Conv2D(1, 4,1, padding=&#39;same&#39;, kernel_initializer=init)(d) # define model model = Model(in_image, d5) # compile model model.compile(loss=&#39;mse&#39;, optimizer=Adam(lr=0.0002, beta_1=0.5), loss_weights=[0.5]) return model . disc=build_discriminator(dataB[0].shape) disc.summary() . WARNING: Logging before flag parsing goes to stderr. W0823 12:23:56.827917 140499755456384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead. W0823 12:23:56.872185 140499755456384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. W0823 12:23:57.044422 140499755456384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. . _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 128, 128, 3) 0 _________________________________________________________________ d_c1 (Conv2D) (None, 64, 64, 64) 3136 _________________________________________________________________ lr1 (LeakyReLU) (None, 64, 64, 64) 0 _________________________________________________________________ d_c3 (Conv2D) (None, 32, 32, 256) 262400 _________________________________________________________________ iN3 (InstanceNormalization) (None, 32, 32, 256) 512 _________________________________________________________________ lr3 (LeakyReLU) (None, 32, 32, 256) 0 _________________________________________________________________ d_c4 (Conv2D) (None, 16, 16, 512) 2097664 _________________________________________________________________ iN4 (InstanceNormalization) (None, 16, 16, 512) 1024 _________________________________________________________________ lr4 (LeakyReLU) (None, 16, 16, 512) 0 _________________________________________________________________ d_c5 (Conv2D) (None, 16, 16, 1) 8193 ================================================================= Total params: 2,372,929 Trainable params: 2,372,929 Non-trainable params: 0 _________________________________________________________________ . function to add padding . def padd3(input_): import tensorflow as tf return tf.pad(input_, [[0, 0], [3, 3], [3, 3], [0, 0]], &quot;REFLECT&quot;) def padd1(input_): import tensorflow as tf return tf.pad(input_, [[0, 0], [1, 1], [1, 1], [0, 0]], &quot;REFLECT&quot;) . The generator uses Resnet Blocks , as defined below . from keras.layers import Add,Lambda def res_block(input_,nf=64,ks=3,s=1,name=&#39;res_blk&#39;): p=int((ks-1)/2) y=Lambda(padd1)(input_) #(tf.pad(input_,[[0,0],[p,p],[p,p],[0,0]],&#39;REFLECT&#39;)) y=iNorm(conv2d(y,nf,ks,s,padding=&#39;VALID&#39;,name=name+&#39;_c1&#39;),name=name+&#39;_iN1&#39;) y=Lambda(padd1)(y) #(tf.pad(tf.nn.relu(y),[[0,0],[p,p],[p,p],[0,0]],&#39;REFLECT&#39;)) y=iNorm(conv2d(y,nf,ks,s,padding=&#39;VALID&#39;,name=name+&#39;_c2&#39;),name=name+&#39;_iN2&#39;) y1=keras.layers.Add()([y,input_]) return y1 . deconvolution layers . def deconv2d(input_, output_dim, ks=4, s=2, stddev=0.02, padding=&#39;SAME&#39;,name=&#39;dc2d&#39;): #Conv2DTranspose(64, (3,3), strides=(2,2), padding=&#39;same&#39;, kernel_initializer=init)(g) dcv=Conv2DTranspose(output_dim,(ks,ks),strides=(s,s),padding=padding,kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),name=name)(input_) return dcv . generator model . from keras.layers import Lambda,Conv2DTranspose def build_generator(image_shape): nf=64 # num filters for first layer input_=Input(shape=(128,128,3)) c0 = Lambda(padd3)(input_) c1 = Activation(&#39;relu&#39;)(iNorm(conv2d(c0, nf, 7, 1, padding=&#39;VALID&#39;, name=&#39;g_e1_c&#39;), &#39;g_e1_bn&#39;)) c2 = Activation(&#39;relu&#39;)(iNorm(conv2d(c1, nf*2, 3, 2, name=&#39;g_e2_c&#39;), &#39;g_e2_bn&#39;)) c3 = Activation(&#39;relu&#39;)(iNorm(conv2d(c2, nf*4 , 3, 2, name=&#39;g_e3_c&#39;), &#39;g_e3_bn&#39;)) r1 = res_block(c3, nf*4, name=&#39;g_r1&#39;) r2 = res_block(r1, nf*4, name=&#39;g_r2&#39;) r3 = res_block(r2, nf*4, name=&#39;g_r3&#39;) r4 = res_block(r3, nf*4, name=&#39;g_r4&#39;) r5 = res_block(r4, nf*4, name=&#39;g_r5&#39;) r6 = res_block(r5, nf*4, name=&#39;g_r6&#39;) r7 = res_block(r6, nf*4, name=&#39;g_r7&#39;) r8 = res_block(r7, nf*4, name=&#39;g_r8&#39;) r9 = res_block(r8, nf*4, name=&#39;g_r9&#39;) d1=Conv2DTranspose(nf*2, (3,3), strides=(2,2), padding=&#39;same&#39;, kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),name=&#39;g_d1_dc&#39;)(r9) d1=Activation(&#39;relu&#39;)(iNorm(d1,name=&#39;g_d1_bn&#39;)) d2=Conv2DTranspose(nf, (3,3), strides=(2,2), padding=&#39;same&#39;, kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),name=&#39;g_d2_dc&#39;)(d1) d2=Activation(&#39;relu&#39;)(iNorm(d2,name=&#39;g_d2_bn&#39;)) d2 = Lambda(padd3)(d2)#(tf.pad(d2, [[0, 0], [3, 3], [3, 3], [0, 0]], &quot;REFLECT&quot;)) d3=conv2d(d2, 3 , 7, 1, padding=&#39;VALID&#39;, name=&#39;g_pred_c&#39;) pred=Activation(&#39;tanh&#39;)(d3) model=Model(input_,pred) return model . gen=build_generator(dataA[0].shape) gen.summary() . __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_3 (InputLayer) (None, 128, 128, 3) 0 __________________________________________________________________________________________________ lambda_20 (Lambda) (None, 134, 134, 3) 0 input_3[0][0] __________________________________________________________________________________________________ g_e1_c (Conv2D) (None, 128, 128, 64) 9472 lambda_20[0][0] __________________________________________________________________________________________________ g_e1_bn (InstanceNormalization) (None, 128, 128, 64) 128 g_e1_c[0][0] __________________________________________________________________________________________________ activation_4 (Activation) (None, 128, 128, 64) 0 g_e1_bn[0][0] __________________________________________________________________________________________________ g_e2_c (Conv2D) (None, 64, 64, 128) 73856 activation_4[0][0] __________________________________________________________________________________________________ g_e2_bn (InstanceNormalization) (None, 64, 64, 128) 256 g_e2_c[0][0] __________________________________________________________________________________________________ activation_5 (Activation) (None, 64, 64, 128) 0 g_e2_bn[0][0] __________________________________________________________________________________________________ g_e3_c (Conv2D) (None, 32, 32, 256) 295168 activation_5[0][0] __________________________________________________________________________________________________ g_e3_bn (InstanceNormalization) (None, 32, 32, 256) 512 g_e3_c[0][0] __________________________________________________________________________________________________ activation_6 (Activation) (None, 32, 32, 256) 0 g_e3_bn[0][0] __________________________________________________________________________________________________ lambda_21 (Lambda) (None, 34, 34, 256) 0 activation_6[0][0] __________________________________________________________________________________________________ g_r1_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_21[0][0] __________________________________________________________________________________________________ g_r1_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r1_c1[0][0] __________________________________________________________________________________________________ lambda_22 (Lambda) (None, 34, 34, 256) 0 g_r1_iN1[0][0] __________________________________________________________________________________________________ g_r1_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_22[0][0] __________________________________________________________________________________________________ g_r1_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r1_c2[0][0] __________________________________________________________________________________________________ add_10 (Add) (None, 32, 32, 256) 0 g_r1_iN2[0][0] activation_6[0][0] __________________________________________________________________________________________________ lambda_23 (Lambda) (None, 34, 34, 256) 0 add_10[0][0] __________________________________________________________________________________________________ g_r2_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_23[0][0] __________________________________________________________________________________________________ g_r2_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r2_c1[0][0] __________________________________________________________________________________________________ lambda_24 (Lambda) (None, 34, 34, 256) 0 g_r2_iN1[0][0] __________________________________________________________________________________________________ g_r2_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_24[0][0] __________________________________________________________________________________________________ g_r2_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r2_c2[0][0] __________________________________________________________________________________________________ add_11 (Add) (None, 32, 32, 256) 0 g_r2_iN2[0][0] add_10[0][0] __________________________________________________________________________________________________ lambda_25 (Lambda) (None, 34, 34, 256) 0 add_11[0][0] __________________________________________________________________________________________________ g_r3_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_25[0][0] __________________________________________________________________________________________________ g_r3_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r3_c1[0][0] __________________________________________________________________________________________________ lambda_26 (Lambda) (None, 34, 34, 256) 0 g_r3_iN1[0][0] __________________________________________________________________________________________________ g_r3_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_26[0][0] __________________________________________________________________________________________________ g_r3_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r3_c2[0][0] __________________________________________________________________________________________________ add_12 (Add) (None, 32, 32, 256) 0 g_r3_iN2[0][0] add_11[0][0] __________________________________________________________________________________________________ lambda_27 (Lambda) (None, 34, 34, 256) 0 add_12[0][0] __________________________________________________________________________________________________ g_r4_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_27[0][0] __________________________________________________________________________________________________ g_r4_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r4_c1[0][0] __________________________________________________________________________________________________ lambda_28 (Lambda) (None, 34, 34, 256) 0 g_r4_iN1[0][0] __________________________________________________________________________________________________ g_r4_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_28[0][0] __________________________________________________________________________________________________ g_r4_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r4_c2[0][0] __________________________________________________________________________________________________ add_13 (Add) (None, 32, 32, 256) 0 g_r4_iN2[0][0] add_12[0][0] __________________________________________________________________________________________________ lambda_29 (Lambda) (None, 34, 34, 256) 0 add_13[0][0] __________________________________________________________________________________________________ g_r5_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_29[0][0] __________________________________________________________________________________________________ g_r5_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r5_c1[0][0] __________________________________________________________________________________________________ lambda_30 (Lambda) (None, 34, 34, 256) 0 g_r5_iN1[0][0] __________________________________________________________________________________________________ g_r5_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_30[0][0] __________________________________________________________________________________________________ g_r5_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r5_c2[0][0] __________________________________________________________________________________________________ add_14 (Add) (None, 32, 32, 256) 0 g_r5_iN2[0][0] add_13[0][0] __________________________________________________________________________________________________ lambda_31 (Lambda) (None, 34, 34, 256) 0 add_14[0][0] __________________________________________________________________________________________________ g_r6_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_31[0][0] __________________________________________________________________________________________________ g_r6_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r6_c1[0][0] __________________________________________________________________________________________________ lambda_32 (Lambda) (None, 34, 34, 256) 0 g_r6_iN1[0][0] __________________________________________________________________________________________________ g_r6_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_32[0][0] __________________________________________________________________________________________________ g_r6_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r6_c2[0][0] __________________________________________________________________________________________________ add_15 (Add) (None, 32, 32, 256) 0 g_r6_iN2[0][0] add_14[0][0] __________________________________________________________________________________________________ lambda_33 (Lambda) (None, 34, 34, 256) 0 add_15[0][0] __________________________________________________________________________________________________ g_r7_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_33[0][0] __________________________________________________________________________________________________ g_r7_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r7_c1[0][0] __________________________________________________________________________________________________ lambda_34 (Lambda) (None, 34, 34, 256) 0 g_r7_iN1[0][0] __________________________________________________________________________________________________ g_r7_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_34[0][0] __________________________________________________________________________________________________ g_r7_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r7_c2[0][0] __________________________________________________________________________________________________ add_16 (Add) (None, 32, 32, 256) 0 g_r7_iN2[0][0] add_15[0][0] __________________________________________________________________________________________________ lambda_35 (Lambda) (None, 34, 34, 256) 0 add_16[0][0] __________________________________________________________________________________________________ g_r8_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_35[0][0] __________________________________________________________________________________________________ g_r8_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r8_c1[0][0] __________________________________________________________________________________________________ lambda_36 (Lambda) (None, 34, 34, 256) 0 g_r8_iN1[0][0] __________________________________________________________________________________________________ g_r8_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_36[0][0] __________________________________________________________________________________________________ g_r8_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r8_c2[0][0] __________________________________________________________________________________________________ add_17 (Add) (None, 32, 32, 256) 0 g_r8_iN2[0][0] add_16[0][0] __________________________________________________________________________________________________ lambda_37 (Lambda) (None, 34, 34, 256) 0 add_17[0][0] __________________________________________________________________________________________________ g_r9_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_37[0][0] __________________________________________________________________________________________________ g_r9_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r9_c1[0][0] __________________________________________________________________________________________________ lambda_38 (Lambda) (None, 34, 34, 256) 0 g_r9_iN1[0][0] __________________________________________________________________________________________________ g_r9_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_38[0][0] __________________________________________________________________________________________________ g_r9_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r9_c2[0][0] __________________________________________________________________________________________________ add_18 (Add) (None, 32, 32, 256) 0 g_r9_iN2[0][0] add_17[0][0] __________________________________________________________________________________________________ g_d1_dc (Conv2DTranspose) (None, 64, 64, 128) 295040 add_18[0][0] __________________________________________________________________________________________________ g_d1_bn (InstanceNormalization) (None, 64, 64, 128) 256 g_d1_dc[0][0] __________________________________________________________________________________________________ activation_7 (Activation) (None, 64, 64, 128) 0 g_d1_bn[0][0] __________________________________________________________________________________________________ g_d2_dc (Conv2DTranspose) (None, 128, 128, 64) 73792 activation_7[0][0] __________________________________________________________________________________________________ g_d2_bn (InstanceNormalization) (None, 128, 128, 64) 128 g_d2_dc[0][0] __________________________________________________________________________________________________ activation_8 (Activation) (None, 128, 128, 64) 0 g_d2_bn[0][0] __________________________________________________________________________________________________ lambda_39 (Lambda) (None, 134, 134, 64) 0 activation_8[0][0] __________________________________________________________________________________________________ g_pred_c (Conv2D) (None, 128, 128, 3) 9411 lambda_39[0][0] __________________________________________________________________________________________________ activation_9 (Activation) (None, 128, 128, 3) 0 g_pred_c[0][0] ================================================================================================== Total params: 11,388,675 Trainable params: 11,388,675 Non-trainable params: 0 __________________________________________________________________________________________________ . composite Model with two genartors and discriminator . def build_composite_model(g_model_1, d_model, g_model_2, image_shape): # ensure the model we&#39;re updating is trainable g_model_1.trainable = True # mark discriminator as not trainable d_model.trainable = False # mark other generator model as not trainable g_model_2.trainable = False # discriminator element input_gen = Input(shape=image_shape) gen1_out = g_model_1(input_gen) output_d = d_model(gen1_out) # identity element input_id = Input(shape=image_shape) output_id = g_model_1(input_id) # forward cycle output_f = g_model_2(gen1_out) # backward cycle gen2_out = g_model_2(input_id) output_b = g_model_1(gen2_out) # define model graph model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b]) # define optimization algorithm configuration opt = Adam(lr=0.0002, beta_1=0.5) # compile model with weighting of least squares loss and L1 loss model.compile(loss=[&#39;mse&#39;, &#39;mae&#39;, &#39;mae&#39;, &#39;mae&#39;], loss_weights=[1, 5, 10, 10], optimizer=opt) return model . The original samples are over 3100 per domain and it is increasing the time for each epoch(has proven problematic in the initial training runs). So we will use a function to get a subsample of the training data , 1000 per Domain . def get_subsample(dataset): t1=np.random.randint(900) t2=np.random.randint(1200,2000) t3=np.random.randint(2500,2800) return np.vstack((dataset[0][t1:t1+300],dataset[0][t2:t2+400],dataset[0][t3:t3+300])),np.vstack((dataset[1][t1:t1+300], dataset[1][t2:t2+400],dataset[1][t3:t3+300])) . def get_subsample2(dataset): t0=np.random.randint(250) t1=np.random.randint(300) t2=np.random.randint(1200,2000) t3=np.random.randint(2500,2800) return np.vstack((dataset[0][t1:t1+800],dataset[0][t2:t2+200])),np.vstack((dataset[1][t1:t1+100], dataset[2][t0:t0+900])) . Utility Functions to load the image samples , generate fake images , save Models , Save genrated images , etc . def load_real_samples2(filename): data = load(filename) X1,X2,X3 = data[&#39;arr_0&#39;],data[&#39;arr_1&#39;],data[&#39;arr_2&#39;] X1= (X1-127.5)/127.5 X2 = (X2-127.5)/127.5 X3 = (X3-127.5)/127.5 return X1,X2,X3 . def load_real_samples(filename): # load the dataset data = load(filename) # unpack arrays X1, X2 = data[&#39;arr_0&#39;], data[&#39;arr_1&#39;] # scale from [0,255] to [-1,1] X1 = (X1 - 127.5) / 127.5 X2 = (X2 - 127.5) / 127.5 return [X1, X2] # select a batch of random samples, returns images and target def generate_real_samples(dataset, n_samples, patch_shape): # choose random instances ix = randint(0, dataset.shape[0], n_samples) # retrieve selected images X = dataset[ix] # generate &#39;real&#39; class labels (1) y = ones((n_samples, patch_shape, patch_shape, 1)) return X, y # generate a batch of images, returns images and targets def generate_fake_samples(g_model, dataset, patch_shape): # generate fake instance X = g_model.predict(dataset) # create &#39;fake&#39; class labels (0) y = zeros((len(X), patch_shape, patch_shape, 1)) return X, y # save the generator models to file def save_models(step, g_model_AtoB, g_model_BtoA): # save the first generator model filename1 = &#39;/content/drive/My Drive/EIP3/session7/g_model_AtoB_%06d.h5&#39; % (step+1) g_model_AtoB.save(filename1) # save the second generator model filename2 = &#39;/content/drive/My Drive/EIP3/session7/g_model_BtoA_%06d.h5&#39; % (step+1) g_model_BtoA.save(filename2) print(&#39;&gt;Saved: %s and %s&#39; % (filename1, filename2)) # save the generator models to file def save_models2(step, g_model_AtoB, g_model_BtoA,d_model_A,d_model_B): # save the first generator model filename1 = &#39;/content/drive/My Drive/EIP3/session7/g_model_AtoB_%06d.h5&#39; % (step+1) g_model_AtoB.save(filename1) # save the second generator model filename2 = &#39;/content/drive/My Drive/EIP3/session7/g_model_BtoA_%06d.h5&#39; % (step+1) g_model_BtoA.save(filename2) # save the first discriminator model A filename3 = &#39;/content/drive/My Drive/EIP3/session7/d_model_A_%06d.h5&#39; % (step+1) d_model_A.save(filename3) # save the first discriminator model B filename4 = &#39;/content/drive/My Drive/EIP3/session7/d_model_B_%06d.h5&#39; % (step+1) d_model_B.save(filename4) print(&#39;&gt;Saved: %s , %s , %s and %s&#39; % (filename1, filename2,filename3,filename4)) . def summarize_performance(step, g_model, trainX, name, n_samples=5): pyplot.figure( figsize=(15, 8), dpi=120) # select a sample of input images X_in, _ = generate_real_samples(trainX, n_samples, 0) # generate translated images X_out, _ = generate_fake_samples(g_model, X_in, 0) # scale all pixels from [-1,1] to [0,1] X_in = (X_in + 1) / 2.0 X_out = (X_out + 1) / 2.0 # plot real images for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(X_in[i]) # plot translated image for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + n_samples + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(X_out[i]) # save plot to file filename1 = &#39;/content/drive/My Drive/EIP3/session7/%s_generated_plot_%06d.png&#39; % (name, (step+1)) pyplot.savefig(filename1) pyplot.close() . Maintain a pool of 50 images as described in the paper . def update_image_pool(pool, images, max_size=50): selected = list() for image in images: if len(pool) &lt; max_size: # stock the pool pool.append(image) selected.append(image) elif random() &lt; 0.5: # use image, but don&#39;t add it to the pool selected.append(image) else: # replace an existing image and use replaced image ix = randint(0, len(pool)) selected.append(pool[ix]) pool[ix] = image return asarray(selected) . function to run the training . def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size,n_epochs): # define properties of the training run n_epochs, n_batch, = n_epochs, batch_size # determine the output square shape of the discriminator n_patch = d_model_A.output_shape[1] # unpack dataset trainA, trainB = get_subsample(dataset) # prepare image pool for fakes poolA, poolB = list(), list() # calculate the number of batches per training epoch bat_per_epo = int(len(trainA) / n_batch) # calculate the number of training iterations n_steps = bat_per_epo * n_epochs # manually enumerate epochs for i in range(n_steps): # select a batch of real samples X_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch) X_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch) # generate a batch of fake samples X_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch) X_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch) # update fakes from pool X_fakeA = update_image_pool(poolA, X_fakeA) X_fakeB = update_image_pool(poolB, X_fakeB) # update generator B-&gt;A via adversarial and cycle loss g_loss2, _, _, _, _ = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA]) # update discriminator for A -&gt; [real/fake] dA_loss1 = d_model_A.train_on_batch(X_realA, y_realA) dA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA) # update generator A-&gt;B via adversarial and cycle loss g_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB]) # update discriminator for B -&gt; [real/fake] dB_loss1 = d_model_B.train_on_batch(X_realB, y_realB) dB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB) # summarize performance print(&#39;&gt;%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]&#39; % (i+1, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2)) # evaluate the model performance every so often if (i+1) % (bat_per_epo * 1) == 0: # plot A-&gt;B translation summarize_performance(i, g_model_AtoB, trainA, &#39;AtoB&#39;) # plot B-&gt;A translation summarize_performance(i, g_model_BtoA, trainB, &#39;BtoA&#39;) if (i+1) % (bat_per_epo * 5) == 0: # save the models save_models2(i, g_model_AtoB, g_model_BtoA,d_model_A,d_model_B) . define the models and run training . from random import random from numpy import load from numpy import zeros from numpy import ones from numpy import asarray from numpy.random import randint from keras.optimizers import Adam # load image data dataset = load_real_samples(&#39;/content/drive/My Drive/EIP3/session7/utkface_128.npz&#39;) print(&#39;Loaded&#39;, dataset[0].shape, dataset[1].shape) # define input shape based on the loaded dataset image_shape = dataset[0].shape[1:] # generator: A -&gt; B g_model_AtoB = build_generator(image_shape) # generator: B -&gt; A g_model_BtoA = build_generator(image_shape) # discriminator: A -&gt; [real/fake] d_model_A = build_discriminator(image_shape) # discriminator: B -&gt; [real/fake] d_model_B = build_discriminator(image_shape) # composite: A -&gt; B -&gt; [real/fake, A] c_model_AtoB = build_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape) # composite: B -&gt; A -&gt; [real/fake, B] c_model_BtoA = build_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape) # train models train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=2,n_epochs=10) . # load image data dataset = load_real_samples(&#39;/content/drive/My Drive/EIP3/session7/utkface_128.npz&#39;) print(&#39;Loaded&#39;, dataset[0].shape, dataset[1].shape) # define input shape based on the loaded dataset image_shape = dataset[0].shape[1:] #load the previously trained model cust = {&#39;InstanceNormalization&#39;: InstanceNormalization, &#39;tf&#39;: tf} # generator: A -&gt; B g_model_AtoB = load_model(&#39;/content/drive/My Drive/EIP3/session7/g_model_AtoB_005625.h5&#39;, cust) # generator: B -&gt; A g_model_BtoA = load_model(&#39;/content/drive/My Drive/EIP3/session7/g_model_BtoA_005625.h5&#39;, cust) # discriminator: A -&gt; [real/fake] d_model_A = build_discriminator(image_shape) # discriminator: B -&gt; [real/fake] d_model_B = build_discriminator(image_shape) # composite: A -&gt; B -&gt; [real/fake, A] c_model_AtoB = build_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape) # composite: B -&gt; A -&gt; [real/fake, B] c_model_BtoA = build_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape) . Run time disconnected and session ended , so load last saved models and continue training . Increase the batch size and reduce the sample size too with get_subsample utility function . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=5) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=5) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=5) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=10) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=10) . dataset = load_real_samples2(&#39;/content/drive/My Drive/EIP3/session7/utkface_128_2.npz&#39;) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=20) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=20) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=20) . dataset = load_real_samples(&#39;/content/drive/My Drive/EIP3/session7/utkface_128.npz&#39;) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=10) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=1) . We have trained the model for slighly over 100 epochs . Although more epochs will give better results, we stop here due to time constraints . We will try out the results of this training . def show_results( g_model, trainX, n_samples=5,title=&#39;A to B&#39;): pyplot.figure( figsize=(12, 6), dpi=120) # select a sample of input images X_in, _ = generate_real_samples(trainX, n_samples, 0) # generate translated images X_out, _ = generate_fake_samples(g_model, X_in, 0) # scale all pixels from [-1,1] to [0,1] X_in = (X_in + 1) / 2.0 X_out = (X_out + 1) / 2.0 # plot real images #pyplot.title(title) for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(X_in[i]) pyplot.show() print(&quot; ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ &quot;+title+&quot; ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓&quot;) # plot translated image pyplot.figure( figsize=(12, 6), dpi=120) for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + n_samples + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(X_out[i]) pyplot.show() . domain A to Domain B generation results : Young to Old . trainA, trainB = dataset show_results(g_model_AtoB, trainA, 5, &#39;YOUNG TO OLD&#39;) . ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ YOUNG TO OLD ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ . show_results(g_model_AtoB, trainA, 5, &#39;YOUNG TO OLD&#39;) . ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ YOUNG TO OLD ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ . show_results(g_model_AtoB, trainA, 5, &#39;YOUNG TO OLD&#39;) . ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ YOUNG TO OLD ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ . domain B to Domain A generation results : Old to Young . show_results(g_model_BtoA, trainB ,5, &#39; Old to Young &#39;) . ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Old to Young ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ . show_results(g_model_BtoA, trainB ,5, &#39; Old to Young &#39;) . ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Old to Young ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ . show_results(g_model_BtoA, trainB ,5, &#39; Old to Young &#39;) . ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Old to Young ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ . show_results(g_model_BtoA, trainB ,5, &#39; Old to Young &#39;) . ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Old to Young ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ . Training the models for more epochs will make the results better, especially for Young to Old Translation . Also we used 128x128 images due to time and compute constraints . Training on the original 200x200 image size would have yielded better results .",
            "url": "https://ravindrabharathi.github.io/blog/dl/cyclegan/gan/2020/04/01/Face-Aging-Cycle-GANs.html",
            "relUrl": "/dl/cyclegan/gan/2020/04/01/Face-Aging-Cycle-GANs.html",
            "date": " • Apr 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ravindrabharathi.github.io/blog/jupyter/2020/02/22/test2.html",
            "relUrl": "/jupyter/2020/02/22/test2.html",
            "date": " • Feb 22, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ravindrabharathi.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": ". About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ravindrabharathi.github.io/blog/jupyter/2020/02/02/test4.html",
            "relUrl": "/jupyter/2020/02/02/test4.html",
            "date": " • Feb 2, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ravindrabharathi.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ravindrabharathi.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ravindrabharathi.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}