{
  
    
        "post0": {
            "title": "Face Aging using CycleGANs",
            "content": ". CycleGANs were introduced in this paper titled Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks where the authors presented an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. . For the images of faces with various ages we will be using the UTKFace dataset wich has a cropped image set of only faces marked with age , gender , race , etc. . We will be using following two good references that use CycleGAN in order to build and train our models . https://github.com/sungnam0/Face-Aging-with-CycleGAN 2.https://machinelearningmastery.com/cyclegan-tutorial-with-keras/ | import necessary modules . import numpy as np import keras import tensorflow as tf from keras.layers import Input,Conv2D,MaxPooling2D,Flatten,Activation,BatchNormalization,K,UpSampling2D from keras.layers import Dropout,GlobalAveragePooling2D,LeakyReLU,Dense,Reshape, concatenate,Conv2DTranspose from keras.models import Model,load_model import matplotlib.pyplot as plt #import keras.backend as K import os import time from datetime import datetime from keras.applications import InceptionResNetV2 from keras.callbacks import TensorBoard from keras.optimizers import Adam from keras.utils import to_categorical from keras_preprocessing import image from numpy import asarray from numpy import vstack from keras.preprocessing.image import img_to_array from keras.preprocessing.image import load_img from numpy import savez_compressed import pandas as pd import os from matplotlib import pyplot from numpy import load from random import random from numpy import load from numpy import zeros from numpy import ones from numpy import asarray from numpy.random import randint from keras.optimizers import Adam . set tf backend config to allocate memory as needed instead of pre-allocating . config = tf.ConfigProto() config.gpu_options.allow_growth = True # Create a session with the above options specified. keras.backend.tensorflow_backend.set_session(tf.Session(config=config)) . mount google drive . from google.colab import drive drive.mount(&#39;/content/drive&#39;, force_remount=True) . Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&amp;response_type=code Enter your authorization code: ·········· Mounted at /content/drive . !ls -al &#39;/content/drive/My Drive/FaceGAN/&#39; . total 105354 drwx 2 root root 4096 Aug 22 10:59 results -rw- 1 root root 1239467 Aug 22 09:56 utk_data.csv drwx 2 root root 4096 Aug 21 08:41 UTKFace -rw- 1 root root 106634631 Aug 21 07:15 UTKFace.tar.gz . get the UTKFace dataset . !tar zxf &#39;/content/drive/My Drive/FaceGAN/UTKFace.tar.gz&#39; UTKFace . Parse the data . The labels of each face image is embedded in the file name, formated like [age][gender][race]_[date&amp;time].jpg . [age] is an integer from 0 to 116, indicating the age [gender] is either 0 (male) or 1 (female) [race] is an integer from 0 to 4, denoting White, Black, Asian, Indian, and Others (like Hispanic, Latino, Middle Eastern). [date&amp;time] is in the format of yyyymmddHHMMSSFFF, showing the date and time an image was collected to UTKFace . data=[] for filename in os.listdir(&#39;./UTKFace&#39;): parts=filename.split(&#39;_&#39;) #print(parts[3]) item={} item[&#39;image&#39;]=filename item[&#39;age&#39;]=parts[0] item[&#39;gender&#39;]=parts[1] item[&#39;race&#39;]=parts[2] if (len(parts)==4): item[&#39;date_time&#39;]=parts[3] data.append(item) utk_data=pd.DataFrame(data) utk_data.describe() . age date_time gender image race . count 23708 | 23705 | 23708 | 23708 | 23708 | . unique 104 | 23479 | 2 | 23708 | 8 | . top 26 | 20170110173815028.jpg.chip.jpg | 0 | 26_1_0_20170112213001988.jpg.chip.jpg | 0 | . freq 2197 | 7 | 12391 | 1 | 10078 | . utk_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 23708 entries, 0 to 23707 Data columns (total 5 columns): age 23708 non-null object date_time 23705 non-null object gender 23708 non-null object image 23708 non-null object race 23708 non-null object dtypes: object(5) memory usage: 926.2+ KB . utk_data.head() . age date_time gender image race . 0 25 | 20170119172104288.jpg.chip.jpg | 1 | 25_1_3_20170119172104288.jpg.chip.jpg | 3 | . 1 25 | 20170117141726361.jpg.chip.jpg | 1 | 25_1_0_20170117141726361.jpg.chip.jpg | 0 | . 2 27 | 20170116001407357.jpg.chip.jpg | 0 | 27_0_1_20170116001407357.jpg.chip.jpg | 1 | . 3 10 | 20170103200501766.jpg.chip.jpg | 0 | 10_0_4_20170103200501766.jpg.chip.jpg | 4 | . 4 26 | 20170116184024662.jpg.chip.jpg | 1 | 26_1_0_20170116184024662.jpg.chip.jpg | 0 | . we do not need date time , so delete it . del utk_data[&#39;date_time&#39;] . define a category for ages and apply it to the dataset . def age_cat_fn(age): age=int(age) if (0&lt;age&lt;18): return 0 elif(18&lt;=age&lt;=25): return 1 elif (25&lt;age&lt;=39): return 2 elif (39&lt; age &lt;=49): return 3 elif (49 &lt; age &lt;=60): return 4 elif age&gt;60: return 5 . utk_data[&#39;age_cat&#39;]=utk_data.age.map(age_cat_fn) . utk_data.to_csv(&#39;utk_data.csv&#39;,sep=&#39;,&#39;) !cp &#39;utk_data.csv&#39; &#39;/content/drive/My Drive/EIP3/session7&#39; . data with age category . utk_data.head() . age gender image race age_cat . 0 25 | 1 | 25_1_3_20170119172104288.jpg.chip.jpg | 3 | 1 | . 1 25 | 1 | 25_1_0_20170117141726361.jpg.chip.jpg | 0 | 1 | . 2 27 | 0 | 27_0_1_20170116001407357.jpg.chip.jpg | 1 | 2 | . 3 10 | 0 | 10_0_4_20170103200501766.jpg.chip.jpg | 4 | 0 | . 4 26 | 1 | 26_1_0_20170116184024662.jpg.chip.jpg | 0 | 2 | . split our data into two Domains . Young : age category 1 | Old : Age category 4 | data_A=utk_data[utk_data.age_cat==1] data_B=utk_data[utk_data.age_cat==4] . print(data_A[&#39;age_cat&#39;].count()) print(data_B[&#39;age_cat&#39;].count()) . 3404 2592 . #import os #utk_data=pd.read_csv(&#39;/content/drive/My Drive/EIP3/session7/utk_data.csv&#39;) . get the iames belonging to the two Domains and save as a compressed numpy array so that we can load them when necesary instead of processing the UTKFace dataset multiple times . image_dir=&#39;./UTKFace/&#39; image_paths_A = data_A[&#39;image&#39;].tolist() image_paths_B = data_B[&#39;image&#39;].tolist() #print(image_paths[:10]) . images_A=None images_B=None #store 2000 images for A for i, image_path in enumerate(image_paths_A): if (i&lt;2000): if (i%1000==0): print(&quot;processing set A image num &quot;+str(i)) try: # Load image loaded_image = image.load_img(image_dir+image_path, target_size=(128,128,3)) # Convert PIL image to numpy ndarray loaded_image = image.img_to_array(loaded_image) # Add another dimension (Add batch dimension) loaded_image = np.expand_dims(loaded_image, axis=0) # Concatenate all images into one tensor if images_A is None: images_A = loaded_image else: images_A = np.concatenate([images_A, loaded_image], axis=0) except Exception as e: print(&quot;Error:&quot;, i, e) #store 2000 images for B for i, image_path in enumerate(image_paths_B): if (i&lt;2000): if (i%999==0): print(&quot;processing set B image num &quot;+str(i)) try: # Load image loaded_image = image.load_img(image_dir+image_path, target_size=(128,128,3)) # Convert PIL image to numpy ndarray loaded_image = image.img_to_array(loaded_image) # Add another dimension (Add batch dimension) loaded_image = np.expand_dims(loaded_image, axis=0) # Concatenate all images into one tensor if images_B is None: images_B = loaded_image else: images_B = np.concatenate([images_B, loaded_image], axis=0) except Exception as e: print(&quot;Error:&quot;, i, e) . processing set A image num 0 processing set A image num 1000 processing set B image num 0 processing set B image num 999 processing set B image num 1998 . print(&#39;images_A :&#39;) print(images_A.shape) print(&#39;images_B :&#39;) print(images_B.shape) . images_A : (2000, 128, 128, 3) images_B : (2000, 128, 128, 3) . filename = &#39;/content/drive/My Drive/EIP3/session7/utkface_128.npz&#39; savez_compressed(filename, images_A, images_B) print(&#39;Saved dataset: &#39;, filename) . Saved dataset: /content/drive/My Drive/EIP3/session7/utkface_128.npz . load the saved numpy arrays and plot some images from either domain . from numpy import load from matplotlib import pyplot # load the dataset data = load(&#39;/content/drive/My Drive/EIP3/session7/utkface_128.npz&#39;) dataA, dataB = data[&#39;arr_0&#39;], data[&#39;arr_1&#39;] print(&#39;Loaded: &#39;, dataA.shape, dataB.shape) # plot source images n_samples = 3 for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(dataA[i].astype(&#39;uint8&#39;)) # plot target image for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + n_samples + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(dataB[i].astype(&#39;uint8&#39;)) pyplot.show() . Loaded: (2000, 128, 128, 3) (2000, 128, 128, 3) . install keras-contrib so that we can use InstanceNormalization instead of BatchNormalization . !pip install git+https://www.github.com/keras-team/keras-contrib.git . Collecting git+https://www.github.com/keras-team/keras-contrib.git Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-225cusg3 Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-225cusg3 Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-contrib==2.0.8) (2.2.4) Requirement already satisfied: keras-preprocessing&gt;=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras-&gt;keras-contrib==2.0.8) (1.1.0) Requirement already satisfied: keras-applications&gt;=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras-&gt;keras-contrib==2.0.8) (1.0.8) Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras-&gt;keras-contrib==2.0.8) (3.13) Requirement already satisfied: six&gt;=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras-&gt;keras-contrib==2.0.8) (1.12.0) Requirement already satisfied: scipy&gt;=0.14 in /usr/local/lib/python3.6/dist-packages (from keras-&gt;keras-contrib==2.0.8) (1.3.1) Requirement already satisfied: numpy&gt;=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras-&gt;keras-contrib==2.0.8) (1.16.4) Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-&gt;keras-contrib==2.0.8) (2.8.0) Building wheels for collected packages: keras-contrib Building wheel for keras-contrib (setup.py) ... done Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101066 sha256=4c2686ed2595b2fbcc1fd6df0f014cb70df688e3b37ff72e3fb6ef31dd35e615 Stored in directory: /tmp/pip-ephem-wheel-cache-rkeooafj/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba Successfully built keras-contrib Installing collected packages: keras-contrib Successfully installed keras-contrib-2.0.8 . from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization from keras.initializers import RandomNormal . define helper functions for the various components of the Model that we are going to build . Conv layers . def conv2d(input_, output_dim, ks=4, s=2, stddev=0.02, padding=&#39;SAME&#39;,name=&#39;c2d&#39;): return Conv2D(output_dim,kernel_size=ks,strides=s,padding=padding,kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),name=name)(input_) . Leaky Relu . def lrelu(input_,name=&#39;lr&#39;): return LeakyReLU(alpha=0.2,name=name)(input_) . InstanceNormalization . def iNorm(input_,name=&#39;iNorm&#39;): return InstanceNormalization(axis=-1,name=name)(input_) . Discriminator Model . def build_discriminator(image_shape): # weight initialization #init = RandomNormal(stddev=0.02) # source image input in_image = Input(shape=image_shape) #C1 d1 = lrelu(conv2d(in_image,64,4,name=&#39;d_c1&#39;),&#39;lr1&#39; ) # C2 d2 = lrelu(iNorm(conv2d(d1,128,4,name=&#39;d_c2&#39;),&#39;iN2&#39;),&#39;lr2&#39;) # C3 d3 = lrelu(iNorm(conv2d(d1,256,4,name=&#39;d_c3&#39;),&#39;iN3&#39;),&#39;lr3&#39;) # C4 d4 = lrelu(iNorm(conv2d(d3,512,4,name=&#39;d_c4&#39;),&#39;iN4&#39;),&#39;lr4&#39;) &#39;&#39;&#39; # second last output layer d = conv2d(in_image,128,3,1) d = iNorm(d) d = lrelu(d) &#39;&#39;&#39; # output d5 = conv2d(d4,1,4,1,name=&#39;d_c5&#39;) #Conv2D(1, 4,1, padding=&#39;same&#39;, kernel_initializer=init)(d) # define model model = Model(in_image, d5) # compile model model.compile(loss=&#39;mse&#39;, optimizer=Adam(lr=0.0002, beta_1=0.5), loss_weights=[0.5]) return model . disc=build_discriminator(dataB[0].shape) disc.summary() . WARNING: Logging before flag parsing goes to stderr. W0823 12:23:56.827917 140499755456384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead. W0823 12:23:56.872185 140499755456384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. W0823 12:23:57.044422 140499755456384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. . _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 128, 128, 3) 0 _________________________________________________________________ d_c1 (Conv2D) (None, 64, 64, 64) 3136 _________________________________________________________________ lr1 (LeakyReLU) (None, 64, 64, 64) 0 _________________________________________________________________ d_c3 (Conv2D) (None, 32, 32, 256) 262400 _________________________________________________________________ iN3 (InstanceNormalization) (None, 32, 32, 256) 512 _________________________________________________________________ lr3 (LeakyReLU) (None, 32, 32, 256) 0 _________________________________________________________________ d_c4 (Conv2D) (None, 16, 16, 512) 2097664 _________________________________________________________________ iN4 (InstanceNormalization) (None, 16, 16, 512) 1024 _________________________________________________________________ lr4 (LeakyReLU) (None, 16, 16, 512) 0 _________________________________________________________________ d_c5 (Conv2D) (None, 16, 16, 1) 8193 ================================================================= Total params: 2,372,929 Trainable params: 2,372,929 Non-trainable params: 0 _________________________________________________________________ . function to add padding . def padd3(input_): import tensorflow as tf return tf.pad(input_, [[0, 0], [3, 3], [3, 3], [0, 0]], &quot;REFLECT&quot;) def padd1(input_): import tensorflow as tf return tf.pad(input_, [[0, 0], [1, 1], [1, 1], [0, 0]], &quot;REFLECT&quot;) . The generator uses Resnet Blocks , as defined below . from keras.layers import Add,Lambda def res_block(input_,nf=64,ks=3,s=1,name=&#39;res_blk&#39;): p=int((ks-1)/2) y=Lambda(padd1)(input_) #(tf.pad(input_,[[0,0],[p,p],[p,p],[0,0]],&#39;REFLECT&#39;)) y=iNorm(conv2d(y,nf,ks,s,padding=&#39;VALID&#39;,name=name+&#39;_c1&#39;),name=name+&#39;_iN1&#39;) y=Lambda(padd1)(y) #(tf.pad(tf.nn.relu(y),[[0,0],[p,p],[p,p],[0,0]],&#39;REFLECT&#39;)) y=iNorm(conv2d(y,nf,ks,s,padding=&#39;VALID&#39;,name=name+&#39;_c2&#39;),name=name+&#39;_iN2&#39;) y1=keras.layers.Add()([y,input_]) return y1 . deconvolution layers . def deconv2d(input_, output_dim, ks=4, s=2, stddev=0.02, padding=&#39;SAME&#39;,name=&#39;dc2d&#39;): #Conv2DTranspose(64, (3,3), strides=(2,2), padding=&#39;same&#39;, kernel_initializer=init)(g) dcv=Conv2DTranspose(output_dim,(ks,ks),strides=(s,s),padding=padding,kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),name=name)(input_) return dcv . generator model . from keras.layers import Lambda,Conv2DTranspose def build_generator(image_shape): nf=64 # num filters for first layer input_=Input(shape=(128,128,3)) c0 = Lambda(padd3)(input_) c1 = Activation(&#39;relu&#39;)(iNorm(conv2d(c0, nf, 7, 1, padding=&#39;VALID&#39;, name=&#39;g_e1_c&#39;), &#39;g_e1_bn&#39;)) c2 = Activation(&#39;relu&#39;)(iNorm(conv2d(c1, nf*2, 3, 2, name=&#39;g_e2_c&#39;), &#39;g_e2_bn&#39;)) c3 = Activation(&#39;relu&#39;)(iNorm(conv2d(c2, nf*4 , 3, 2, name=&#39;g_e3_c&#39;), &#39;g_e3_bn&#39;)) r1 = res_block(c3, nf*4, name=&#39;g_r1&#39;) r2 = res_block(r1, nf*4, name=&#39;g_r2&#39;) r3 = res_block(r2, nf*4, name=&#39;g_r3&#39;) r4 = res_block(r3, nf*4, name=&#39;g_r4&#39;) r5 = res_block(r4, nf*4, name=&#39;g_r5&#39;) r6 = res_block(r5, nf*4, name=&#39;g_r6&#39;) r7 = res_block(r6, nf*4, name=&#39;g_r7&#39;) r8 = res_block(r7, nf*4, name=&#39;g_r8&#39;) r9 = res_block(r8, nf*4, name=&#39;g_r9&#39;) d1=Conv2DTranspose(nf*2, (3,3), strides=(2,2), padding=&#39;same&#39;, kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),name=&#39;g_d1_dc&#39;)(r9) d1=Activation(&#39;relu&#39;)(iNorm(d1,name=&#39;g_d1_bn&#39;)) d2=Conv2DTranspose(nf, (3,3), strides=(2,2), padding=&#39;same&#39;, kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),name=&#39;g_d2_dc&#39;)(d1) d2=Activation(&#39;relu&#39;)(iNorm(d2,name=&#39;g_d2_bn&#39;)) d2 = Lambda(padd3)(d2)#(tf.pad(d2, [[0, 0], [3, 3], [3, 3], [0, 0]], &quot;REFLECT&quot;)) d3=conv2d(d2, 3 , 7, 1, padding=&#39;VALID&#39;, name=&#39;g_pred_c&#39;) pred=Activation(&#39;tanh&#39;)(d3) model=Model(input_,pred) return model . gen=build_generator(dataA[0].shape) gen.summary() . __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_3 (InputLayer) (None, 128, 128, 3) 0 __________________________________________________________________________________________________ lambda_20 (Lambda) (None, 134, 134, 3) 0 input_3[0][0] __________________________________________________________________________________________________ g_e1_c (Conv2D) (None, 128, 128, 64) 9472 lambda_20[0][0] __________________________________________________________________________________________________ g_e1_bn (InstanceNormalization) (None, 128, 128, 64) 128 g_e1_c[0][0] __________________________________________________________________________________________________ activation_4 (Activation) (None, 128, 128, 64) 0 g_e1_bn[0][0] __________________________________________________________________________________________________ g_e2_c (Conv2D) (None, 64, 64, 128) 73856 activation_4[0][0] __________________________________________________________________________________________________ g_e2_bn (InstanceNormalization) (None, 64, 64, 128) 256 g_e2_c[0][0] __________________________________________________________________________________________________ activation_5 (Activation) (None, 64, 64, 128) 0 g_e2_bn[0][0] __________________________________________________________________________________________________ g_e3_c (Conv2D) (None, 32, 32, 256) 295168 activation_5[0][0] __________________________________________________________________________________________________ g_e3_bn (InstanceNormalization) (None, 32, 32, 256) 512 g_e3_c[0][0] __________________________________________________________________________________________________ activation_6 (Activation) (None, 32, 32, 256) 0 g_e3_bn[0][0] __________________________________________________________________________________________________ lambda_21 (Lambda) (None, 34, 34, 256) 0 activation_6[0][0] __________________________________________________________________________________________________ g_r1_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_21[0][0] __________________________________________________________________________________________________ g_r1_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r1_c1[0][0] __________________________________________________________________________________________________ lambda_22 (Lambda) (None, 34, 34, 256) 0 g_r1_iN1[0][0] __________________________________________________________________________________________________ g_r1_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_22[0][0] __________________________________________________________________________________________________ g_r1_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r1_c2[0][0] __________________________________________________________________________________________________ add_10 (Add) (None, 32, 32, 256) 0 g_r1_iN2[0][0] activation_6[0][0] __________________________________________________________________________________________________ lambda_23 (Lambda) (None, 34, 34, 256) 0 add_10[0][0] __________________________________________________________________________________________________ g_r2_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_23[0][0] __________________________________________________________________________________________________ g_r2_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r2_c1[0][0] __________________________________________________________________________________________________ lambda_24 (Lambda) (None, 34, 34, 256) 0 g_r2_iN1[0][0] __________________________________________________________________________________________________ g_r2_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_24[0][0] __________________________________________________________________________________________________ g_r2_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r2_c2[0][0] __________________________________________________________________________________________________ add_11 (Add) (None, 32, 32, 256) 0 g_r2_iN2[0][0] add_10[0][0] __________________________________________________________________________________________________ lambda_25 (Lambda) (None, 34, 34, 256) 0 add_11[0][0] __________________________________________________________________________________________________ g_r3_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_25[0][0] __________________________________________________________________________________________________ g_r3_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r3_c1[0][0] __________________________________________________________________________________________________ lambda_26 (Lambda) (None, 34, 34, 256) 0 g_r3_iN1[0][0] __________________________________________________________________________________________________ g_r3_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_26[0][0] __________________________________________________________________________________________________ g_r3_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r3_c2[0][0] __________________________________________________________________________________________________ add_12 (Add) (None, 32, 32, 256) 0 g_r3_iN2[0][0] add_11[0][0] __________________________________________________________________________________________________ lambda_27 (Lambda) (None, 34, 34, 256) 0 add_12[0][0] __________________________________________________________________________________________________ g_r4_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_27[0][0] __________________________________________________________________________________________________ g_r4_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r4_c1[0][0] __________________________________________________________________________________________________ lambda_28 (Lambda) (None, 34, 34, 256) 0 g_r4_iN1[0][0] __________________________________________________________________________________________________ g_r4_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_28[0][0] __________________________________________________________________________________________________ g_r4_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r4_c2[0][0] __________________________________________________________________________________________________ add_13 (Add) (None, 32, 32, 256) 0 g_r4_iN2[0][0] add_12[0][0] __________________________________________________________________________________________________ lambda_29 (Lambda) (None, 34, 34, 256) 0 add_13[0][0] __________________________________________________________________________________________________ g_r5_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_29[0][0] __________________________________________________________________________________________________ g_r5_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r5_c1[0][0] __________________________________________________________________________________________________ lambda_30 (Lambda) (None, 34, 34, 256) 0 g_r5_iN1[0][0] __________________________________________________________________________________________________ g_r5_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_30[0][0] __________________________________________________________________________________________________ g_r5_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r5_c2[0][0] __________________________________________________________________________________________________ add_14 (Add) (None, 32, 32, 256) 0 g_r5_iN2[0][0] add_13[0][0] __________________________________________________________________________________________________ lambda_31 (Lambda) (None, 34, 34, 256) 0 add_14[0][0] __________________________________________________________________________________________________ g_r6_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_31[0][0] __________________________________________________________________________________________________ g_r6_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r6_c1[0][0] __________________________________________________________________________________________________ lambda_32 (Lambda) (None, 34, 34, 256) 0 g_r6_iN1[0][0] __________________________________________________________________________________________________ g_r6_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_32[0][0] __________________________________________________________________________________________________ g_r6_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r6_c2[0][0] __________________________________________________________________________________________________ add_15 (Add) (None, 32, 32, 256) 0 g_r6_iN2[0][0] add_14[0][0] __________________________________________________________________________________________________ lambda_33 (Lambda) (None, 34, 34, 256) 0 add_15[0][0] __________________________________________________________________________________________________ g_r7_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_33[0][0] __________________________________________________________________________________________________ g_r7_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r7_c1[0][0] __________________________________________________________________________________________________ lambda_34 (Lambda) (None, 34, 34, 256) 0 g_r7_iN1[0][0] __________________________________________________________________________________________________ g_r7_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_34[0][0] __________________________________________________________________________________________________ g_r7_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r7_c2[0][0] __________________________________________________________________________________________________ add_16 (Add) (None, 32, 32, 256) 0 g_r7_iN2[0][0] add_15[0][0] __________________________________________________________________________________________________ lambda_35 (Lambda) (None, 34, 34, 256) 0 add_16[0][0] __________________________________________________________________________________________________ g_r8_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_35[0][0] __________________________________________________________________________________________________ g_r8_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r8_c1[0][0] __________________________________________________________________________________________________ lambda_36 (Lambda) (None, 34, 34, 256) 0 g_r8_iN1[0][0] __________________________________________________________________________________________________ g_r8_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_36[0][0] __________________________________________________________________________________________________ g_r8_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r8_c2[0][0] __________________________________________________________________________________________________ add_17 (Add) (None, 32, 32, 256) 0 g_r8_iN2[0][0] add_16[0][0] __________________________________________________________________________________________________ lambda_37 (Lambda) (None, 34, 34, 256) 0 add_17[0][0] __________________________________________________________________________________________________ g_r9_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_37[0][0] __________________________________________________________________________________________________ g_r9_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r9_c1[0][0] __________________________________________________________________________________________________ lambda_38 (Lambda) (None, 34, 34, 256) 0 g_r9_iN1[0][0] __________________________________________________________________________________________________ g_r9_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_38[0][0] __________________________________________________________________________________________________ g_r9_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r9_c2[0][0] __________________________________________________________________________________________________ add_18 (Add) (None, 32, 32, 256) 0 g_r9_iN2[0][0] add_17[0][0] __________________________________________________________________________________________________ g_d1_dc (Conv2DTranspose) (None, 64, 64, 128) 295040 add_18[0][0] __________________________________________________________________________________________________ g_d1_bn (InstanceNormalization) (None, 64, 64, 128) 256 g_d1_dc[0][0] __________________________________________________________________________________________________ activation_7 (Activation) (None, 64, 64, 128) 0 g_d1_bn[0][0] __________________________________________________________________________________________________ g_d2_dc (Conv2DTranspose) (None, 128, 128, 64) 73792 activation_7[0][0] __________________________________________________________________________________________________ g_d2_bn (InstanceNormalization) (None, 128, 128, 64) 128 g_d2_dc[0][0] __________________________________________________________________________________________________ activation_8 (Activation) (None, 128, 128, 64) 0 g_d2_bn[0][0] __________________________________________________________________________________________________ lambda_39 (Lambda) (None, 134, 134, 64) 0 activation_8[0][0] __________________________________________________________________________________________________ g_pred_c (Conv2D) (None, 128, 128, 3) 9411 lambda_39[0][0] __________________________________________________________________________________________________ activation_9 (Activation) (None, 128, 128, 3) 0 g_pred_c[0][0] ================================================================================================== Total params: 11,388,675 Trainable params: 11,388,675 Non-trainable params: 0 __________________________________________________________________________________________________ . composite Model with two genartors and discriminator . def build_composite_model(g_model_1, d_model, g_model_2, image_shape): # ensure the model we&#39;re updating is trainable g_model_1.trainable = True # mark discriminator as not trainable d_model.trainable = False # mark other generator model as not trainable g_model_2.trainable = False # discriminator element input_gen = Input(shape=image_shape) gen1_out = g_model_1(input_gen) output_d = d_model(gen1_out) # identity element input_id = Input(shape=image_shape) output_id = g_model_1(input_id) # forward cycle output_f = g_model_2(gen1_out) # backward cycle gen2_out = g_model_2(input_id) output_b = g_model_1(gen2_out) # define model graph model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b]) # define optimization algorithm configuration opt = Adam(lr=0.0002, beta_1=0.5) # compile model with weighting of least squares loss and L1 loss model.compile(loss=[&#39;mse&#39;, &#39;mae&#39;, &#39;mae&#39;, &#39;mae&#39;], loss_weights=[1, 5, 10, 10], optimizer=opt) return model . The original samples are over 3100 per domain and it is increasing the time for each epoch(has proven problematic in the initial training runs). So we will use a function to get a subsample of the training data , 1000 per Domain . def get_subsample(dataset): t1=np.random.randint(900) t2=np.random.randint(1200,2000) t3=np.random.randint(2500,2800) return np.vstack((dataset[0][t1:t1+300],dataset[0][t2:t2+400],dataset[0][t3:t3+300])),np.vstack((dataset[1][t1:t1+300], dataset[1][t2:t2+400],dataset[1][t3:t3+300])) . def get_subsample2(dataset): t0=np.random.randint(250) t1=np.random.randint(300) t2=np.random.randint(1200,2000) t3=np.random.randint(2500,2800) return np.vstack((dataset[0][t1:t1+800],dataset[0][t2:t2+200])),np.vstack((dataset[1][t1:t1+100], dataset[2][t0:t0+900])) . Utility Functions to load the image samples , generate fake images , save Models , Save genrated images , etc . def load_real_samples2(filename): data = load(filename) X1,X2,X3 = data[&#39;arr_0&#39;],data[&#39;arr_1&#39;],data[&#39;arr_2&#39;] X1= (X1-127.5)/127.5 X2 = (X2-127.5)/127.5 X3 = (X3-127.5)/127.5 return X1,X2,X3 . def load_real_samples(filename): # load the dataset data = load(filename) # unpack arrays X1, X2 = data[&#39;arr_0&#39;], data[&#39;arr_1&#39;] # scale from [0,255] to [-1,1] X1 = (X1 - 127.5) / 127.5 X2 = (X2 - 127.5) / 127.5 return [X1, X2] # select a batch of random samples, returns images and target def generate_real_samples(dataset, n_samples, patch_shape): # choose random instances ix = randint(0, dataset.shape[0], n_samples) # retrieve selected images X = dataset[ix] # generate &#39;real&#39; class labels (1) y = ones((n_samples, patch_shape, patch_shape, 1)) return X, y # generate a batch of images, returns images and targets def generate_fake_samples(g_model, dataset, patch_shape): # generate fake instance X = g_model.predict(dataset) # create &#39;fake&#39; class labels (0) y = zeros((len(X), patch_shape, patch_shape, 1)) return X, y # save the generator models to file def save_models(step, g_model_AtoB, g_model_BtoA): # save the first generator model filename1 = &#39;/content/drive/My Drive/EIP3/session7/g_model_AtoB_%06d.h5&#39; % (step+1) g_model_AtoB.save(filename1) # save the second generator model filename2 = &#39;/content/drive/My Drive/EIP3/session7/g_model_BtoA_%06d.h5&#39; % (step+1) g_model_BtoA.save(filename2) print(&#39;&gt;Saved: %s and %s&#39; % (filename1, filename2)) # save the generator models to file def save_models2(step, g_model_AtoB, g_model_BtoA,d_model_A,d_model_B): # save the first generator model filename1 = &#39;/content/drive/My Drive/EIP3/session7/g_model_AtoB_%06d.h5&#39; % (step+1) g_model_AtoB.save(filename1) # save the second generator model filename2 = &#39;/content/drive/My Drive/EIP3/session7/g_model_BtoA_%06d.h5&#39; % (step+1) g_model_BtoA.save(filename2) # save the first discriminator model A filename3 = &#39;/content/drive/My Drive/EIP3/session7/d_model_A_%06d.h5&#39; % (step+1) d_model_A.save(filename3) # save the first discriminator model B filename4 = &#39;/content/drive/My Drive/EIP3/session7/d_model_B_%06d.h5&#39; % (step+1) d_model_B.save(filename4) print(&#39;&gt;Saved: %s , %s , %s and %s&#39; % (filename1, filename2,filename3,filename4)) . def summarize_performance(step, g_model, trainX, name, n_samples=5): pyplot.figure( figsize=(15, 8), dpi=120) # select a sample of input images X_in, _ = generate_real_samples(trainX, n_samples, 0) # generate translated images X_out, _ = generate_fake_samples(g_model, X_in, 0) # scale all pixels from [-1,1] to [0,1] X_in = (X_in + 1) / 2.0 X_out = (X_out + 1) / 2.0 # plot real images for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(X_in[i]) # plot translated image for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + n_samples + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(X_out[i]) # save plot to file filename1 = &#39;/content/drive/My Drive/EIP3/session7/%s_generated_plot_%06d.png&#39; % (name, (step+1)) pyplot.savefig(filename1) pyplot.close() . Maintain a pool of 50 images as described in the paper . def update_image_pool(pool, images, max_size=50): selected = list() for image in images: if len(pool) &lt; max_size: # stock the pool pool.append(image) selected.append(image) elif random() &lt; 0.5: # use image, but don&#39;t add it to the pool selected.append(image) else: # replace an existing image and use replaced image ix = randint(0, len(pool)) selected.append(pool[ix]) pool[ix] = image return asarray(selected) . function to run the training . def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size,n_epochs): # define properties of the training run n_epochs, n_batch, = n_epochs, batch_size # determine the output square shape of the discriminator n_patch = d_model_A.output_shape[1] # unpack dataset trainA, trainB = get_subsample(dataset) # prepare image pool for fakes poolA, poolB = list(), list() # calculate the number of batches per training epoch bat_per_epo = int(len(trainA) / n_batch) # calculate the number of training iterations n_steps = bat_per_epo * n_epochs # manually enumerate epochs for i in range(n_steps): # select a batch of real samples X_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch) X_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch) # generate a batch of fake samples X_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch) X_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch) # update fakes from pool X_fakeA = update_image_pool(poolA, X_fakeA) X_fakeB = update_image_pool(poolB, X_fakeB) # update generator B-&gt;A via adversarial and cycle loss g_loss2, _, _, _, _ = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA]) # update discriminator for A -&gt; [real/fake] dA_loss1 = d_model_A.train_on_batch(X_realA, y_realA) dA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA) # update generator A-&gt;B via adversarial and cycle loss g_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB]) # update discriminator for B -&gt; [real/fake] dB_loss1 = d_model_B.train_on_batch(X_realB, y_realB) dB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB) # summarize performance print(&#39;&gt;%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]&#39; % (i+1, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2)) # evaluate the model performance every so often if (i+1) % (bat_per_epo * 1) == 0: # plot A-&gt;B translation summarize_performance(i, g_model_AtoB, trainA, &#39;AtoB&#39;) # plot B-&gt;A translation summarize_performance(i, g_model_BtoA, trainB, &#39;BtoA&#39;) if (i+1) % (bat_per_epo * 5) == 0: # save the models save_models2(i, g_model_AtoB, g_model_BtoA,d_model_A,d_model_B) . define the models and run training . from random import random from numpy import load from numpy import zeros from numpy import ones from numpy import asarray from numpy.random import randint from keras.optimizers import Adam # load image data dataset = load_real_samples(&#39;/content/drive/My Drive/EIP3/session7/utkface_128.npz&#39;) print(&#39;Loaded&#39;, dataset[0].shape, dataset[1].shape) # define input shape based on the loaded dataset image_shape = dataset[0].shape[1:] # generator: A -&gt; B g_model_AtoB = build_generator(image_shape) # generator: B -&gt; A g_model_BtoA = build_generator(image_shape) # discriminator: A -&gt; [real/fake] d_model_A = build_discriminator(image_shape) # discriminator: B -&gt; [real/fake] d_model_B = build_discriminator(image_shape) # composite: A -&gt; B -&gt; [real/fake, A] c_model_AtoB = build_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape) # composite: B -&gt; A -&gt; [real/fake, B] c_model_BtoA = build_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape) # train models train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=2,n_epochs=10) . # load image data dataset = load_real_samples(&#39;/content/drive/My Drive/EIP3/session7/utkface_128.npz&#39;) print(&#39;Loaded&#39;, dataset[0].shape, dataset[1].shape) # define input shape based on the loaded dataset image_shape = dataset[0].shape[1:] #load the previously trained model cust = {&#39;InstanceNormalization&#39;: InstanceNormalization, &#39;tf&#39;: tf} # generator: A -&gt; B g_model_AtoB = load_model(&#39;/content/drive/My Drive/EIP3/session7/g_model_AtoB_005625.h5&#39;, cust) # generator: B -&gt; A g_model_BtoA = load_model(&#39;/content/drive/My Drive/EIP3/session7/g_model_BtoA_005625.h5&#39;, cust) # discriminator: A -&gt; [real/fake] d_model_A = build_discriminator(image_shape) # discriminator: B -&gt; [real/fake] d_model_B = build_discriminator(image_shape) # composite: A -&gt; B -&gt; [real/fake, A] c_model_AtoB = build_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape) # composite: B -&gt; A -&gt; [real/fake, B] c_model_BtoA = build_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape) . Run time disconnected and session ended , so load last saved models and continue training . Increase the batch size and reduce the sample size too with get_subsample utility function . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=5) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=5) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=5) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=10) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=10) . dataset = load_real_samples2(&#39;/content/drive/My Drive/EIP3/session7/utkface_128_2.npz&#39;) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=20) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=20) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=20) . dataset = load_real_samples(&#39;/content/drive/My Drive/EIP3/session7/utkface_128.npz&#39;) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=10) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=1) . We have trained the model for slighly over 100 epochs . Although more epochs will give better results, we stop here due to time constraints . We will try out the results of this training . def show_results( g_model, trainX, n_samples=5,title=&#39;A to B&#39;): pyplot.figure( figsize=(12, 6), dpi=120) # select a sample of input images X_in, _ = generate_real_samples(trainX, n_samples, 0) # generate translated images X_out, _ = generate_fake_samples(g_model, X_in, 0) # scale all pixels from [-1,1] to [0,1] X_in = (X_in + 1) / 2.0 X_out = (X_out + 1) / 2.0 # plot real images #pyplot.title(title) for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(X_in[i]) pyplot.show() print(&quot; ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ &quot;+title+&quot; ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓&quot;) # plot translated image pyplot.figure( figsize=(12, 6), dpi=120) for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + n_samples + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(X_out[i]) pyplot.show() . domain A to Domain B generation results : Young to Old . trainA, trainB = dataset show_results(g_model_AtoB, trainA, 5, &#39;YOUNG TO OLD&#39;) . ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ YOUNG TO OLD ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ . show_results(g_model_AtoB, trainA, 5, &#39;YOUNG TO OLD&#39;) . ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ YOUNG TO OLD ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ . show_results(g_model_AtoB, trainA, 5, &#39;YOUNG TO OLD&#39;) . ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ YOUNG TO OLD ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ . domain B to Domain A generation results : Old to Young . show_results(g_model_BtoA, trainB ,5, &#39; Old to Young &#39;) . ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Old to Young ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ . show_results(g_model_BtoA, trainB ,5, &#39; Old to Young &#39;) . ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Old to Young ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ . show_results(g_model_BtoA, trainB ,5, &#39; Old to Young &#39;) . ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Old to Young ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ . show_results(g_model_BtoA, trainB ,5, &#39; Old to Young &#39;) . ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Old to Young ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ . Training the models for more epochs will make the results better, especially for Young to Old Translation . Also we used 128x128 images due to time and compute constraints . Training on the original 200x200 image size would have yielded better results .",
            "url": "https://ravindrabharathi.github.io/blog/dl/cyclegan/gan/2020/04/01/Face-Aging-Cycle-GANs.html",
            "relUrl": "/dl/cyclegan/gan/2020/04/01/Face-Aging-Cycle-GANs.html",
            "date": " • Apr 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ravindrabharathi.github.io/blog/jupyter/2020/02/22/test2.html",
            "relUrl": "/jupyter/2020/02/22/test2.html",
            "date": " • Feb 22, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "LR Finder",
            "content": "We will use CIFAR-10 dataset for this exercise and build the model using Keras .. . import necessary modules . from keras import backend as K import time import matplotlib.pyplot as plt import numpy as np % matplotlib inline np.random.seed(2017) from keras import regularizers from keras.models import Sequential from keras.layers.convolutional import Convolution2D, MaxPooling2D,AveragePooling2D from keras.layers import Activation, Flatten, Dense, Dropout from keras.layers.normalization import BatchNormalization from keras.utils import np_utils from keras.preprocessing.image import ImageDataGenerator . Using TensorFlow backend. . get CIFAR10 dataset and set the train and test data . from keras.datasets import cifar10 (train_features, train_labels), (test_features, test_labels) = cifar10.load_data() num_train, img_rows, img_cols,img_channels = train_features.shape num_test, _, _, _ = test_features.shape num_classes = len(np.unique(train_labels)) . Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz 170500096/170498071 [==============================] - 9s 0us/step . print (num_classes) print (num_train) print (train_features.shape) . 10 50000 (50000, 32, 32, 3) . inspect some of the images from the dataset by printing . class_names = [&#39;airplane&#39;,&#39;automobile&#39;,&#39;bird&#39;,&#39;cat&#39;,&#39;deer&#39;, &#39;dog&#39;,&#39;frog&#39;,&#39;horse&#39;,&#39;ship&#39;,&#39;truck&#39;] fig = plt.figure(figsize=(8,3)) for i in range(num_classes): ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[]) idx = np.where(train_labels[:]==i)[0] features_idx = train_features[idx,::] img_num = np.random.randint(features_idx.shape[0]) im = features_idx[img_num] ax.set_title(class_names[i]) plt.imshow(im) plt.show() . function for plotting accuracy vs number of epochs . def plot_model_history(model_history): fig, axs = plt.subplots(1,2,figsize=(15,5)) # summarize history for accuracy axs[0].plot(range(1,len(model_history.history[&#39;acc&#39;])+1),model_history.history[&#39;acc&#39;]) axs[0].plot(range(1,len(model_history.history[&#39;val_acc&#39;])+1),model_history.history[&#39;val_acc&#39;]) axs[0].set_title(&#39;Model Accuracy&#39;) axs[0].set_ylabel(&#39;Accuracy&#39;) axs[0].set_xlabel(&#39;Epoch&#39;) axs[0].set_xticks(np.arange(1,len(model_history.history[&#39;acc&#39;])+1),len(model_history.history[&#39;acc&#39;])/10) axs[0].legend([&#39;train&#39;, &#39;val&#39;], loc=&#39;best&#39;) # summarize history for loss axs[1].plot(range(1,len(model_history.history[&#39;loss&#39;])+1),model_history.history[&#39;loss&#39;]) axs[1].plot(range(1,len(model_history.history[&#39;val_loss&#39;])+1),model_history.history[&#39;val_loss&#39;]) axs[1].set_title(&#39;Model Loss&#39;) axs[1].set_ylabel(&#39;Loss&#39;) axs[1].set_xlabel(&#39;Epoch&#39;) axs[1].set_xticks(np.arange(1,len(model_history.history[&#39;loss&#39;])+1),len(model_history.history[&#39;loss&#39;])/10) axs[1].legend([&#39;train&#39;, &#39;val&#39;], loc=&#39;best&#39;) plt.show() . function to calculate accuracy on test data . def accuracy(test_x, test_y, model): result = model.predict(test_x) predicted_class = np.argmax(result, axis=1) true_class = np.argmax(test_y, axis=1) num_correct = np.sum(predicted_class == true_class) accuracy = float(num_correct)/result.shape[0] return (accuracy * 100) . function to get max training accuracy from model history . def get_max_train_accuracy(model_info): train_acc=model_info.history[&#39;acc&#39;] max_train_acc=max(train_acc) return (max_train_acc * 100) . function to get max validation accuracy from model history . def get_max_val_accuracy(model_info): val_acc=model_info.history[&#39;val_acc&#39;] max_val_acc=max(val_acc) return (max_val_acc * 100) . standardize pixel values of train and test images and convert train and test labels to categorical one hot encoded vectors . train_features = train_features.astype(&#39;float32&#39;)/255 test_features = test_features.astype(&#39;float32&#39;)/255 # convert class labels to binary class labels train_labels = np_utils.to_categorical(train_labels, num_classes) test_labels = np_utils.to_categorical(test_labels, num_classes) . train_labels . array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 1.], [0., 0., 0., ..., 0., 0., 1.], ..., [0., 0., 0., ..., 0., 0., 1.], [0., 1., 0., ..., 0., 0., 0.], [0., 1., 0., ..., 0., 0., 0.]], dtype=float32) . train dataset stats :mean , standard deviation for whole dataset , for a batch of 128 images , etc . (trainX, trainy), (testX, testy) = cifar10.load_data() print(&#39;Statistics train=%.3f (%.3f), test=%.3f (%.3f)&#39; % (trainX.mean(), trainX.std(), testX.mean(), testX.std())) # create generator that centers pixel values datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True) # calculate the mean on the training dataset datagen.fit(trainX) #print(&#39;Data Generator mean=%.3f, std=%.3f&#39; % (datagen.mean, datagen.std)) # demonstrate effect on a single batch of samples iterator = datagen.flow(trainX, trainy, batch_size=128) # get a batch batchX, batchy = iterator.next() # pixel stats in the batch print(batchX.shape, batchX.mean(), batchX.std()) # demonstrate effect on entire training dataset iterator = datagen.flow(trainX, trainy, batch_size=len(trainX), shuffle=False) # get a batch batchX, batchy = iterator.next() # pixel stats in the batch print(batchX.shape, batchX.mean(), batchX.std()) . Statistics train=120.708 (64.150), test=121.529 (64.061) (128, 32, 32, 3) 0.01989002 1.0052702 (50000, 32, 32, 3) -1.6605131e-06 1.0000001 . iterator1 = datagen.flow(testX, testy, batch_size=len(testX), shuffle=False) batch_testX, batch_testy = iterator1.next() X_train = batchX X_test = batch_testX y_train=batchy y_test=batch_testy . Y_train = np_utils.to_categorical(y_train, 10) Y_test = np_utils.to_categorical(y_test, 10) . Use the following standardization/regularization techniques for the model . Using Image Normalization | Making use of Batch Normalization | Making use of L2 Regularizer | Properly using Dropout | model1 = Sequential() model1.add(Convolution2D(32, 3, 3, border_mode=&#39;same&#39;,kernel_regularizer=regularizers.l2(0.0001), input_shape=(32, 32, 3))) model1.add(Activation(&#39;relu&#39;)) model1.add(BatchNormalization()) model1.add(Convolution2D(64, 3, 3,kernel_regularizer=regularizers.l2(0.0001),border_mode=&#39;same&#39;)) model1.add(Activation(&#39;relu&#39;)) model1.add(BatchNormalization()) model1.add(MaxPooling2D(pool_size=(2, 2))) model1.add(Dropout(0.2)) model1.add(Convolution2D(32, 1, 1)) model1.add(Convolution2D(64, 3, 3,kernel_regularizer=regularizers.l2(0.0001),border_mode=&#39;same&#39;)) model1.add(Activation(&#39;relu&#39;)) model1.add(BatchNormalization()) model1.add(Convolution2D(128, 3, 3,kernel_regularizer=regularizers.l2(0.0001),border_mode=&#39;same&#39;)) model1.add(Activation(&#39;relu&#39;)) model1.add(BatchNormalization()) model1.add(MaxPooling2D(pool_size=(2, 2))) model1.add(Dropout(0.3)) model1.add(Convolution2D(32, 1, 1)) model1.add(Convolution2D(128, 3, 3,kernel_regularizer=regularizers.l2(0.0001), border_mode=&#39;same&#39;)) model1.add(Activation(&#39;relu&#39;)) model1.add(BatchNormalization()) model1.add(Convolution2D(256, 3, 3,kernel_regularizer=regularizers.l2(0.0001), border_mode=&#39;same&#39;, name=&#39;LC1&#39;)) model1.add(Activation(&#39;relu&#39;,name=&#39;R1&#39;)) model1.add(BatchNormalization(name=&#39;BN1&#39;)) model1.add(MaxPooling2D(pool_size=(2, 2))) model1.add(Dropout(0.5)) model1.add(Convolution2D(10, 1, 1, name=&quot;red1&quot;)) model1.add(AveragePooling2D(pool_size = (4,4))) model1.add(Flatten()) model1.add(Activation(&#39;softmax&#39;)) . WARNING: Logging before flag parsing goes to stderr. W0720 16:04:14.356874 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead. /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), kernel_regularizer=&lt;keras.reg..., input_shape=(32, 32, 3..., padding=&#34;same&#34;)` W0720 16:04:14.388915 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. W0720 16:04:14.394831 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead. W0720 16:04:14.438685 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead. W0720 16:04:14.439589 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead. W0720 16:04:16.704932 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead. /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), kernel_regularizer=&lt;keras.reg..., padding=&#34;same&#34;)` W0720 16:04:16.960443 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead. W0720 16:04:16.970278 140478930995072 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating: Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1))` del sys.path[0] /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), kernel_regularizer=&lt;keras.reg..., padding=&#34;same&#34;)` app.launch_new_instance() /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), kernel_regularizer=&lt;keras.reg..., padding=&#34;same&#34;)` /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1))` /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), kernel_regularizer=&lt;keras.reg..., padding=&#34;same&#34;)` /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), kernel_regularizer=&lt;keras.reg..., name=&#34;LC1&#34;, padding=&#34;same&#34;)` /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), name=&#34;red1&#34;)` W0720 16:04:17.392197 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead. . print model summary . model1.summary() . _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 32, 32, 32) 896 _________________________________________________________________ activation_1 (Activation) (None, 32, 32, 32) 0 _________________________________________________________________ batch_normalization_1 (Batch (None, 32, 32, 32) 128 _________________________________________________________________ conv2d_2 (Conv2D) (None, 32, 32, 64) 18496 _________________________________________________________________ activation_2 (Activation) (None, 32, 32, 64) 0 _________________________________________________________________ batch_normalization_2 (Batch (None, 32, 32, 64) 256 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 16, 16, 64) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 16, 16, 32) 2080 _________________________________________________________________ conv2d_4 (Conv2D) (None, 16, 16, 64) 18496 _________________________________________________________________ activation_3 (Activation) (None, 16, 16, 64) 0 _________________________________________________________________ batch_normalization_3 (Batch (None, 16, 16, 64) 256 _________________________________________________________________ conv2d_5 (Conv2D) (None, 16, 16, 128) 73856 _________________________________________________________________ activation_4 (Activation) (None, 16, 16, 128) 0 _________________________________________________________________ batch_normalization_4 (Batch (None, 16, 16, 128) 512 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 8, 8, 128) 0 _________________________________________________________________ dropout_2 (Dropout) (None, 8, 8, 128) 0 _________________________________________________________________ conv2d_6 (Conv2D) (None, 8, 8, 32) 4128 _________________________________________________________________ conv2d_7 (Conv2D) (None, 8, 8, 128) 36992 _________________________________________________________________ activation_5 (Activation) (None, 8, 8, 128) 0 _________________________________________________________________ batch_normalization_5 (Batch (None, 8, 8, 128) 512 _________________________________________________________________ LC1 (Conv2D) (None, 8, 8, 256) 295168 _________________________________________________________________ R1 (Activation) (None, 8, 8, 256) 0 _________________________________________________________________ BN1 (BatchNormalization) (None, 8, 8, 256) 1024 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 4, 4, 256) 0 _________________________________________________________________ dropout_3 (Dropout) (None, 4, 4, 256) 0 _________________________________________________________________ red1 (Conv2D) (None, 4, 4, 10) 2570 _________________________________________________________________ average_pooling2d_1 (Average (None, 1, 1, 10) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 10) 0 _________________________________________________________________ activation_6 (Activation) (None, 10) 0 ================================================================= Total params: 455,370 Trainable params: 454,026 Non-trainable params: 1,344 _________________________________________________________________ . Total params: 455,370 . LR Finder . Leslie N. Smith&#39;s 2015 paper “Cyclical Learning Rates for Training Neural Networks&quot; describes an effective technique of finding a range of learning rates for neural network where loss descends steeply and where the training diverges due to training rate being too large. . This page http://puzzlemusa.com/2018/05/14/learning-rate-finder-using-keras/ describes how to use this technique of LR Finder . We will use LR Finder to fix our initial learning rate to train the model . In order to use the model with LR Finder compile the model with SGD optimizer . model1.compile(optimizer=&#39;sgd&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . W0720 16:04:17.436660 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. . Get LR finder callback from this reference http://puzzlemusa.com/2018/05/14/learning-rate-finder-using-keras/ . Add functions to print LR at min loss and min smoothed loss . from keras.callbacks import Callback class LR_Finder(Callback): def __init__(self, start_lr=1e-5, end_lr=10, step_size=None, beta=.98): super().__init__() self.start_lr = start_lr self.end_lr = end_lr self.step_size = step_size self.beta = beta self.lr_mult = (end_lr / start_lr) ** (1 / step_size) #print(&quot;lr mult : &quot;+str(self.lr_mult)) def on_train_begin(self, logs=None): self.best_loss = 1e9 self.avg_loss = 0 self.losses, self.smoothed_losses, self.lrs, self.iterations = [], [], [], [] self.iteration = 0 logs = logs or {} K.set_value(self.model.optimizer.lr, self.start_lr) def on_batch_end(self, epoch, logs=None): logs = logs or {} loss = logs.get(&#39;loss&#39;) self.iteration += 1 self.avg_loss = self.beta * self.avg_loss + (1 - self.beta) * loss smoothed_loss = self.avg_loss / (1 - self.beta ** self.iteration) # Check if the loss is not exploding if self.iteration &gt; 1 and smoothed_loss &gt; self.best_loss * 4: self.model.stop_training = True return if smoothed_loss &lt; self.best_loss or self.iteration == 1: self.best_loss = smoothed_loss lr = self.start_lr * (self.lr_mult ** self.iteration) #print(&quot;lr = &quot;+str(lr)) self.losses.append(loss) self.smoothed_losses.append(smoothed_loss) self.lrs.append(lr) self.iterations.append(self.iteration) K.set_value(self.model.optimizer.lr, lr) def plot_lr(self): plt.figure(figsize=(18,12)) plt.xlabel(&#39;Iterations&#39;) plt.ylabel(&#39;Learning rate&#39;) plt.plot(self.iterations, self.lrs) def plot(self, n_skip=10): plt.figure(figsize=(18,12)) plt.ylabel(&#39;Loss&#39;) plt.xlabel(&#39;Learning rate (log scale)&#39;) plt.plot(self.lrs[n_skip:-5], self.losses[n_skip:-5]) plt.xscale(&#39;log&#39;) def plot_smoothed_loss(self, n_skip=10): plt.figure(figsize=(18,12)) plt.ylabel(&#39;Smoothed Losses&#39;) plt.xlabel(&#39;Learning rate (log scale)&#39;) plt.plot(self.lrs[n_skip:-5], self.smoothed_losses[n_skip:-5]) plt.xscale(&#39;log&#39;) def plot_loss(self): plt.figure(figsize=(18,12)) plt.ylabel(&#39;Losses&#39;) plt.xlabel(&#39;Iterations&#39;) plt.plot(self.iterations[10:], self.losses[10:]) def get_best_loss(self): return self.best_loss def find_lr_at_best_loss(self): print(&quot;====================================================================&quot;) print(&quot;LR at min loss &quot;) print(&quot;LR at min loss : &quot;+str(self.lrs[np.argmin(self.losses)])) print(&quot;LR at min smoothed loss : &quot;+str(self.lrs[np.argmin(self.smoothed_losses)])) print(&quot;====================================================================&quot;) . Run LR_finder for 1 epoch with start_lr=1e-5 , end_lr=10 . batch_size=128 lr_finder = LR_Finder(start_lr=1e-5, end_lr=10, step_size=np.ceil(X_train.shape[0]/batch_size)) model1.fit(X_train, Y_train, epochs=1, batch_size=batch_size,callbacks=[lr_finder] ) . W0720 16:04:17.724291 140478930995072 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where . Epoch 1/1 50000/50000 [==============================] - 14s 283us/step - loss: 3.2580 - acc: 0.1908 . &lt;keras.callbacks.History at 0x7fc3a9a1d588&gt; . plot the following . 1. LR vs iterations . 2. Loss vs LR(log scale) . 3. Smoothed Loss vs LR(log scale) . From the smoothed loss vs LR plot we can see that the max descent for loss is between lr of 0.01 and 0.1 . lr_finder.plot_lr() . lr_finder.plot() . lr_finder.plot_smoothed_loss() . lr_finder.find_lr_at_best_loss() . ==================================================================== LR at min loss LR at min loss : 0.04489251258218551 LR at min smoothed loss : 0.04489251258218551 ==================================================================== . K.eval(lr_finder.model.optimizer.lr) . 10.0 . From the loss vs lr plots ,The max rate of descent seems to be between 0.01 and 0.1 and it looks like min loss is between lr values of 0.1 and 0.01 . printing the lr corresponding to the min loss and min smoothed loss confirms this observation . Let us pick an initial learning rate of 0.045 since that is where the smoothened loss curve starts going up . we will use SGD optimizer with lr=0.045 and momentum =0.9 to compile the model . from keras import optimizers opt= optimizers.SGD(lr=0.05,momentum=0.9) model1.compile(optimizer=opt, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . Cutout Augmentation . Cutout was first presented as an effective augmentation technique in these two papers : Improved Regularization of Convolutional Neural Networks with Cutout and Random Erasing Data Augmentation The idea is to randomly cut away patches of information from images that a model is training on to force it to learn from more parts of the image. This would help the model learn more features about a class instead of depending on some simple assumptions using smaller areas within the image . This helps the model generalize better and make better predictions . We will use python code for random erasing found at https://github.com/yu4u/cutout-random-erasing . !wget https://raw.githubusercontent.com/yu4u/cutout-random-erasing/master/random_eraser.py . --2019-07-20 16:05:13-- https://raw.githubusercontent.com/yu4u/cutout-random-erasing/master/random_eraser.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 888 [text/plain] Saving to: ‘random_eraser.py’ random_eraser.py 100%[===================&gt;] 888 --.-KB/s in 0s 2019-07-20 16:05:13 (158 MB/s) - ‘random_eraser.py’ saved [888/888] . train the model for 100 epochs . Use image augmentation of random cutout , horizontal flip . plot accuracy vs epochs , print accuracy and max val accuracy . from keras.preprocessing.image import ImageDataGenerator from random_eraser import get_random_eraser batch_size=128 train_datagen=ImageDataGenerator( featurewise_center=True, # set input mean to 0 over the dataset #samplewise_center=False, # set each sample mean to 0 featurewise_std_normalization=True, # divide inputs by std of the dataset #samplewise_std_normalization=False, # divide each input by its std preprocessing_function=get_random_eraser(v_l=0, v_h=1), horizontal_flip=True ) val_datagen= ImageDataGenerator( featurewise_center=True, # set input mean to 0 over the dataset featurewise_std_normalization=True, # divide inputs by std of the dataset ) train_datagen.fit(X_train) val_datagen.fit(X_test) training_generator=train_datagen.flow(X_train, Y_train, batch_size=batch_size,shuffle=True,seed=42) validation_generator=val_datagen.flow(X_test, Y_test, batch_size=batch_size,shuffle=True,seed=42) # train the model start = time.time() # Train the model model_info = model1.fit_generator(training_generator, epochs=100, steps_per_epoch=np.ceil(X_train.shape[0]/batch_size), validation_steps=np.ceil(X_test.shape[0]/batch_size), validation_data=validation_generator, shuffle=True, verbose=0) end = time.time() print (&quot;Model took %0.2f seconds to train&quot;%(end - start)) # plot model history plot_model_history(model_info) # compute test accuracy print (&quot;Accuracy on test data is: %0.2f&quot;%accuracy(X_test, Y_test, model1)) print (&quot;Max training accuracy is: %0.2f&quot;%get_max_train_accuracy(model_info)) print (&quot;Max validation accuracy is: %0.2f&quot;%get_max_val_accuracy(model_info)) . Model took 1151.59 seconds to train . Accuracy on test data is: 86.24 Max training accuracy is: 87.53 Max validation accuracy is: 87.81 . the model was trained for 100 epochs and reached a max val accuracy of 87.81 .We also notice that it almost the same as the max training accuracy . Training more epochs would yield better accuracy values . We will stop at 100 epochs and prepare to print Grad-CAM visualization on some misclassified images from this model&#39;s prediction on the test data . Grad-CAM . Now let us define the function for Grad-CAM visualization . This function named gradcam takes as input the model , the set of images , the labels for each image and the layer to be used for calculating gradients . It returns a list of dictionaries containing original image , the heatmap, the titles to display during visualization . import cv2 from mpl_toolkits.axes_grid1 import ImageGrid from google.colab.patches import cv2_imshow from IPython.core.display import display, HTML #select test images and corresponding labels to print heatmap #x=np.array([test_features[41],test_features[410],test_features[222],test_features[950]]) #y=[test_labels[41],test_labels[410],test_labels[222],test_labels[950]] def gradcam(model1,x,y,which_layer): # results=[] #make prediction for these 4 images preds = model1.predict(x) for j in range(x.shape[0]): #get class id from the prediction values class_idx = np.argmax(preds[j]) class_output = model1.output[:, class_idx] ## choose the layer nearest to prediction that has a size of about 7x7 or 8x8 #in this case it is the layer being sent to the gradcam function last_conv_layer = model1.get_layer(which_layer) # compute gradients and from heatmap grads = K.gradients(class_output, last_conv_layer.output)[0] pooled_grads = K.mean(grads, axis=(0, 1, 2)) iterate = K.function([model1.input], [pooled_grads, last_conv_layer.output[0]]) pooled_grads_value, conv_layer_output_value = iterate([x]) #apply the pooled grad value to the conv layer channels for i in range(256): conv_layer_output_value[:, :, i] *= pooled_grads_value[i] #get the mean of the weighted values and assign to heatmap heatmap = np.mean(conv_layer_output_value, axis=-1) #retain only positive values (or 0) in heatmap heatmap = np.maximum(heatmap, 0) #convert values between 0 and 1 using divide by max value heatmap /= np.max(heatmap) #we now have a heatmap with size equal to the output size of the layer we chose #img is the image we are running gradcam on img = x[j] #resize heatmap 8x8 to image size of 32x32 heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0])) #convert pixel values to be between 0 and 255 heatmap = np.uint8(255 * heatmap) #apply suitable cv2 colormap . In this case colormap_JET heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET) # convert from BGR to RGB if we want to display using matplotlib heatmap1 = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB) # create superimposed image if we want to print using cv2 (cv2_imshow supported in colab) superimposed_img = cv2.addWeighted(img, 0.5, heatmap1, 0.5, 0,dtype=5) #create a dictionary object with details of image, heatmap, its title title1=str(j+1)+&quot;: &quot;+ class_names[np.argmax(y[j])]+&quot; predicted as &quot;+str(class_names[class_idx]) title2=&#39;superimposed heatmap&#39; image1=img image2=heatmap1 image3=superimposed_img imageObj={&#39;image1&#39;:image1,&#39;image2&#39;:image2,&#39;image3&#39;:image3,&#39;title1&#39;:title1,&#39;title2&#39;:title2} #append the image dict object to results list results.append(imageObj) #print(j) #return grad-cam results as a list of dictionary objects , each containing an image and its heatmap return results . Define the function to display the Grad-CAM visualizations . This function displays a set of two images with heatmap visuals per row . def displayRow(images): # we will plot 2 images in a row # cv.imshow does not work in jupyter notebooks and colab # cv2_imshow patch works on colab but matplotlib gives us a little more flexibility in formatting the display # we will use matplotlib to print the image and its heatmap fig = plt.figure(1, (13,13)) grid = ImageGrid(fig, 111, nrows_ncols=(1,5), axes_pad=1,label_mode=&quot;1&quot; ) #horizontal spacer #grid[0].imshow(np.ones((32, 10)),alpha=0) #grid[0].axis(&#39;off&#39;) #first image #print(&quot; original class is :&quot;+class_names[np.argmax(y[j])]+&quot; and predicted class is :&quot;+str(class_names[class_idx])) grid[0].imshow(images[0][&#39;image1&#39;]) grid[0].set_title(images[0][&#39;title1&#39;]) grid[0].axis(&#39;off&#39;) #print the original image and on top of it place the heat map at 60% transparency grid[1].imshow(images[0][&#39;image1&#39;],alpha=0.9) grid[1].imshow(images[0][&#39;image2&#39;],alpha=0.6) grid[1].set_title(images[0][&#39;title2&#39;]) grid[1].axis(&#39;off&#39;) #vertical separator grid[2].imshow(np.ones((32, 1))) grid[2].axis(&#39;off&#39;) #second image #print(&quot; original class is :&quot;+class_names[np.argmax(y[j])]+&quot; and predicted class is :&quot;+str(class_names[class_idx])) grid[3].imshow(images[1][&#39;image1&#39;]) grid[3].set_title(images[1][&#39;title1&#39;]) grid[3].axis(&#39;off&#39;) #print the original image and on top of it place the heat map at 60% transparency grid[4].imshow(images[1][&#39;image1&#39;],alpha=0.9) grid[4].imshow(images[1][&#39;image2&#39;],alpha=0.6) grid[4].set_title(images[1][&#39;title2&#39;]) grid[4].axis(&#39;off&#39;) plt.show() display(HTML(&quot;&lt;hr size=&#39;5&#39; color=&#39;black&#39; width=&#39;100%&#39; align=&#39;center&#39; /&gt;&quot;)) . Make predictions using the model and collect all the images that were classified wrongly . pred=model1.predict(X_test) pred2=np.argmax(pred,axis=1) wrong_set=[] correct_set=[] wrong_labels=[] true_labels=[] wrong_indices=[] for i in range(X_test.shape[0]): if (pred2[i]==np.argmax(test_labels[i])): correct_set.append(X_test[i]) else: wrong_indices.append(i) wrong_labels.append(class_names[pred2[i]]) true_labels.append(class_names[np.argmax(test_labels[i])]) wrong_set.append(X_test[i]) . Now take the first 26 images and the corresponding labels to create the data for Grad-CAM visualization . w_list=wrong_indices[:26] x=[] y=[] for i in range(len(w_list)): x.append(test_features[w_list[i]]) y.append(test_labels[w_list[i]]) #convert the image list to numpy array x=np.array(x) . Obtain results from the gradcam function . results=gradcam(model1,x,y,&#39;R1&#39;) # we choose this layer as the layer nearest to prediction having a size of 8x8 . display the results from gradcam function with images and corresponding heatmap visuals . display(HTML(&quot;&lt;h2 align=&#39;center&#39;&gt;First 26 misclassified images with Grad-CAM heatmap &lt;/h2&gt;&lt;hr size=&#39;5&#39; color=&#39;black&#39; width=&#39;100%&#39; align=&#39;center&#39; /&gt;&quot;)) for i in range(0,len(results),2): images=[] images.append(results[i]) images.append(results[i+1]) displayRow(images) . First 26 misclassified images with Grad-CAM heatmap . . . . . . . . . . . . . . We used LR finder to fix an optimum learning rate of 0.045 for training the model on cifar 10 dataset . The model reached a max val accuracy of 87.81 and was almost the same as the max training accuracy indicating that this model would reach even higher accuracies with more epochs . We then used Grad-CAM to visualize the heatmaps for a set of 26 images that this model misclassifed. .",
            "url": "https://ravindrabharathi.github.io/blog/lr%20finder/grad-cam/heatmap%20visualization/optimimum%20lr/2020/02/10/LR-Finder.html",
            "relUrl": "/lr%20finder/grad-cam/heatmap%20visualization/optimimum%20lr/2020/02/10/LR-Finder.html",
            "date": " • Feb 10, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": ". About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ravindrabharathi.github.io/blog/jupyter/2020/02/02/test4.html",
            "relUrl": "/jupyter/2020/02/02/test4.html",
            "date": " • Feb 2, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ravindrabharathi.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ravindrabharathi.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ravindrabharathi.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}