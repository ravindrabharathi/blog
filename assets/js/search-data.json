{
  
    
        "post0": {
            "title": "Face Aging using CycleGANs",
            "content": ". CycleGANs were introduced in this paper titled Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks where the authors presented an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. . For the images of faces with various ages we will be using the UTKFace dataset wich has a cropped image set of only faces marked with age , gender , race , etc. . We will be using following two good references that use CycleGAN in order to build and train our models . https://github.com/sungnam0/Face-Aging-with-CycleGAN 2.https://machinelearningmastery.com/cyclegan-tutorial-with-keras/ | import necessary modules . import numpy as np import keras import tensorflow as tf from keras.layers import Input,Conv2D,MaxPooling2D,Flatten,Activation,BatchNormalization,K,UpSampling2D from keras.layers import Dropout,GlobalAveragePooling2D,LeakyReLU,Dense,Reshape, concatenate,Conv2DTranspose from keras.models import Model,load_model import matplotlib.pyplot as plt #import keras.backend as K import os import time from datetime import datetime from keras.applications import InceptionResNetV2 from keras.callbacks import TensorBoard from keras.optimizers import Adam from keras.utils import to_categorical from keras_preprocessing import image from numpy import asarray from numpy import vstack from keras.preprocessing.image import img_to_array from keras.preprocessing.image import load_img from numpy import savez_compressed import pandas as pd import os from matplotlib import pyplot from numpy import load from random import random from numpy import load from numpy import zeros from numpy import ones from numpy import asarray from numpy.random import randint from keras.optimizers import Adam . set tf backend config to allocate memory as needed instead of pre-allocating . config = tf.ConfigProto() config.gpu_options.allow_growth = True # Create a session with the above options specified. keras.backend.tensorflow_backend.set_session(tf.Session(config=config)) . mount google drive . from google.colab import drive drive.mount(&#39;/content/drive&#39;, force_remount=True) . Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&amp;response_type=code Enter your authorization code: ·········· Mounted at /content/drive . !ls -al &#39;/content/drive/My Drive/FaceGAN/&#39; . total 105354 drwx 2 root root 4096 Aug 22 10:59 results -rw- 1 root root 1239467 Aug 22 09:56 utk_data.csv drwx 2 root root 4096 Aug 21 08:41 UTKFace -rw- 1 root root 106634631 Aug 21 07:15 UTKFace.tar.gz . get the UTKFace dataset . !tar zxf &#39;/content/drive/My Drive/FaceGAN/UTKFace.tar.gz&#39; UTKFace . Parse the data . The labels of each face image is embedded in the file name, formated like [age][gender][race]_[date&amp;time].jpg . [age] is an integer from 0 to 116, indicating the age [gender] is either 0 (male) or 1 (female) [race] is an integer from 0 to 4, denoting White, Black, Asian, Indian, and Others (like Hispanic, Latino, Middle Eastern). [date&amp;time] is in the format of yyyymmddHHMMSSFFF, showing the date and time an image was collected to UTKFace . data=[] for filename in os.listdir(&#39;./UTKFace&#39;): parts=filename.split(&#39;_&#39;) #print(parts[3]) item={} item[&#39;image&#39;]=filename item[&#39;age&#39;]=parts[0] item[&#39;gender&#39;]=parts[1] item[&#39;race&#39;]=parts[2] if (len(parts)==4): item[&#39;date_time&#39;]=parts[3] data.append(item) utk_data=pd.DataFrame(data) utk_data.describe() . age date_time gender image race . count 23708 | 23705 | 23708 | 23708 | 23708 | . unique 104 | 23479 | 2 | 23708 | 8 | . top 26 | 20170110173815028.jpg.chip.jpg | 0 | 26_1_0_20170112213001988.jpg.chip.jpg | 0 | . freq 2197 | 7 | 12391 | 1 | 10078 | . utk_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 23708 entries, 0 to 23707 Data columns (total 5 columns): age 23708 non-null object date_time 23705 non-null object gender 23708 non-null object image 23708 non-null object race 23708 non-null object dtypes: object(5) memory usage: 926.2+ KB . utk_data.head() . age date_time gender image race . 0 25 | 20170119172104288.jpg.chip.jpg | 1 | 25_1_3_20170119172104288.jpg.chip.jpg | 3 | . 1 25 | 20170117141726361.jpg.chip.jpg | 1 | 25_1_0_20170117141726361.jpg.chip.jpg | 0 | . 2 27 | 20170116001407357.jpg.chip.jpg | 0 | 27_0_1_20170116001407357.jpg.chip.jpg | 1 | . 3 10 | 20170103200501766.jpg.chip.jpg | 0 | 10_0_4_20170103200501766.jpg.chip.jpg | 4 | . 4 26 | 20170116184024662.jpg.chip.jpg | 1 | 26_1_0_20170116184024662.jpg.chip.jpg | 0 | . we do not need date time , so delete it . del utk_data[&#39;date_time&#39;] . define a category for ages and apply it to the dataset . def age_cat_fn(age): age=int(age) if (0&lt;age&lt;18): return 0 elif(18&lt;=age&lt;=25): return 1 elif (25&lt;age&lt;=39): return 2 elif (39&lt; age &lt;=49): return 3 elif (49 &lt; age &lt;=60): return 4 elif age&gt;60: return 5 . utk_data[&#39;age_cat&#39;]=utk_data.age.map(age_cat_fn) . utk_data.to_csv(&#39;utk_data.csv&#39;,sep=&#39;,&#39;) !cp &#39;utk_data.csv&#39; &#39;/content/drive/My Drive/EIP3/session7&#39; . data with age category . utk_data.head() . age gender image race age_cat . 0 25 | 1 | 25_1_3_20170119172104288.jpg.chip.jpg | 3 | 1 | . 1 25 | 1 | 25_1_0_20170117141726361.jpg.chip.jpg | 0 | 1 | . 2 27 | 0 | 27_0_1_20170116001407357.jpg.chip.jpg | 1 | 2 | . 3 10 | 0 | 10_0_4_20170103200501766.jpg.chip.jpg | 4 | 0 | . 4 26 | 1 | 26_1_0_20170116184024662.jpg.chip.jpg | 0 | 2 | . split our data into two Domains . Young : age category 1 | Old : Age category 4 | data_A=utk_data[utk_data.age_cat==1] data_B=utk_data[utk_data.age_cat==4] . print(data_A[&#39;age_cat&#39;].count()) print(data_B[&#39;age_cat&#39;].count()) . 3404 2592 . #import os #utk_data=pd.read_csv(&#39;/content/drive/My Drive/EIP3/session7/utk_data.csv&#39;) . get the iames belonging to the two Domains and save as a compressed numpy array so that we can load them when necesary instead of processing the UTKFace dataset multiple times . image_dir=&#39;./UTKFace/&#39; image_paths_A = data_A[&#39;image&#39;].tolist() image_paths_B = data_B[&#39;image&#39;].tolist() #print(image_paths[:10]) . images_A=None images_B=None #store 2000 images for A for i, image_path in enumerate(image_paths_A): if (i&lt;2000): if (i%1000==0): print(&quot;processing set A image num &quot;+str(i)) try: # Load image loaded_image = image.load_img(image_dir+image_path, target_size=(128,128,3)) # Convert PIL image to numpy ndarray loaded_image = image.img_to_array(loaded_image) # Add another dimension (Add batch dimension) loaded_image = np.expand_dims(loaded_image, axis=0) # Concatenate all images into one tensor if images_A is None: images_A = loaded_image else: images_A = np.concatenate([images_A, loaded_image], axis=0) except Exception as e: print(&quot;Error:&quot;, i, e) #store 2000 images for B for i, image_path in enumerate(image_paths_B): if (i&lt;2000): if (i%999==0): print(&quot;processing set B image num &quot;+str(i)) try: # Load image loaded_image = image.load_img(image_dir+image_path, target_size=(128,128,3)) # Convert PIL image to numpy ndarray loaded_image = image.img_to_array(loaded_image) # Add another dimension (Add batch dimension) loaded_image = np.expand_dims(loaded_image, axis=0) # Concatenate all images into one tensor if images_B is None: images_B = loaded_image else: images_B = np.concatenate([images_B, loaded_image], axis=0) except Exception as e: print(&quot;Error:&quot;, i, e) . processing set A image num 0 processing set A image num 1000 processing set B image num 0 processing set B image num 999 processing set B image num 1998 . print(&#39;images_A :&#39;) print(images_A.shape) print(&#39;images_B :&#39;) print(images_B.shape) . images_A : (2000, 128, 128, 3) images_B : (2000, 128, 128, 3) . filename = &#39;/content/drive/My Drive/EIP3/session7/utkface_128.npz&#39; savez_compressed(filename, images_A, images_B) print(&#39;Saved dataset: &#39;, filename) . Saved dataset: /content/drive/My Drive/EIP3/session7/utkface_128.npz . load the saved numpy arrays and plot some images from either domain . from numpy import load from matplotlib import pyplot # load the dataset data = load(&#39;/content/drive/My Drive/EIP3/session7/utkface_128.npz&#39;) dataA, dataB = data[&#39;arr_0&#39;], data[&#39;arr_1&#39;] print(&#39;Loaded: &#39;, dataA.shape, dataB.shape) # plot source images n_samples = 3 for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(dataA[i].astype(&#39;uint8&#39;)) # plot target image for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + n_samples + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(dataB[i].astype(&#39;uint8&#39;)) pyplot.show() . Loaded: (2000, 128, 128, 3) (2000, 128, 128, 3) . install keras-contrib so that we can use InstanceNormalization instead of BatchNormalization . !pip install git+https://www.github.com/keras-team/keras-contrib.git . Collecting git+https://www.github.com/keras-team/keras-contrib.git Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-225cusg3 Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-225cusg3 Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-contrib==2.0.8) (2.2.4) Requirement already satisfied: keras-preprocessing&gt;=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras-&gt;keras-contrib==2.0.8) (1.1.0) Requirement already satisfied: keras-applications&gt;=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras-&gt;keras-contrib==2.0.8) (1.0.8) Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras-&gt;keras-contrib==2.0.8) (3.13) Requirement already satisfied: six&gt;=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras-&gt;keras-contrib==2.0.8) (1.12.0) Requirement already satisfied: scipy&gt;=0.14 in /usr/local/lib/python3.6/dist-packages (from keras-&gt;keras-contrib==2.0.8) (1.3.1) Requirement already satisfied: numpy&gt;=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras-&gt;keras-contrib==2.0.8) (1.16.4) Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-&gt;keras-contrib==2.0.8) (2.8.0) Building wheels for collected packages: keras-contrib Building wheel for keras-contrib (setup.py) ... done Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101066 sha256=4c2686ed2595b2fbcc1fd6df0f014cb70df688e3b37ff72e3fb6ef31dd35e615 Stored in directory: /tmp/pip-ephem-wheel-cache-rkeooafj/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba Successfully built keras-contrib Installing collected packages: keras-contrib Successfully installed keras-contrib-2.0.8 . from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization from keras.initializers import RandomNormal . define helper functions for the various components of the Model that we are going to build . Conv layers . def conv2d(input_, output_dim, ks=4, s=2, stddev=0.02, padding=&#39;SAME&#39;,name=&#39;c2d&#39;): return Conv2D(output_dim,kernel_size=ks,strides=s,padding=padding,kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),name=name)(input_) . Leaky Relu . def lrelu(input_,name=&#39;lr&#39;): return LeakyReLU(alpha=0.2,name=name)(input_) . InstanceNormalization . def iNorm(input_,name=&#39;iNorm&#39;): return InstanceNormalization(axis=-1,name=name)(input_) . Discriminator Model . def build_discriminator(image_shape): # weight initialization #init = RandomNormal(stddev=0.02) # source image input in_image = Input(shape=image_shape) #C1 d1 = lrelu(conv2d(in_image,64,4,name=&#39;d_c1&#39;),&#39;lr1&#39; ) # C2 d2 = lrelu(iNorm(conv2d(d1,128,4,name=&#39;d_c2&#39;),&#39;iN2&#39;),&#39;lr2&#39;) # C3 d3 = lrelu(iNorm(conv2d(d1,256,4,name=&#39;d_c3&#39;),&#39;iN3&#39;),&#39;lr3&#39;) # C4 d4 = lrelu(iNorm(conv2d(d3,512,4,name=&#39;d_c4&#39;),&#39;iN4&#39;),&#39;lr4&#39;) &#39;&#39;&#39; # second last output layer d = conv2d(in_image,128,3,1) d = iNorm(d) d = lrelu(d) &#39;&#39;&#39; # output d5 = conv2d(d4,1,4,1,name=&#39;d_c5&#39;) #Conv2D(1, 4,1, padding=&#39;same&#39;, kernel_initializer=init)(d) # define model model = Model(in_image, d5) # compile model model.compile(loss=&#39;mse&#39;, optimizer=Adam(lr=0.0002, beta_1=0.5), loss_weights=[0.5]) return model . disc=build_discriminator(dataB[0].shape) disc.summary() . WARNING: Logging before flag parsing goes to stderr. W0823 12:23:56.827917 140499755456384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead. W0823 12:23:56.872185 140499755456384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. W0823 12:23:57.044422 140499755456384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. . _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 128, 128, 3) 0 _________________________________________________________________ d_c1 (Conv2D) (None, 64, 64, 64) 3136 _________________________________________________________________ lr1 (LeakyReLU) (None, 64, 64, 64) 0 _________________________________________________________________ d_c3 (Conv2D) (None, 32, 32, 256) 262400 _________________________________________________________________ iN3 (InstanceNormalization) (None, 32, 32, 256) 512 _________________________________________________________________ lr3 (LeakyReLU) (None, 32, 32, 256) 0 _________________________________________________________________ d_c4 (Conv2D) (None, 16, 16, 512) 2097664 _________________________________________________________________ iN4 (InstanceNormalization) (None, 16, 16, 512) 1024 _________________________________________________________________ lr4 (LeakyReLU) (None, 16, 16, 512) 0 _________________________________________________________________ d_c5 (Conv2D) (None, 16, 16, 1) 8193 ================================================================= Total params: 2,372,929 Trainable params: 2,372,929 Non-trainable params: 0 _________________________________________________________________ . function to add padding . def padd3(input_): import tensorflow as tf return tf.pad(input_, [[0, 0], [3, 3], [3, 3], [0, 0]], &quot;REFLECT&quot;) def padd1(input_): import tensorflow as tf return tf.pad(input_, [[0, 0], [1, 1], [1, 1], [0, 0]], &quot;REFLECT&quot;) . The generator uses Resnet Blocks , as defined below . from keras.layers import Add,Lambda def res_block(input_,nf=64,ks=3,s=1,name=&#39;res_blk&#39;): p=int((ks-1)/2) y=Lambda(padd1)(input_) #(tf.pad(input_,[[0,0],[p,p],[p,p],[0,0]],&#39;REFLECT&#39;)) y=iNorm(conv2d(y,nf,ks,s,padding=&#39;VALID&#39;,name=name+&#39;_c1&#39;),name=name+&#39;_iN1&#39;) y=Lambda(padd1)(y) #(tf.pad(tf.nn.relu(y),[[0,0],[p,p],[p,p],[0,0]],&#39;REFLECT&#39;)) y=iNorm(conv2d(y,nf,ks,s,padding=&#39;VALID&#39;,name=name+&#39;_c2&#39;),name=name+&#39;_iN2&#39;) y1=keras.layers.Add()([y,input_]) return y1 . deconvolution layers . def deconv2d(input_, output_dim, ks=4, s=2, stddev=0.02, padding=&#39;SAME&#39;,name=&#39;dc2d&#39;): #Conv2DTranspose(64, (3,3), strides=(2,2), padding=&#39;same&#39;, kernel_initializer=init)(g) dcv=Conv2DTranspose(output_dim,(ks,ks),strides=(s,s),padding=padding,kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),name=name)(input_) return dcv . generator model . from keras.layers import Lambda,Conv2DTranspose def build_generator(image_shape): nf=64 # num filters for first layer input_=Input(shape=(128,128,3)) c0 = Lambda(padd3)(input_) c1 = Activation(&#39;relu&#39;)(iNorm(conv2d(c0, nf, 7, 1, padding=&#39;VALID&#39;, name=&#39;g_e1_c&#39;), &#39;g_e1_bn&#39;)) c2 = Activation(&#39;relu&#39;)(iNorm(conv2d(c1, nf*2, 3, 2, name=&#39;g_e2_c&#39;), &#39;g_e2_bn&#39;)) c3 = Activation(&#39;relu&#39;)(iNorm(conv2d(c2, nf*4 , 3, 2, name=&#39;g_e3_c&#39;), &#39;g_e3_bn&#39;)) r1 = res_block(c3, nf*4, name=&#39;g_r1&#39;) r2 = res_block(r1, nf*4, name=&#39;g_r2&#39;) r3 = res_block(r2, nf*4, name=&#39;g_r3&#39;) r4 = res_block(r3, nf*4, name=&#39;g_r4&#39;) r5 = res_block(r4, nf*4, name=&#39;g_r5&#39;) r6 = res_block(r5, nf*4, name=&#39;g_r6&#39;) r7 = res_block(r6, nf*4, name=&#39;g_r7&#39;) r8 = res_block(r7, nf*4, name=&#39;g_r8&#39;) r9 = res_block(r8, nf*4, name=&#39;g_r9&#39;) d1=Conv2DTranspose(nf*2, (3,3), strides=(2,2), padding=&#39;same&#39;, kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),name=&#39;g_d1_dc&#39;)(r9) d1=Activation(&#39;relu&#39;)(iNorm(d1,name=&#39;g_d1_bn&#39;)) d2=Conv2DTranspose(nf, (3,3), strides=(2,2), padding=&#39;same&#39;, kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),name=&#39;g_d2_dc&#39;)(d1) d2=Activation(&#39;relu&#39;)(iNorm(d2,name=&#39;g_d2_bn&#39;)) d2 = Lambda(padd3)(d2)#(tf.pad(d2, [[0, 0], [3, 3], [3, 3], [0, 0]], &quot;REFLECT&quot;)) d3=conv2d(d2, 3 , 7, 1, padding=&#39;VALID&#39;, name=&#39;g_pred_c&#39;) pred=Activation(&#39;tanh&#39;)(d3) model=Model(input_,pred) return model . gen=build_generator(dataA[0].shape) gen.summary() . __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_3 (InputLayer) (None, 128, 128, 3) 0 __________________________________________________________________________________________________ lambda_20 (Lambda) (None, 134, 134, 3) 0 input_3[0][0] __________________________________________________________________________________________________ g_e1_c (Conv2D) (None, 128, 128, 64) 9472 lambda_20[0][0] __________________________________________________________________________________________________ g_e1_bn (InstanceNormalization) (None, 128, 128, 64) 128 g_e1_c[0][0] __________________________________________________________________________________________________ activation_4 (Activation) (None, 128, 128, 64) 0 g_e1_bn[0][0] __________________________________________________________________________________________________ g_e2_c (Conv2D) (None, 64, 64, 128) 73856 activation_4[0][0] __________________________________________________________________________________________________ g_e2_bn (InstanceNormalization) (None, 64, 64, 128) 256 g_e2_c[0][0] __________________________________________________________________________________________________ activation_5 (Activation) (None, 64, 64, 128) 0 g_e2_bn[0][0] __________________________________________________________________________________________________ g_e3_c (Conv2D) (None, 32, 32, 256) 295168 activation_5[0][0] __________________________________________________________________________________________________ g_e3_bn (InstanceNormalization) (None, 32, 32, 256) 512 g_e3_c[0][0] __________________________________________________________________________________________________ activation_6 (Activation) (None, 32, 32, 256) 0 g_e3_bn[0][0] __________________________________________________________________________________________________ lambda_21 (Lambda) (None, 34, 34, 256) 0 activation_6[0][0] __________________________________________________________________________________________________ g_r1_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_21[0][0] __________________________________________________________________________________________________ g_r1_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r1_c1[0][0] __________________________________________________________________________________________________ lambda_22 (Lambda) (None, 34, 34, 256) 0 g_r1_iN1[0][0] __________________________________________________________________________________________________ g_r1_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_22[0][0] __________________________________________________________________________________________________ g_r1_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r1_c2[0][0] __________________________________________________________________________________________________ add_10 (Add) (None, 32, 32, 256) 0 g_r1_iN2[0][0] activation_6[0][0] __________________________________________________________________________________________________ lambda_23 (Lambda) (None, 34, 34, 256) 0 add_10[0][0] __________________________________________________________________________________________________ g_r2_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_23[0][0] __________________________________________________________________________________________________ g_r2_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r2_c1[0][0] __________________________________________________________________________________________________ lambda_24 (Lambda) (None, 34, 34, 256) 0 g_r2_iN1[0][0] __________________________________________________________________________________________________ g_r2_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_24[0][0] __________________________________________________________________________________________________ g_r2_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r2_c2[0][0] __________________________________________________________________________________________________ add_11 (Add) (None, 32, 32, 256) 0 g_r2_iN2[0][0] add_10[0][0] __________________________________________________________________________________________________ lambda_25 (Lambda) (None, 34, 34, 256) 0 add_11[0][0] __________________________________________________________________________________________________ g_r3_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_25[0][0] __________________________________________________________________________________________________ g_r3_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r3_c1[0][0] __________________________________________________________________________________________________ lambda_26 (Lambda) (None, 34, 34, 256) 0 g_r3_iN1[0][0] __________________________________________________________________________________________________ g_r3_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_26[0][0] __________________________________________________________________________________________________ g_r3_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r3_c2[0][0] __________________________________________________________________________________________________ add_12 (Add) (None, 32, 32, 256) 0 g_r3_iN2[0][0] add_11[0][0] __________________________________________________________________________________________________ lambda_27 (Lambda) (None, 34, 34, 256) 0 add_12[0][0] __________________________________________________________________________________________________ g_r4_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_27[0][0] __________________________________________________________________________________________________ g_r4_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r4_c1[0][0] __________________________________________________________________________________________________ lambda_28 (Lambda) (None, 34, 34, 256) 0 g_r4_iN1[0][0] __________________________________________________________________________________________________ g_r4_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_28[0][0] __________________________________________________________________________________________________ g_r4_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r4_c2[0][0] __________________________________________________________________________________________________ add_13 (Add) (None, 32, 32, 256) 0 g_r4_iN2[0][0] add_12[0][0] __________________________________________________________________________________________________ lambda_29 (Lambda) (None, 34, 34, 256) 0 add_13[0][0] __________________________________________________________________________________________________ g_r5_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_29[0][0] __________________________________________________________________________________________________ g_r5_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r5_c1[0][0] __________________________________________________________________________________________________ lambda_30 (Lambda) (None, 34, 34, 256) 0 g_r5_iN1[0][0] __________________________________________________________________________________________________ g_r5_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_30[0][0] __________________________________________________________________________________________________ g_r5_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r5_c2[0][0] __________________________________________________________________________________________________ add_14 (Add) (None, 32, 32, 256) 0 g_r5_iN2[0][0] add_13[0][0] __________________________________________________________________________________________________ lambda_31 (Lambda) (None, 34, 34, 256) 0 add_14[0][0] __________________________________________________________________________________________________ g_r6_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_31[0][0] __________________________________________________________________________________________________ g_r6_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r6_c1[0][0] __________________________________________________________________________________________________ lambda_32 (Lambda) (None, 34, 34, 256) 0 g_r6_iN1[0][0] __________________________________________________________________________________________________ g_r6_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_32[0][0] __________________________________________________________________________________________________ g_r6_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r6_c2[0][0] __________________________________________________________________________________________________ add_15 (Add) (None, 32, 32, 256) 0 g_r6_iN2[0][0] add_14[0][0] __________________________________________________________________________________________________ lambda_33 (Lambda) (None, 34, 34, 256) 0 add_15[0][0] __________________________________________________________________________________________________ g_r7_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_33[0][0] __________________________________________________________________________________________________ g_r7_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r7_c1[0][0] __________________________________________________________________________________________________ lambda_34 (Lambda) (None, 34, 34, 256) 0 g_r7_iN1[0][0] __________________________________________________________________________________________________ g_r7_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_34[0][0] __________________________________________________________________________________________________ g_r7_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r7_c2[0][0] __________________________________________________________________________________________________ add_16 (Add) (None, 32, 32, 256) 0 g_r7_iN2[0][0] add_15[0][0] __________________________________________________________________________________________________ lambda_35 (Lambda) (None, 34, 34, 256) 0 add_16[0][0] __________________________________________________________________________________________________ g_r8_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_35[0][0] __________________________________________________________________________________________________ g_r8_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r8_c1[0][0] __________________________________________________________________________________________________ lambda_36 (Lambda) (None, 34, 34, 256) 0 g_r8_iN1[0][0] __________________________________________________________________________________________________ g_r8_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_36[0][0] __________________________________________________________________________________________________ g_r8_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r8_c2[0][0] __________________________________________________________________________________________________ add_17 (Add) (None, 32, 32, 256) 0 g_r8_iN2[0][0] add_16[0][0] __________________________________________________________________________________________________ lambda_37 (Lambda) (None, 34, 34, 256) 0 add_17[0][0] __________________________________________________________________________________________________ g_r9_c1 (Conv2D) (None, 32, 32, 256) 590080 lambda_37[0][0] __________________________________________________________________________________________________ g_r9_iN1 (InstanceNormalization (None, 32, 32, 256) 512 g_r9_c1[0][0] __________________________________________________________________________________________________ lambda_38 (Lambda) (None, 34, 34, 256) 0 g_r9_iN1[0][0] __________________________________________________________________________________________________ g_r9_c2 (Conv2D) (None, 32, 32, 256) 590080 lambda_38[0][0] __________________________________________________________________________________________________ g_r9_iN2 (InstanceNormalization (None, 32, 32, 256) 512 g_r9_c2[0][0] __________________________________________________________________________________________________ add_18 (Add) (None, 32, 32, 256) 0 g_r9_iN2[0][0] add_17[0][0] __________________________________________________________________________________________________ g_d1_dc (Conv2DTranspose) (None, 64, 64, 128) 295040 add_18[0][0] __________________________________________________________________________________________________ g_d1_bn (InstanceNormalization) (None, 64, 64, 128) 256 g_d1_dc[0][0] __________________________________________________________________________________________________ activation_7 (Activation) (None, 64, 64, 128) 0 g_d1_bn[0][0] __________________________________________________________________________________________________ g_d2_dc (Conv2DTranspose) (None, 128, 128, 64) 73792 activation_7[0][0] __________________________________________________________________________________________________ g_d2_bn (InstanceNormalization) (None, 128, 128, 64) 128 g_d2_dc[0][0] __________________________________________________________________________________________________ activation_8 (Activation) (None, 128, 128, 64) 0 g_d2_bn[0][0] __________________________________________________________________________________________________ lambda_39 (Lambda) (None, 134, 134, 64) 0 activation_8[0][0] __________________________________________________________________________________________________ g_pred_c (Conv2D) (None, 128, 128, 3) 9411 lambda_39[0][0] __________________________________________________________________________________________________ activation_9 (Activation) (None, 128, 128, 3) 0 g_pred_c[0][0] ================================================================================================== Total params: 11,388,675 Trainable params: 11,388,675 Non-trainable params: 0 __________________________________________________________________________________________________ . composite Model with two genartors and discriminator . def build_composite_model(g_model_1, d_model, g_model_2, image_shape): # ensure the model we&#39;re updating is trainable g_model_1.trainable = True # mark discriminator as not trainable d_model.trainable = False # mark other generator model as not trainable g_model_2.trainable = False # discriminator element input_gen = Input(shape=image_shape) gen1_out = g_model_1(input_gen) output_d = d_model(gen1_out) # identity element input_id = Input(shape=image_shape) output_id = g_model_1(input_id) # forward cycle output_f = g_model_2(gen1_out) # backward cycle gen2_out = g_model_2(input_id) output_b = g_model_1(gen2_out) # define model graph model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b]) # define optimization algorithm configuration opt = Adam(lr=0.0002, beta_1=0.5) # compile model with weighting of least squares loss and L1 loss model.compile(loss=[&#39;mse&#39;, &#39;mae&#39;, &#39;mae&#39;, &#39;mae&#39;], loss_weights=[1, 5, 10, 10], optimizer=opt) return model . The original samples are over 3100 per domain and it is increasing the time for each epoch(has proven problematic in the initial training runs). So we will use a function to get a subsample of the training data , 1000 per Domain . def get_subsample(dataset): t1=np.random.randint(900) t2=np.random.randint(1200,2000) t3=np.random.randint(2500,2800) return np.vstack((dataset[0][t1:t1+300],dataset[0][t2:t2+400],dataset[0][t3:t3+300])),np.vstack((dataset[1][t1:t1+300], dataset[1][t2:t2+400],dataset[1][t3:t3+300])) . def get_subsample2(dataset): t0=np.random.randint(250) t1=np.random.randint(300) t2=np.random.randint(1200,2000) t3=np.random.randint(2500,2800) return np.vstack((dataset[0][t1:t1+800],dataset[0][t2:t2+200])),np.vstack((dataset[1][t1:t1+100], dataset[2][t0:t0+900])) . Utility Functions to load the image samples , generate fake images , save Models , Save genrated images , etc . def load_real_samples2(filename): data = load(filename) X1,X2,X3 = data[&#39;arr_0&#39;],data[&#39;arr_1&#39;],data[&#39;arr_2&#39;] X1= (X1-127.5)/127.5 X2 = (X2-127.5)/127.5 X3 = (X3-127.5)/127.5 return X1,X2,X3 . def load_real_samples(filename): # load the dataset data = load(filename) # unpack arrays X1, X2 = data[&#39;arr_0&#39;], data[&#39;arr_1&#39;] # scale from [0,255] to [-1,1] X1 = (X1 - 127.5) / 127.5 X2 = (X2 - 127.5) / 127.5 return [X1, X2] # select a batch of random samples, returns images and target def generate_real_samples(dataset, n_samples, patch_shape): # choose random instances ix = randint(0, dataset.shape[0], n_samples) # retrieve selected images X = dataset[ix] # generate &#39;real&#39; class labels (1) y = ones((n_samples, patch_shape, patch_shape, 1)) return X, y # generate a batch of images, returns images and targets def generate_fake_samples(g_model, dataset, patch_shape): # generate fake instance X = g_model.predict(dataset) # create &#39;fake&#39; class labels (0) y = zeros((len(X), patch_shape, patch_shape, 1)) return X, y # save the generator models to file def save_models(step, g_model_AtoB, g_model_BtoA): # save the first generator model filename1 = &#39;/content/drive/My Drive/EIP3/session7/g_model_AtoB_%06d.h5&#39; % (step+1) g_model_AtoB.save(filename1) # save the second generator model filename2 = &#39;/content/drive/My Drive/EIP3/session7/g_model_BtoA_%06d.h5&#39; % (step+1) g_model_BtoA.save(filename2) print(&#39;&gt;Saved: %s and %s&#39; % (filename1, filename2)) # save the generator models to file def save_models2(step, g_model_AtoB, g_model_BtoA,d_model_A,d_model_B): # save the first generator model filename1 = &#39;/content/drive/My Drive/EIP3/session7/g_model_AtoB_%06d.h5&#39; % (step+1) g_model_AtoB.save(filename1) # save the second generator model filename2 = &#39;/content/drive/My Drive/EIP3/session7/g_model_BtoA_%06d.h5&#39; % (step+1) g_model_BtoA.save(filename2) # save the first discriminator model A filename3 = &#39;/content/drive/My Drive/EIP3/session7/d_model_A_%06d.h5&#39; % (step+1) d_model_A.save(filename3) # save the first discriminator model B filename4 = &#39;/content/drive/My Drive/EIP3/session7/d_model_B_%06d.h5&#39; % (step+1) d_model_B.save(filename4) print(&#39;&gt;Saved: %s , %s , %s and %s&#39; % (filename1, filename2,filename3,filename4)) . def summarize_performance(step, g_model, trainX, name, n_samples=5): pyplot.figure( figsize=(15, 8), dpi=120) # select a sample of input images X_in, _ = generate_real_samples(trainX, n_samples, 0) # generate translated images X_out, _ = generate_fake_samples(g_model, X_in, 0) # scale all pixels from [-1,1] to [0,1] X_in = (X_in + 1) / 2.0 X_out = (X_out + 1) / 2.0 # plot real images for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(X_in[i]) # plot translated image for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + n_samples + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(X_out[i]) # save plot to file filename1 = &#39;/content/drive/My Drive/EIP3/session7/%s_generated_plot_%06d.png&#39; % (name, (step+1)) pyplot.savefig(filename1) pyplot.close() . Maintain a pool of 50 images as described in the paper . def update_image_pool(pool, images, max_size=50): selected = list() for image in images: if len(pool) &lt; max_size: # stock the pool pool.append(image) selected.append(image) elif random() &lt; 0.5: # use image, but don&#39;t add it to the pool selected.append(image) else: # replace an existing image and use replaced image ix = randint(0, len(pool)) selected.append(pool[ix]) pool[ix] = image return asarray(selected) . function to run the training . def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size,n_epochs): # define properties of the training run n_epochs, n_batch, = n_epochs, batch_size # determine the output square shape of the discriminator n_patch = d_model_A.output_shape[1] # unpack dataset trainA, trainB = get_subsample(dataset) # prepare image pool for fakes poolA, poolB = list(), list() # calculate the number of batches per training epoch bat_per_epo = int(len(trainA) / n_batch) # calculate the number of training iterations n_steps = bat_per_epo * n_epochs # manually enumerate epochs for i in range(n_steps): # select a batch of real samples X_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch) X_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch) # generate a batch of fake samples X_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch) X_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch) # update fakes from pool X_fakeA = update_image_pool(poolA, X_fakeA) X_fakeB = update_image_pool(poolB, X_fakeB) # update generator B-&gt;A via adversarial and cycle loss g_loss2, _, _, _, _ = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA]) # update discriminator for A -&gt; [real/fake] dA_loss1 = d_model_A.train_on_batch(X_realA, y_realA) dA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA) # update generator A-&gt;B via adversarial and cycle loss g_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB]) # update discriminator for B -&gt; [real/fake] dB_loss1 = d_model_B.train_on_batch(X_realB, y_realB) dB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB) # summarize performance print(&#39;&gt;%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]&#39; % (i+1, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2)) # evaluate the model performance every so often if (i+1) % (bat_per_epo * 1) == 0: # plot A-&gt;B translation summarize_performance(i, g_model_AtoB, trainA, &#39;AtoB&#39;) # plot B-&gt;A translation summarize_performance(i, g_model_BtoA, trainB, &#39;BtoA&#39;) if (i+1) % (bat_per_epo * 5) == 0: # save the models save_models2(i, g_model_AtoB, g_model_BtoA,d_model_A,d_model_B) . define the models and run training . from random import random from numpy import load from numpy import zeros from numpy import ones from numpy import asarray from numpy.random import randint from keras.optimizers import Adam # load image data dataset = load_real_samples(&#39;/content/drive/My Drive/EIP3/session7/utkface_128.npz&#39;) print(&#39;Loaded&#39;, dataset[0].shape, dataset[1].shape) # define input shape based on the loaded dataset image_shape = dataset[0].shape[1:] # generator: A -&gt; B g_model_AtoB = build_generator(image_shape) # generator: B -&gt; A g_model_BtoA = build_generator(image_shape) # discriminator: A -&gt; [real/fake] d_model_A = build_discriminator(image_shape) # discriminator: B -&gt; [real/fake] d_model_B = build_discriminator(image_shape) # composite: A -&gt; B -&gt; [real/fake, A] c_model_AtoB = build_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape) # composite: B -&gt; A -&gt; [real/fake, B] c_model_BtoA = build_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape) # train models train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=2,n_epochs=10) . # load image data dataset = load_real_samples(&#39;/content/drive/My Drive/EIP3/session7/utkface_128.npz&#39;) print(&#39;Loaded&#39;, dataset[0].shape, dataset[1].shape) # define input shape based on the loaded dataset image_shape = dataset[0].shape[1:] #load the previously trained model cust = {&#39;InstanceNormalization&#39;: InstanceNormalization, &#39;tf&#39;: tf} # generator: A -&gt; B g_model_AtoB = load_model(&#39;/content/drive/My Drive/EIP3/session7/g_model_AtoB_005625.h5&#39;, cust) # generator: B -&gt; A g_model_BtoA = load_model(&#39;/content/drive/My Drive/EIP3/session7/g_model_BtoA_005625.h5&#39;, cust) # discriminator: A -&gt; [real/fake] d_model_A = build_discriminator(image_shape) # discriminator: B -&gt; [real/fake] d_model_B = build_discriminator(image_shape) # composite: A -&gt; B -&gt; [real/fake, A] c_model_AtoB = build_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape) # composite: B -&gt; A -&gt; [real/fake, B] c_model_BtoA = build_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape) . Run time disconnected and session ended , so load last saved models and continue training . Increase the batch size and reduce the sample size too with get_subsample utility function . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=5) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=5) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=5) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=10) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=10) . dataset = load_real_samples2(&#39;/content/drive/My Drive/EIP3/session7/utkface_128_2.npz&#39;) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=20) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=20) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=20) . dataset = load_real_samples(&#39;/content/drive/My Drive/EIP3/session7/utkface_128.npz&#39;) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=10) . train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset,batch_size=24,n_epochs=1) . We have trained the model for slighly over 100 epochs . Although more epochs will give better results, we stop here due to time constraints . We will try out the results of this training . def show_results( g_model, trainX, n_samples=5,title=&#39;A to B&#39;): pyplot.figure( figsize=(12, 6), dpi=120) # select a sample of input images X_in, _ = generate_real_samples(trainX, n_samples, 0) # generate translated images X_out, _ = generate_fake_samples(g_model, X_in, 0) # scale all pixels from [-1,1] to [0,1] X_in = (X_in + 1) / 2.0 X_out = (X_out + 1) / 2.0 # plot real images #pyplot.title(title) for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(X_in[i]) pyplot.show() print(&quot; ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ &quot;+title+&quot; ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓&quot;) # plot translated image pyplot.figure( figsize=(12, 6), dpi=120) for i in range(n_samples): pyplot.subplot(2, n_samples, 1 + n_samples + i) pyplot.axis(&#39;off&#39;) pyplot.imshow(X_out[i]) pyplot.show() . domain A to Domain B generation results : Young to Old . trainA, trainB = dataset show_results(g_model_AtoB, trainA, 5, &#39;YOUNG TO OLD&#39;) . ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ YOUNG TO OLD ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ . show_results(g_model_AtoB, trainA, 5, &#39;YOUNG TO OLD&#39;) . ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ YOUNG TO OLD ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ . show_results(g_model_AtoB, trainA, 5, &#39;YOUNG TO OLD&#39;) . ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ YOUNG TO OLD ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ . domain B to Domain A generation results : Old to Young . show_results(g_model_BtoA, trainB ,5, &#39; Old to Young &#39;) . ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Old to Young ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ . show_results(g_model_BtoA, trainB ,5, &#39; Old to Young &#39;) . ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Old to Young ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ . show_results(g_model_BtoA, trainB ,5, &#39; Old to Young &#39;) . ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Old to Young ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ . show_results(g_model_BtoA, trainB ,5, &#39; Old to Young &#39;) . ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Old to Young ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ . Training the models for more epochs will make the results better, especially for Young to Old Translation . Also we used 128x128 images due to time and compute constraints . Training on the original 200x200 image size would have yielded better results .",
            "url": "https://ravindrabharathi.github.io/blog/dl/cyclegan/gan/2020/04/01/Face-Aging-Cycle-GANs.html",
            "relUrl": "/dl/cyclegan/gan/2020/04/01/Face-Aging-Cycle-GANs.html",
            "date": " • Apr 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Classify images in CIFAR-100 Dataset",
            "content": "We will use a ResNet34 pretrained model from https://github.com/qubvel/classification_models . We will use Resnet34 model to try and achieve 80% validation accuracy . Since pretrained weights are only available for imagenet and models expect a 224x224 image size , we will resize the cifar100 images to 224x224 while training . . In the pretrained model we will remove the top prediction layers and freeze the last 11 layers . We will add a GlobalAveragepooling2D layer , a dense layer and a softmax activation to form our prediction layer for cifar100. The first part will be to train with the frozen layers in base model . After training for about 30 epochs , we will unfreeze the layers and train further . . install the required files from qubvel keras applications project in order to get the pretrained ResNet model . !pip install git+https://github.com/qubvel/classification_models.git . Collecting git+https://github.com/qubvel/classification_models.git Cloning https://github.com/qubvel/classification_models.git to /tmp/pip-req-build-5x6yx4oj Running command git clone -q https://github.com/qubvel/classification_models.git /tmp/pip-req-build-5x6yx4oj Running command git submodule update --init --recursive -q Requirement already satisfied: keras_applications&lt;=1.0.8,&gt;=1.0.7 in /usr/local/lib/python3.6/dist-packages (from image-classifiers==1.0.0) (1.0.8) Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras_applications&lt;=1.0.8,&gt;=1.0.7-&gt;image-classifiers==1.0.0) (2.8.0) Requirement already satisfied: numpy&gt;=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras_applications&lt;=1.0.8,&gt;=1.0.7-&gt;image-classifiers==1.0.0) (1.17.4) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py-&gt;keras_applications&lt;=1.0.8,&gt;=1.0.7-&gt;image-classifiers==1.0.0) (1.12.0) Building wheels for collected packages: image-classifiers Building wheel for image-classifiers (setup.py) ... done Created wheel for image-classifiers: filename=image_classifiers-1.0.0-cp36-none-any.whl size=19950 sha256=be0aa5db89758bc9c55b9e6009bebb1222b086b087e45a6c54c4fb8e56b48877 Stored in directory: /tmp/pip-ephem-wheel-cache-cw05kyqp/wheels/de/2b/fd/29a6d33edb8c28bc7d94e95ea1d39c9a218ac500a3cfb1b197 Successfully built image-classifiers Installing collected packages: image-classifiers Successfully installed image-classifiers-1.0.0 . import necessary keras modules , numpy and matplotlib . from keras import backend as K import time import matplotlib.pyplot as plt import numpy as np % matplotlib inline np.random.seed(2017) #from keras.models import Sequential from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D from keras.layers import Activation, Flatten, Dropout from keras.layers import BatchNormalization from keras.utils import np_utils import os . Using TensorFlow backend. . The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x. We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the %tensorflow_version 1.x magic: more info. . import ResNet34 and image preprocessing from the project we installed earlier . import keras import cv2 #from classification_models.resnet import ResNet34, preprocess_input from classification_models.keras import Classifiers ResNet34, preprocess_input = Classifiers.get(&#39;resnet34&#39;) . get cifar100 dataset from keras datasets . from keras.datasets import cifar100 (train_features, train_labels), (test_features, test_labels) = cifar100.load_data() num_train, img_channels, img_rows, img_cols = train_features.shape num_test, _, _, _ = test_features.shape num_classes = len(np.unique(train_labels)) . Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz 169009152/169001437 [==============================] - 4s 0us/step . preprocess the images to make sure that they are in the format required by the pretrained model . train_features = preprocess_input(train_features) test_features = preprocess_input(test_features) . print max and min pixel values in the images which we can use in the ramdom-erase/cutout augmentation later . print(np.max(train_features),np.min(train_features)) . 255 0 . store cifar100 train and test images in a local data folder. We will load these images using an imagedatagenerator and resize to 224x224 which is default size for Resnet-imagenet models . !rm -R ./data/ . rm: cannot remove &#39;./data/&#39;: No such file or directory . sub_dir=&#39;train&#39; data_dir=&#39;./data&#39; if not os.path.exists(data_dir): os.mkdir(data_dir) image_dir=&#39;./data/&#39;+sub_dir+&#39;/&#39; if not os.path.exists(image_dir): os.mkdir(image_dir) . os.getcwd() . &#39;/content&#39; . os.path.exists(&#39;./data/train&#39;) . True . def save_img(images,sub_dir): c=0 os.chdir(&#39;/content/&#39;) curr_dir = os.getcwd() image_dir=&#39;./data/&#39;+sub_dir+&#39;/&#39; if not os.path.exists(image_dir): os.mkdir(image_dir) os.chdir(image_dir) print(&#39;current working directory is &#39;+os.getcwd()) for img in images: c +=1 filename=str(c)+&#39;.jpg&#39; cv2.imwrite(filename,img) print(&quot;files resized and saved to &quot;+image_dir) os.chdir(curr_dir) print(&#39;current working directory is &#39;+os.getcwd()) . save_img(train_features,&#39;train&#39;) . current working directory is /content/data/train files resized and saved to ./data/train/ current working directory is /content . save_img(test_features,&#39;test&#39;) . current working directory is /content/data/test files resized and saved to ./data/test/ current working directory is /content . !ls ./data . test train . mount google drive to save best model while training . from google.colab import drive drive.mount(&#39;/gdrive&#39;,force_remount=True) . Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&amp;response_type=code&amp;scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly Enter your authorization code: ·········· Mounted at /gdrive . import pandas and create a dataframe with image files and labels information. We will use this dataframe with Keras imagedatagenerator to load images for training and testing and calculate loss using the corresponding label values . import pandas as pd . def form_df(label_type=&#39;train&#39;): if label_type==&#39;train&#39;: labels=train_labels else: labels=test_labels file_name=[] class_label=[] for i in range(len(labels)): filename=str(i+1)+&#39;.jpg&#39; file_name.append(filename) class_label.append(str(labels[i][0])) df=pd.DataFrame({&#39;File&#39;:file_name,&#39;Class&#39;:class_label}) return df . train_df=form_df(&#39;train&#39;) print(train_df.head()) train_df.info() . File Class 0 1.jpg 19 1 2.jpg 29 2 3.jpg 0 3 4.jpg 11 4 5.jpg 1 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 50000 entries, 0 to 49999 Data columns (total 2 columns): File 50000 non-null object Class 50000 non-null object dtypes: object(2) memory usage: 781.4+ KB . train_df.tail() . File Class . 49995 49996.jpg | 80 | . 49996 49997.jpg | 7 | . 49997 49998.jpg | 3 | . 49998 49999.jpg | 7 | . 49999 50000.jpg | 73 | . test_df=form_df(&#39;test&#39;) print(test_df.head()) print(test_df.tail()) print(test_df.info()) . File Class 0 1.jpg 49 1 2.jpg 33 2 3.jpg 72 3 4.jpg 51 4 5.jpg 71 File Class 9995 9996.jpg 83 9996 9997.jpg 14 9997 9998.jpg 51 9998 9999.jpg 42 9999 10000.jpg 70 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 10000 entries, 0 to 9999 Data columns (total 2 columns): File 10000 non-null object Class 10000 non-null object dtypes: object(2) memory usage: 156.4+ KB None . function for random-pad-crop augmentation . def pad4(img): pad_size=img.shape[1]//8 img=np.pad(img, [ (pad_size, pad_size), (pad_size, pad_size), (0, 0)], mode=&#39;reflect&#39;) return img def random_pad_crop_img(img,crop_size=224): crop_size=img.shape[1] img=pad4(img) pad=img.shape[1]-crop_size x1=np.random.randint(pad) x2=x1+crop_size y1=np.random.randint(pad) y2=y1+crop_size img=img[x1:x2,y1:y2,:] return img . we will now get the ResNet34 model weights for imagenet (Cifar is not available in this library). . input shape set to 224,224,3 . Add GlobalAveragePooling to convert these to 1D inputs suitable for the softmax prediction layer . Add a Dense Layer instead of the one we removed from the pretrained model . Add softmax prediction . for the first train run we will freeze the all layers of the pretrained model except the last 11 layers . from keras.layers import GlobalAveragePooling2D, Add, Lambda, Dense, GlobalMaxPooling2D #base modek from REsnet34 base_model = ResNet34(input_shape=(224,224,3), weights=&#39;imagenet&#39;, include_top=False) #Freeze all but last 11 layers for layer in base_model.layers[:-11]: layer.trainable=False for layer in base_model.layers: print(layer, layer.trainable) #Add our own Top/Prediction layers x = GlobalAveragePooling2D()(base_model.output) x= Dense(num_classes,use_bias=False)(x) output = keras.layers.Activation(&#39;softmax&#39;)(x) model = keras.models.Model(inputs=[base_model.input], outputs=[output]) . WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead. Downloading data from https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000_no_top.h5 85524480/85521592 [==============================] - 3s 0us/step &lt;keras.engine.input_layer.InputLayer object at 0x7fef643e3400&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7fef64b22128&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7fef643e50b8&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7fef5436d160&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7fef543195f8&gt; False &lt;keras.layers.core.Activation object at 0x7fef54319d30&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7fef54319f60&gt; False &lt;keras.layers.pooling.MaxPooling2D object at 0x7fef54323400&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7fef542df3c8&gt; False &lt;keras.layers.core.Activation object at 0x7fef542df898&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7fef54298278&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7fef542dfc18&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7fef542df6a0&gt; False &lt;keras.layers.core.Activation object at 0x7fef542a6a58&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7fef542a6e80&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7fef542a6fd0&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7fef542df860&gt; False &lt;keras.layers.merge.Add object at 0x7fef54271160&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7fef54271208&gt; False &lt;keras.layers.core.Activation object at 0x7fef542717f0&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7fef54271710&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7fef54212518&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7fef5422cfd0&gt; False &lt;keras.layers.core.Activation object at 0x7fef54236e48&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7fef54236dd8&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7fef54242748&gt; False &lt;keras.layers.merge.Add object at 0x7fef541eacc0&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7fef541ead68&gt; False &lt;keras.layers.core.Activation object at 0x7fef541eafd0&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7fef541f1390&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7fef541fc630&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7fef541b1c88&gt; False &lt;keras.layers.core.Activation object at 0x7fef541b1f98&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7fef541b8a90&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7fef541c6d30&gt; False &lt;keras.layers.merge.Add object at 0x7fef5417ba58&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7fef5417bb00&gt; False &lt;keras.layers.core.Activation object at 0x7fef5417bf98&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7fef5413a828&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7fef5411bf28&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7fef54147748&gt; False &lt;keras.layers.core.Activation object at 0x7fef54147d68&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7fef54147f28&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7fef540d06d8&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7fef5417f550&gt; False &lt;keras.layers.merge.Add object at 0x7fef54090748&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7fef540907f0&gt; False &lt;keras.layers.core.Activation object at 0x7fef54090cf8&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7fef540975c0&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7fef540974a8&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7fef54058748&gt; False &lt;keras.layers.core.Activation object at 0x7fef54058e10&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7fef54058e48&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7fef5405d4a8&gt; False &lt;keras.layers.merge.Add object at 0x7fef5401f518&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7fef5401f5c0&gt; False &lt;keras.layers.core.Activation object at 0x7fef5401f908&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7fef5401fa20&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7fef54025278&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7fef501364e0&gt; False &lt;keras.layers.core.Activation object at 0x7fef50136e10&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7fef50136f98&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7fef5013c240&gt; False &lt;keras.layers.merge.Add object at 0x7fef500fe2b0&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7fef500fe358&gt; False &lt;keras.layers.core.Activation object at 0x7fef500fe860&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7fef500fe6a0&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7fef500fefd0&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7fef500c5278&gt; False &lt;keras.layers.core.Activation object at 0x7fef500c5fd0&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7fef500c5e80&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7fef50067550&gt; False &lt;keras.layers.merge.Add object at 0x7fef50089048&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7fef500890f0&gt; False &lt;keras.layers.core.Activation object at 0x7fef500896d8&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7fef50045d30&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7fef50089d68&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7fef50055cf8&gt; False &lt;keras.layers.core.Activation object at 0x7fef5004ae10&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7fef5005ad30&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7feef01b66a0&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7fef500895f8&gt; False &lt;keras.layers.merge.Add object at 0x7feef016bcf8&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7feef016bf28&gt; False &lt;keras.layers.core.Activation object at 0x7feef01c7eb8&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7feef01733c8&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7feef01957b8&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7feef0133cf8&gt; False &lt;keras.layers.core.Activation object at 0x7feef01a6eb8&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7feef013bb00&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7feef0142860&gt; False &lt;keras.layers.merge.Add object at 0x7feef00faac8&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7feef00fab70&gt; False &lt;keras.layers.core.Activation object at 0x7feef00faf98&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7feef0101208&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7feef0110d68&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7feef00c1a90&gt; False &lt;keras.layers.core.Activation object at 0x7feef00c1f98&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7feef00c1fd0&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7feef00c97f0&gt; False &lt;keras.layers.merge.Add object at 0x7feef0088860&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7feef0088908&gt; False &lt;keras.layers.core.Activation object at 0x7feef0088e10&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7feef008f6d8&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7feef008f5c0&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7feef004f828&gt; False &lt;keras.layers.core.Activation object at 0x7feef004fef0&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7feef004ff28&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7feef0058588&gt; False &lt;keras.layers.merge.Add object at 0x7feef00185f8&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7feef00186a0&gt; False &lt;keras.layers.core.Activation object at 0x7feef0018b00&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7feef0020400&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7feef0020358&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7feeeffe15c0&gt; False &lt;keras.layers.core.Activation object at 0x7feeeffe1cc0&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7feeeffe1ef0&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7feeeffe8320&gt; False &lt;keras.layers.merge.Add object at 0x7feeeffa7390&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7feeeffa7438&gt; False &lt;keras.layers.core.Activation object at 0x7feeeffa7780&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7feeeffa7ef0&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7feeeff37940&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7feeefef2358&gt; False &lt;keras.layers.core.Activation object at 0x7feeefef2c88&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7feeefef2e10&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7feeefef2f60&gt; False &lt;keras.layers.merge.Add object at 0x7feeefeba128&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7feeefeba1d0&gt; False &lt;keras.layers.core.Activation object at 0x7feeefeba7b8&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7feeefe72e10&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7feeefe78320&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7feeefe82dd8&gt; False &lt;keras.layers.core.Activation object at 0x7feeefe8ae80&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7feeefe8ae10&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7feeefe2d860&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7feeefeba6d8&gt; False &lt;keras.layers.merge.Add object at 0x7feeefe47dd8&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7feeefe47ef0&gt; False &lt;keras.layers.core.Activation object at 0x7feeefe39f98&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7feeefe504a8&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7feeefe57d30&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7feeefe11dd8&gt; False &lt;keras.layers.core.Activation object at 0x7feeefdfff98&gt; False &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7feeefe18be0&gt; False &lt;keras.layers.convolutional.Conv2D object at 0x7feeefe24eb8&gt; False &lt;keras.layers.merge.Add object at 0x7feeefdd7ba8&gt; False &lt;keras.layers.normalization.BatchNormalization object at 0x7feeefdd7c50&gt; True &lt;keras.layers.core.Activation object at 0x7feeefdd7f98&gt; True &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7feeefddf278&gt; True &lt;keras.layers.convolutional.Conv2D object at 0x7feeefddf908&gt; True &lt;keras.layers.normalization.BatchNormalization object at 0x7feeefd9eb70&gt; True &lt;keras.layers.core.Activation object at 0x7feeefd9ef60&gt; True &lt;keras.layers.convolutional.ZeroPadding2D object at 0x7feeefda6978&gt; True &lt;keras.layers.convolutional.Conv2D object at 0x7feeefd33fd0&gt; True &lt;keras.layers.merge.Add object at 0x7feeefd68940&gt; True &lt;keras.layers.normalization.BatchNormalization object at 0x7fef542df390&gt; True &lt;keras.layers.core.Activation object at 0x7feeefd68e10&gt; True . print model summary . model.summary() . Model: &#34;model_2&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== data (InputLayer) (None, 224, 224, 3) 0 __________________________________________________________________________________________________ bn_data (BatchNormalization) (None, 224, 224, 3) 9 data[0][0] __________________________________________________________________________________________________ zero_padding2d_1 (ZeroPadding2D (None, 230, 230, 3) 0 bn_data[0][0] __________________________________________________________________________________________________ conv0 (Conv2D) (None, 112, 112, 64) 9408 zero_padding2d_1[0][0] __________________________________________________________________________________________________ bn0 (BatchNormalization) (None, 112, 112, 64) 256 conv0[0][0] __________________________________________________________________________________________________ relu0 (Activation) (None, 112, 112, 64) 0 bn0[0][0] __________________________________________________________________________________________________ zero_padding2d_2 (ZeroPadding2D (None, 114, 114, 64) 0 relu0[0][0] __________________________________________________________________________________________________ pooling0 (MaxPooling2D) (None, 56, 56, 64) 0 zero_padding2d_2[0][0] __________________________________________________________________________________________________ stage1_unit1_bn1 (BatchNormaliz (None, 56, 56, 64) 256 pooling0[0][0] __________________________________________________________________________________________________ stage1_unit1_relu1 (Activation) (None, 56, 56, 64) 0 stage1_unit1_bn1[0][0] __________________________________________________________________________________________________ zero_padding2d_3 (ZeroPadding2D (None, 58, 58, 64) 0 stage1_unit1_relu1[0][0] __________________________________________________________________________________________________ stage1_unit1_conv1 (Conv2D) (None, 56, 56, 64) 36864 zero_padding2d_3[0][0] __________________________________________________________________________________________________ stage1_unit1_bn2 (BatchNormaliz (None, 56, 56, 64) 256 stage1_unit1_conv1[0][0] __________________________________________________________________________________________________ stage1_unit1_relu2 (Activation) (None, 56, 56, 64) 0 stage1_unit1_bn2[0][0] __________________________________________________________________________________________________ zero_padding2d_4 (ZeroPadding2D (None, 58, 58, 64) 0 stage1_unit1_relu2[0][0] __________________________________________________________________________________________________ stage1_unit1_conv2 (Conv2D) (None, 56, 56, 64) 36864 zero_padding2d_4[0][0] __________________________________________________________________________________________________ stage1_unit1_sc (Conv2D) (None, 56, 56, 64) 4096 stage1_unit1_relu1[0][0] __________________________________________________________________________________________________ add_1 (Add) (None, 56, 56, 64) 0 stage1_unit1_conv2[0][0] stage1_unit1_sc[0][0] __________________________________________________________________________________________________ stage1_unit2_bn1 (BatchNormaliz (None, 56, 56, 64) 256 add_1[0][0] __________________________________________________________________________________________________ stage1_unit2_relu1 (Activation) (None, 56, 56, 64) 0 stage1_unit2_bn1[0][0] __________________________________________________________________________________________________ zero_padding2d_5 (ZeroPadding2D (None, 58, 58, 64) 0 stage1_unit2_relu1[0][0] __________________________________________________________________________________________________ stage1_unit2_conv1 (Conv2D) (None, 56, 56, 64) 36864 zero_padding2d_5[0][0] __________________________________________________________________________________________________ stage1_unit2_bn2 (BatchNormaliz (None, 56, 56, 64) 256 stage1_unit2_conv1[0][0] __________________________________________________________________________________________________ stage1_unit2_relu2 (Activation) (None, 56, 56, 64) 0 stage1_unit2_bn2[0][0] __________________________________________________________________________________________________ zero_padding2d_6 (ZeroPadding2D (None, 58, 58, 64) 0 stage1_unit2_relu2[0][0] __________________________________________________________________________________________________ stage1_unit2_conv2 (Conv2D) (None, 56, 56, 64) 36864 zero_padding2d_6[0][0] __________________________________________________________________________________________________ add_2 (Add) (None, 56, 56, 64) 0 stage1_unit2_conv2[0][0] add_1[0][0] __________________________________________________________________________________________________ stage1_unit3_bn1 (BatchNormaliz (None, 56, 56, 64) 256 add_2[0][0] __________________________________________________________________________________________________ stage1_unit3_relu1 (Activation) (None, 56, 56, 64) 0 stage1_unit3_bn1[0][0] __________________________________________________________________________________________________ zero_padding2d_7 (ZeroPadding2D (None, 58, 58, 64) 0 stage1_unit3_relu1[0][0] __________________________________________________________________________________________________ stage1_unit3_conv1 (Conv2D) (None, 56, 56, 64) 36864 zero_padding2d_7[0][0] __________________________________________________________________________________________________ stage1_unit3_bn2 (BatchNormaliz (None, 56, 56, 64) 256 stage1_unit3_conv1[0][0] __________________________________________________________________________________________________ stage1_unit3_relu2 (Activation) (None, 56, 56, 64) 0 stage1_unit3_bn2[0][0] __________________________________________________________________________________________________ zero_padding2d_8 (ZeroPadding2D (None, 58, 58, 64) 0 stage1_unit3_relu2[0][0] __________________________________________________________________________________________________ stage1_unit3_conv2 (Conv2D) (None, 56, 56, 64) 36864 zero_padding2d_8[0][0] __________________________________________________________________________________________________ add_3 (Add) (None, 56, 56, 64) 0 stage1_unit3_conv2[0][0] add_2[0][0] __________________________________________________________________________________________________ stage2_unit1_bn1 (BatchNormaliz (None, 56, 56, 64) 256 add_3[0][0] __________________________________________________________________________________________________ stage2_unit1_relu1 (Activation) (None, 56, 56, 64) 0 stage2_unit1_bn1[0][0] __________________________________________________________________________________________________ zero_padding2d_9 (ZeroPadding2D (None, 58, 58, 64) 0 stage2_unit1_relu1[0][0] __________________________________________________________________________________________________ stage2_unit1_conv1 (Conv2D) (None, 28, 28, 128) 73728 zero_padding2d_9[0][0] __________________________________________________________________________________________________ stage2_unit1_bn2 (BatchNormaliz (None, 28, 28, 128) 512 stage2_unit1_conv1[0][0] __________________________________________________________________________________________________ stage2_unit1_relu2 (Activation) (None, 28, 28, 128) 0 stage2_unit1_bn2[0][0] __________________________________________________________________________________________________ zero_padding2d_10 (ZeroPadding2 (None, 30, 30, 128) 0 stage2_unit1_relu2[0][0] __________________________________________________________________________________________________ stage2_unit1_conv2 (Conv2D) (None, 28, 28, 128) 147456 zero_padding2d_10[0][0] __________________________________________________________________________________________________ stage2_unit1_sc (Conv2D) (None, 28, 28, 128) 8192 stage2_unit1_relu1[0][0] __________________________________________________________________________________________________ add_4 (Add) (None, 28, 28, 128) 0 stage2_unit1_conv2[0][0] stage2_unit1_sc[0][0] __________________________________________________________________________________________________ stage2_unit2_bn1 (BatchNormaliz (None, 28, 28, 128) 512 add_4[0][0] __________________________________________________________________________________________________ stage2_unit2_relu1 (Activation) (None, 28, 28, 128) 0 stage2_unit2_bn1[0][0] __________________________________________________________________________________________________ zero_padding2d_11 (ZeroPadding2 (None, 30, 30, 128) 0 stage2_unit2_relu1[0][0] __________________________________________________________________________________________________ stage2_unit2_conv1 (Conv2D) (None, 28, 28, 128) 147456 zero_padding2d_11[0][0] __________________________________________________________________________________________________ stage2_unit2_bn2 (BatchNormaliz (None, 28, 28, 128) 512 stage2_unit2_conv1[0][0] __________________________________________________________________________________________________ stage2_unit2_relu2 (Activation) (None, 28, 28, 128) 0 stage2_unit2_bn2[0][0] __________________________________________________________________________________________________ zero_padding2d_12 (ZeroPadding2 (None, 30, 30, 128) 0 stage2_unit2_relu2[0][0] __________________________________________________________________________________________________ stage2_unit2_conv2 (Conv2D) (None, 28, 28, 128) 147456 zero_padding2d_12[0][0] __________________________________________________________________________________________________ add_5 (Add) (None, 28, 28, 128) 0 stage2_unit2_conv2[0][0] add_4[0][0] __________________________________________________________________________________________________ stage2_unit3_bn1 (BatchNormaliz (None, 28, 28, 128) 512 add_5[0][0] __________________________________________________________________________________________________ stage2_unit3_relu1 (Activation) (None, 28, 28, 128) 0 stage2_unit3_bn1[0][0] __________________________________________________________________________________________________ zero_padding2d_13 (ZeroPadding2 (None, 30, 30, 128) 0 stage2_unit3_relu1[0][0] __________________________________________________________________________________________________ stage2_unit3_conv1 (Conv2D) (None, 28, 28, 128) 147456 zero_padding2d_13[0][0] __________________________________________________________________________________________________ stage2_unit3_bn2 (BatchNormaliz (None, 28, 28, 128) 512 stage2_unit3_conv1[0][0] __________________________________________________________________________________________________ stage2_unit3_relu2 (Activation) (None, 28, 28, 128) 0 stage2_unit3_bn2[0][0] __________________________________________________________________________________________________ zero_padding2d_14 (ZeroPadding2 (None, 30, 30, 128) 0 stage2_unit3_relu2[0][0] __________________________________________________________________________________________________ stage2_unit3_conv2 (Conv2D) (None, 28, 28, 128) 147456 zero_padding2d_14[0][0] __________________________________________________________________________________________________ add_6 (Add) (None, 28, 28, 128) 0 stage2_unit3_conv2[0][0] add_5[0][0] __________________________________________________________________________________________________ stage2_unit4_bn1 (BatchNormaliz (None, 28, 28, 128) 512 add_6[0][0] __________________________________________________________________________________________________ stage2_unit4_relu1 (Activation) (None, 28, 28, 128) 0 stage2_unit4_bn1[0][0] __________________________________________________________________________________________________ zero_padding2d_15 (ZeroPadding2 (None, 30, 30, 128) 0 stage2_unit4_relu1[0][0] __________________________________________________________________________________________________ stage2_unit4_conv1 (Conv2D) (None, 28, 28, 128) 147456 zero_padding2d_15[0][0] __________________________________________________________________________________________________ stage2_unit4_bn2 (BatchNormaliz (None, 28, 28, 128) 512 stage2_unit4_conv1[0][0] __________________________________________________________________________________________________ stage2_unit4_relu2 (Activation) (None, 28, 28, 128) 0 stage2_unit4_bn2[0][0] __________________________________________________________________________________________________ zero_padding2d_16 (ZeroPadding2 (None, 30, 30, 128) 0 stage2_unit4_relu2[0][0] __________________________________________________________________________________________________ stage2_unit4_conv2 (Conv2D) (None, 28, 28, 128) 147456 zero_padding2d_16[0][0] __________________________________________________________________________________________________ add_7 (Add) (None, 28, 28, 128) 0 stage2_unit4_conv2[0][0] add_6[0][0] __________________________________________________________________________________________________ stage3_unit1_bn1 (BatchNormaliz (None, 28, 28, 128) 512 add_7[0][0] __________________________________________________________________________________________________ stage3_unit1_relu1 (Activation) (None, 28, 28, 128) 0 stage3_unit1_bn1[0][0] __________________________________________________________________________________________________ zero_padding2d_17 (ZeroPadding2 (None, 30, 30, 128) 0 stage3_unit1_relu1[0][0] __________________________________________________________________________________________________ stage3_unit1_conv1 (Conv2D) (None, 14, 14, 256) 294912 zero_padding2d_17[0][0] __________________________________________________________________________________________________ stage3_unit1_bn2 (BatchNormaliz (None, 14, 14, 256) 1024 stage3_unit1_conv1[0][0] __________________________________________________________________________________________________ stage3_unit1_relu2 (Activation) (None, 14, 14, 256) 0 stage3_unit1_bn2[0][0] __________________________________________________________________________________________________ zero_padding2d_18 (ZeroPadding2 (None, 16, 16, 256) 0 stage3_unit1_relu2[0][0] __________________________________________________________________________________________________ stage3_unit1_conv2 (Conv2D) (None, 14, 14, 256) 589824 zero_padding2d_18[0][0] __________________________________________________________________________________________________ stage3_unit1_sc (Conv2D) (None, 14, 14, 256) 32768 stage3_unit1_relu1[0][0] __________________________________________________________________________________________________ add_8 (Add) (None, 14, 14, 256) 0 stage3_unit1_conv2[0][0] stage3_unit1_sc[0][0] __________________________________________________________________________________________________ stage3_unit2_bn1 (BatchNormaliz (None, 14, 14, 256) 1024 add_8[0][0] __________________________________________________________________________________________________ stage3_unit2_relu1 (Activation) (None, 14, 14, 256) 0 stage3_unit2_bn1[0][0] __________________________________________________________________________________________________ zero_padding2d_19 (ZeroPadding2 (None, 16, 16, 256) 0 stage3_unit2_relu1[0][0] __________________________________________________________________________________________________ stage3_unit2_conv1 (Conv2D) (None, 14, 14, 256) 589824 zero_padding2d_19[0][0] __________________________________________________________________________________________________ stage3_unit2_bn2 (BatchNormaliz (None, 14, 14, 256) 1024 stage3_unit2_conv1[0][0] __________________________________________________________________________________________________ stage3_unit2_relu2 (Activation) (None, 14, 14, 256) 0 stage3_unit2_bn2[0][0] __________________________________________________________________________________________________ zero_padding2d_20 (ZeroPadding2 (None, 16, 16, 256) 0 stage3_unit2_relu2[0][0] __________________________________________________________________________________________________ stage3_unit2_conv2 (Conv2D) (None, 14, 14, 256) 589824 zero_padding2d_20[0][0] __________________________________________________________________________________________________ add_9 (Add) (None, 14, 14, 256) 0 stage3_unit2_conv2[0][0] add_8[0][0] __________________________________________________________________________________________________ stage3_unit3_bn1 (BatchNormaliz (None, 14, 14, 256) 1024 add_9[0][0] __________________________________________________________________________________________________ stage3_unit3_relu1 (Activation) (None, 14, 14, 256) 0 stage3_unit3_bn1[0][0] __________________________________________________________________________________________________ zero_padding2d_21 (ZeroPadding2 (None, 16, 16, 256) 0 stage3_unit3_relu1[0][0] __________________________________________________________________________________________________ stage3_unit3_conv1 (Conv2D) (None, 14, 14, 256) 589824 zero_padding2d_21[0][0] __________________________________________________________________________________________________ stage3_unit3_bn2 (BatchNormaliz (None, 14, 14, 256) 1024 stage3_unit3_conv1[0][0] __________________________________________________________________________________________________ stage3_unit3_relu2 (Activation) (None, 14, 14, 256) 0 stage3_unit3_bn2[0][0] __________________________________________________________________________________________________ zero_padding2d_22 (ZeroPadding2 (None, 16, 16, 256) 0 stage3_unit3_relu2[0][0] __________________________________________________________________________________________________ stage3_unit3_conv2 (Conv2D) (None, 14, 14, 256) 589824 zero_padding2d_22[0][0] __________________________________________________________________________________________________ add_10 (Add) (None, 14, 14, 256) 0 stage3_unit3_conv2[0][0] add_9[0][0] __________________________________________________________________________________________________ stage3_unit4_bn1 (BatchNormaliz (None, 14, 14, 256) 1024 add_10[0][0] __________________________________________________________________________________________________ stage3_unit4_relu1 (Activation) (None, 14, 14, 256) 0 stage3_unit4_bn1[0][0] __________________________________________________________________________________________________ zero_padding2d_23 (ZeroPadding2 (None, 16, 16, 256) 0 stage3_unit4_relu1[0][0] __________________________________________________________________________________________________ stage3_unit4_conv1 (Conv2D) (None, 14, 14, 256) 589824 zero_padding2d_23[0][0] __________________________________________________________________________________________________ stage3_unit4_bn2 (BatchNormaliz (None, 14, 14, 256) 1024 stage3_unit4_conv1[0][0] __________________________________________________________________________________________________ stage3_unit4_relu2 (Activation) (None, 14, 14, 256) 0 stage3_unit4_bn2[0][0] __________________________________________________________________________________________________ zero_padding2d_24 (ZeroPadding2 (None, 16, 16, 256) 0 stage3_unit4_relu2[0][0] __________________________________________________________________________________________________ stage3_unit4_conv2 (Conv2D) (None, 14, 14, 256) 589824 zero_padding2d_24[0][0] __________________________________________________________________________________________________ add_11 (Add) (None, 14, 14, 256) 0 stage3_unit4_conv2[0][0] add_10[0][0] __________________________________________________________________________________________________ stage3_unit5_bn1 (BatchNormaliz (None, 14, 14, 256) 1024 add_11[0][0] __________________________________________________________________________________________________ stage3_unit5_relu1 (Activation) (None, 14, 14, 256) 0 stage3_unit5_bn1[0][0] __________________________________________________________________________________________________ zero_padding2d_25 (ZeroPadding2 (None, 16, 16, 256) 0 stage3_unit5_relu1[0][0] __________________________________________________________________________________________________ stage3_unit5_conv1 (Conv2D) (None, 14, 14, 256) 589824 zero_padding2d_25[0][0] __________________________________________________________________________________________________ stage3_unit5_bn2 (BatchNormaliz (None, 14, 14, 256) 1024 stage3_unit5_conv1[0][0] __________________________________________________________________________________________________ stage3_unit5_relu2 (Activation) (None, 14, 14, 256) 0 stage3_unit5_bn2[0][0] __________________________________________________________________________________________________ zero_padding2d_26 (ZeroPadding2 (None, 16, 16, 256) 0 stage3_unit5_relu2[0][0] __________________________________________________________________________________________________ stage3_unit5_conv2 (Conv2D) (None, 14, 14, 256) 589824 zero_padding2d_26[0][0] __________________________________________________________________________________________________ add_12 (Add) (None, 14, 14, 256) 0 stage3_unit5_conv2[0][0] add_11[0][0] __________________________________________________________________________________________________ stage3_unit6_bn1 (BatchNormaliz (None, 14, 14, 256) 1024 add_12[0][0] __________________________________________________________________________________________________ stage3_unit6_relu1 (Activation) (None, 14, 14, 256) 0 stage3_unit6_bn1[0][0] __________________________________________________________________________________________________ zero_padding2d_27 (ZeroPadding2 (None, 16, 16, 256) 0 stage3_unit6_relu1[0][0] __________________________________________________________________________________________________ stage3_unit6_conv1 (Conv2D) (None, 14, 14, 256) 589824 zero_padding2d_27[0][0] __________________________________________________________________________________________________ stage3_unit6_bn2 (BatchNormaliz (None, 14, 14, 256) 1024 stage3_unit6_conv1[0][0] __________________________________________________________________________________________________ stage3_unit6_relu2 (Activation) (None, 14, 14, 256) 0 stage3_unit6_bn2[0][0] __________________________________________________________________________________________________ zero_padding2d_28 (ZeroPadding2 (None, 16, 16, 256) 0 stage3_unit6_relu2[0][0] __________________________________________________________________________________________________ stage3_unit6_conv2 (Conv2D) (None, 14, 14, 256) 589824 zero_padding2d_28[0][0] __________________________________________________________________________________________________ add_13 (Add) (None, 14, 14, 256) 0 stage3_unit6_conv2[0][0] add_12[0][0] __________________________________________________________________________________________________ stage4_unit1_bn1 (BatchNormaliz (None, 14, 14, 256) 1024 add_13[0][0] __________________________________________________________________________________________________ stage4_unit1_relu1 (Activation) (None, 14, 14, 256) 0 stage4_unit1_bn1[0][0] __________________________________________________________________________________________________ zero_padding2d_29 (ZeroPadding2 (None, 16, 16, 256) 0 stage4_unit1_relu1[0][0] __________________________________________________________________________________________________ stage4_unit1_conv1 (Conv2D) (None, 7, 7, 512) 1179648 zero_padding2d_29[0][0] __________________________________________________________________________________________________ stage4_unit1_bn2 (BatchNormaliz (None, 7, 7, 512) 2048 stage4_unit1_conv1[0][0] __________________________________________________________________________________________________ stage4_unit1_relu2 (Activation) (None, 7, 7, 512) 0 stage4_unit1_bn2[0][0] __________________________________________________________________________________________________ zero_padding2d_30 (ZeroPadding2 (None, 9, 9, 512) 0 stage4_unit1_relu2[0][0] __________________________________________________________________________________________________ stage4_unit1_conv2 (Conv2D) (None, 7, 7, 512) 2359296 zero_padding2d_30[0][0] __________________________________________________________________________________________________ stage4_unit1_sc (Conv2D) (None, 7, 7, 512) 131072 stage4_unit1_relu1[0][0] __________________________________________________________________________________________________ add_14 (Add) (None, 7, 7, 512) 0 stage4_unit1_conv2[0][0] stage4_unit1_sc[0][0] __________________________________________________________________________________________________ stage4_unit2_bn1 (BatchNormaliz (None, 7, 7, 512) 2048 add_14[0][0] __________________________________________________________________________________________________ stage4_unit2_relu1 (Activation) (None, 7, 7, 512) 0 stage4_unit2_bn1[0][0] __________________________________________________________________________________________________ zero_padding2d_31 (ZeroPadding2 (None, 9, 9, 512) 0 stage4_unit2_relu1[0][0] __________________________________________________________________________________________________ stage4_unit2_conv1 (Conv2D) (None, 7, 7, 512) 2359296 zero_padding2d_31[0][0] __________________________________________________________________________________________________ stage4_unit2_bn2 (BatchNormaliz (None, 7, 7, 512) 2048 stage4_unit2_conv1[0][0] __________________________________________________________________________________________________ stage4_unit2_relu2 (Activation) (None, 7, 7, 512) 0 stage4_unit2_bn2[0][0] __________________________________________________________________________________________________ zero_padding2d_32 (ZeroPadding2 (None, 9, 9, 512) 0 stage4_unit2_relu2[0][0] __________________________________________________________________________________________________ stage4_unit2_conv2 (Conv2D) (None, 7, 7, 512) 2359296 zero_padding2d_32[0][0] __________________________________________________________________________________________________ add_15 (Add) (None, 7, 7, 512) 0 stage4_unit2_conv2[0][0] add_14[0][0] __________________________________________________________________________________________________ stage4_unit3_bn1 (BatchNormaliz (None, 7, 7, 512) 2048 add_15[0][0] __________________________________________________________________________________________________ stage4_unit3_relu1 (Activation) (None, 7, 7, 512) 0 stage4_unit3_bn1[0][0] __________________________________________________________________________________________________ zero_padding2d_33 (ZeroPadding2 (None, 9, 9, 512) 0 stage4_unit3_relu1[0][0] __________________________________________________________________________________________________ stage4_unit3_conv1 (Conv2D) (None, 7, 7, 512) 2359296 zero_padding2d_33[0][0] __________________________________________________________________________________________________ stage4_unit3_bn2 (BatchNormaliz (None, 7, 7, 512) 2048 stage4_unit3_conv1[0][0] __________________________________________________________________________________________________ stage4_unit3_relu2 (Activation) (None, 7, 7, 512) 0 stage4_unit3_bn2[0][0] __________________________________________________________________________________________________ zero_padding2d_34 (ZeroPadding2 (None, 9, 9, 512) 0 stage4_unit3_relu2[0][0] __________________________________________________________________________________________________ stage4_unit3_conv2 (Conv2D) (None, 7, 7, 512) 2359296 zero_padding2d_34[0][0] __________________________________________________________________________________________________ add_16 (Add) (None, 7, 7, 512) 0 stage4_unit3_conv2[0][0] add_15[0][0] __________________________________________________________________________________________________ bn1 (BatchNormalization) (None, 7, 7, 512) 2048 add_16[0][0] __________________________________________________________________________________________________ relu1 (Activation) (None, 7, 7, 512) 0 bn1[0][0] __________________________________________________________________________________________________ global_average_pooling2d_1 (Glo (None, 512) 0 relu1[0][0] __________________________________________________________________________________________________ dense_1 (Dense) (None, 100) 51200 global_average_pooling2d_1[0][0] __________________________________________________________________________________________________ activation_1 (Activation) (None, 100) 0 dense_1[0][0] ================================================================================================== Total params: 21,353,673 Trainable params: 4,772,864 Non-trainable params: 16,580,809 __________________________________________________________________________________________________ . Compile the model using Stochastic Gradient descent optimizer with momentum of 0.9 and lr of 0.015 . from keras.optimizers import SGD opt=SGD(lr=0.015, momentum=0.9, nesterov=True) model.compile(optimizer=opt, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead. . we want to get the model with best validation accuracy for the prediction task and so we will save the best model from the various epochs in Google Drive using ModelCheckpoint callback available in Keras . define a Modelcheckpoint to save the best Model . from keras.callbacks import ModelCheckpoint model_save_path=&#39;/gdrive/My Drive/EVA/session20/best_model2.h5&#39; chkpoint_model=ModelCheckpoint(model_save_path, monitor=&#39;val_acc&#39;, verbose=1, save_best_only=True, save_weights_only=False, mode=&#39;max&#39;) . Cutout Augmentation . Cutout was first presented as an effective augmentation technique in these two papers : . Improved Regularization of Convolutional Neural Networks with Cutout and Random Erasing Data Augmentation . The idea is to randomly cut away patches of information from images that a model is training on to force it to learn from more parts of the image. This would help the model learn more features about a class instead of depending on some simple assumptions using smaller areas within the image . This helps the model generalize better and make better predictions . . We will use python code for random erasing found at https://github.com/yu4u/cutout-random-erasing . !wget https://raw.githubusercontent.com/yu4u/cutout-random-erasing/master/random_eraser.py . --2019-12-04 02:57:37-- https://raw.githubusercontent.com/yu4u/cutout-random-erasing/master/random_eraser.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 888 [text/plain] Saving to: ‘random_eraser.py’ random_eraser.py 100%[===================&gt;] 888 --.-KB/s in 0s 2019-12-04 02:57:37 (191 MB/s) - ‘random_eraser.py’ saved [888/888] . Train the model for 100 epochs using a batch size of 128 . We will use a ImageDataGenerator to apply image augmentation of random-pad-crop, horizontal Flip and CutOut augmentation for the training . from random_eraser import get_random_eraser eraser = get_random_eraser(p=0.8, s_l=0.15, s_h=0.25,r_1=0.5, r_2=1/0.5,v_l=0,v_h=255,pixel_level=False) def img_aug1(img): img=random_pad_crop_img(img) img=eraser(img) return img . def scheduler(epoch): if epoch &lt; 30: return 0.01 elif 30 &lt; epoch &lt; 50: return 0.008 else: return 0.008 * tensorflow.math.exp(0.1 * (50 - epoch)) lr_callback = keras.callbacks.LearningRateScheduler(scheduler) . from tensorflow.keras.preprocessing.image import ImageDataGenerator EPOCHS=100 batch_size=128 train_datagen=ImageDataGenerator( preprocessing_function=img_aug1, horizontal_flip=True ) val_datagen= ImageDataGenerator( ) training_generator = train_datagen.flow_from_dataframe(train_df, directory=&#39;./data/train/&#39;, x_col=&#39;File&#39;, y_col=&#39;Class&#39;, target_size=(224, 224), color_mode=&#39;rgb&#39;, interpolation=&#39;bicubic&#39;, class_mode=&#39;categorical&#39;, batch_size=batch_size, shuffle=True, seed=42) validation_generator = val_datagen.flow_from_dataframe(test_df, directory=&#39;./data/test/&#39;, x_col=&#39;File&#39;, y_col=&#39;Class&#39;, target_size=(224, 224),interpolation=&#39;bicubic&#39;, color_mode=&#39;rgb&#39;, class_mode=&#39;categorical&#39;, batch_size=batch_size, shuffle=True, seed=42) . Found 50000 validated image filenames belonging to 100 classes. Found 10000 validated image filenames belonging to 100 classes. . def scheduler(epoch): if epoch &lt; 5: return 0.02 elif 5 &lt; epoch &lt; 12: return 0.015 elif 12 &lt; epoch &lt; 20: return 0.010 elif 20 &lt; epoch &lt; 25: return 0.007 else: return 0.003 lr_callback = keras.callbacks.LearningRateScheduler(scheduler) . model.fit_generator(training_generator, epochs=30, steps_per_epoch=np.ceil(train_features.shape[0]/batch_size), validation_steps=np.ceil(test_features.shape[0]/batch_size), validation_data=validation_generator, shuffle=True, callbacks=[chkpoint_model,lr_callback], verbose=1) . WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead. Epoch 1/30 391/391 [==============================] - 143s 367ms/step - loss: 3.0429 - acc: 0.2592 - val_loss: 3.3339 - val_acc: 0.2888 Epoch 00001: val_acc improved from -inf to 0.28880, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 2/30 391/391 [==============================] - 131s 334ms/step - loss: 2.4142 - acc: 0.3753 - val_loss: 2.7019 - val_acc: 0.3728 Epoch 00002: val_acc improved from 0.28880 to 0.37280, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 3/30 391/391 [==============================] - 130s 332ms/step - loss: 2.2485 - acc: 0.4119 - val_loss: 2.9838 - val_acc: 0.3559 Epoch 00003: val_acc did not improve from 0.37280 Epoch 4/30 391/391 [==============================] - 130s 332ms/step - loss: 2.1262 - acc: 0.4380 - val_loss: 2.9980 - val_acc: 0.3657 Epoch 00004: val_acc did not improve from 0.37280 Epoch 5/30 391/391 [==============================] - 130s 333ms/step - loss: 2.0500 - acc: 0.4526 - val_loss: 3.5103 - val_acc: 0.3361 Epoch 00005: val_acc did not improve from 0.37280 Epoch 6/30 391/391 [==============================] - 129s 331ms/step - loss: 1.8938 - acc: 0.4893 - val_loss: 3.0115 - val_acc: 0.3799 Epoch 00006: val_acc improved from 0.37280 to 0.37990, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 7/30 391/391 [==============================] - 129s 331ms/step - loss: 1.9202 - acc: 0.4823 - val_loss: 3.1336 - val_acc: 0.3733 Epoch 00007: val_acc did not improve from 0.37990 Epoch 8/30 391/391 [==============================] - 129s 330ms/step - loss: 1.8907 - acc: 0.4902 - val_loss: 3.0686 - val_acc: 0.3787 Epoch 00008: val_acc did not improve from 0.37990 Epoch 9/30 391/391 [==============================] - 130s 332ms/step - loss: 1.8420 - acc: 0.5018 - val_loss: 3.2207 - val_acc: 0.3743 Epoch 00009: val_acc did not improve from 0.37990 Epoch 10/30 391/391 [==============================] - 129s 331ms/step - loss: 1.8068 - acc: 0.5090 - val_loss: 3.5011 - val_acc: 0.3613 Epoch 00010: val_acc did not improve from 0.37990 Epoch 11/30 391/391 [==============================] - 129s 330ms/step - loss: 1.7793 - acc: 0.5141 - val_loss: 3.0006 - val_acc: 0.3945 Epoch 00011: val_acc improved from 0.37990 to 0.39450, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 12/30 391/391 [==============================] - 129s 331ms/step - loss: 1.7491 - acc: 0.5239 - val_loss: 3.0046 - val_acc: 0.4053 Epoch 00012: val_acc improved from 0.39450 to 0.40530, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 13/30 391/391 [==============================] - 130s 332ms/step - loss: 1.6473 - acc: 0.5471 - val_loss: 3.0496 - val_acc: 0.4013 Epoch 00013: val_acc did not improve from 0.40530 Epoch 14/30 391/391 [==============================] - 129s 331ms/step - loss: 1.6570 - acc: 0.5460 - val_loss: 3.2120 - val_acc: 0.3886 Epoch 00014: val_acc did not improve from 0.40530 Epoch 15/30 391/391 [==============================] - 129s 330ms/step - loss: 1.6426 - acc: 0.5465 - val_loss: 2.8877 - val_acc: 0.4152 Epoch 00015: val_acc improved from 0.40530 to 0.41520, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 16/30 391/391 [==============================] - 128s 328ms/step - loss: 1.6199 - acc: 0.5533 - val_loss: 3.2430 - val_acc: 0.3968 Epoch 00016: val_acc did not improve from 0.41520 Epoch 17/30 391/391 [==============================] - 122s 311ms/step - loss: 1.6093 - acc: 0.5553 - val_loss: 3.3633 - val_acc: 0.3853 Epoch 00017: val_acc did not improve from 0.41520 Epoch 18/30 391/391 [==============================] - 122s 312ms/step - loss: 1.5891 - acc: 0.5611 - val_loss: 3.5916 - val_acc: 0.3703 Epoch 00018: val_acc did not improve from 0.41520 Epoch 19/30 391/391 [==============================] - 121s 310ms/step - loss: 1.5707 - acc: 0.5665 - val_loss: 2.8044 - val_acc: 0.4265 Epoch 00019: val_acc improved from 0.41520 to 0.42650, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 20/30 391/391 [==============================] - 122s 311ms/step - loss: 1.5550 - acc: 0.5687 - val_loss: 2.9007 - val_acc: 0.4178 Epoch 00020: val_acc did not improve from 0.42650 Epoch 21/30 391/391 [==============================] - 122s 312ms/step - loss: 1.4868 - acc: 0.5854 - val_loss: 3.1454 - val_acc: 0.4037 Epoch 00021: val_acc did not improve from 0.42650 Epoch 22/30 391/391 [==============================] - 121s 310ms/step - loss: 1.5035 - acc: 0.5839 - val_loss: 3.3994 - val_acc: 0.3845 Epoch 00022: val_acc did not improve from 0.42650 Epoch 23/30 391/391 [==============================] - 121s 310ms/step - loss: 1.4860 - acc: 0.5860 - val_loss: 3.1657 - val_acc: 0.4043 Epoch 00023: val_acc did not improve from 0.42650 Epoch 24/30 391/391 [==============================] - 120s 308ms/step - loss: 1.4809 - acc: 0.5875 - val_loss: 3.0020 - val_acc: 0.4094 Epoch 00024: val_acc did not improve from 0.42650 Epoch 25/30 391/391 [==============================] - 121s 309ms/step - loss: 1.4730 - acc: 0.5906 - val_loss: 2.8147 - val_acc: 0.4342 Epoch 00025: val_acc improved from 0.42650 to 0.43420, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 26/30 391/391 [==============================] - 122s 312ms/step - loss: 1.4163 - acc: 0.6054 - val_loss: 2.9704 - val_acc: 0.4182 Epoch 00026: val_acc did not improve from 0.43420 Epoch 27/30 391/391 [==============================] - 121s 309ms/step - loss: 1.4087 - acc: 0.6058 - val_loss: 2.8172 - val_acc: 0.4336 Epoch 00027: val_acc did not improve from 0.43420 Epoch 28/30 391/391 [==============================] - 121s 309ms/step - loss: 1.3998 - acc: 0.6061 - val_loss: 2.9284 - val_acc: 0.4253 Epoch 00028: val_acc did not improve from 0.43420 Epoch 29/30 391/391 [==============================] - 120s 308ms/step - loss: 1.3893 - acc: 0.6134 - val_loss: 2.9667 - val_acc: 0.4245 Epoch 00029: val_acc did not improve from 0.43420 Epoch 30/30 391/391 [==============================] - 121s 309ms/step - loss: 1.3943 - acc: 0.6102 - val_loss: 2.9288 - val_acc: 0.4246 Epoch 00030: val_acc did not improve from 0.43420 . &lt;keras.callbacks.History at 0x7feeefb15898&gt; . After the initial 30 epochs of training the last few layers , now unfreeze all the layers and train again for 100 epochs . for layer in model.layers: layer.trainable=True . opt=SGD(lr=0.01, momentum=0.9, nesterov=True) model.compile(optimizer=opt, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . import math def scheduler1(epoch): if epoch &lt; 15: return 0.01 elif 15 &lt; epoch &lt; 30: return 0.008 else: return 0.008 * math.exp(0.1 * (30 - epoch)) lr_callback = keras.callbacks.LearningRateScheduler(scheduler1) . model.fit_generator(training_generator, epochs=EPOCHS, steps_per_epoch=np.ceil(train_features.shape[0]/batch_size), validation_steps=np.ceil(test_features.shape[0]/batch_size), validation_data=validation_generator, shuffle=True, callbacks=[chkpoint_model,lr_callback], verbose=1) . Epoch 1/100 391/391 [==============================] - 186s 476ms/step - loss: 1.2726 - acc: 0.6361 - val_loss: 1.2386 - val_acc: 0.6603 Epoch 00001: val_acc improved from 0.43420 to 0.66030, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 2/100 391/391 [==============================] - 178s 456ms/step - loss: 0.9419 - acc: 0.7234 - val_loss: 1.0315 - val_acc: 0.7018 Epoch 00002: val_acc improved from 0.66030 to 0.70180, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 3/100 391/391 [==============================] - 178s 456ms/step - loss: 0.8065 - acc: 0.7602 - val_loss: 0.9088 - val_acc: 0.7393 Epoch 00003: val_acc improved from 0.70180 to 0.73930, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 4/100 391/391 [==============================] - 178s 456ms/step - loss: 0.7124 - acc: 0.7846 - val_loss: 0.9617 - val_acc: 0.7312 Epoch 00004: val_acc did not improve from 0.73930 Epoch 5/100 391/391 [==============================] - 178s 455ms/step - loss: 0.6331 - acc: 0.8057 - val_loss: 0.9588 - val_acc: 0.7385 Epoch 00005: val_acc did not improve from 0.73930 Epoch 6/100 391/391 [==============================] - 179s 457ms/step - loss: 0.5726 - acc: 0.8238 - val_loss: 1.0137 - val_acc: 0.7265 Epoch 00006: val_acc did not improve from 0.73930 Epoch 7/100 391/391 [==============================] - 178s 455ms/step - loss: 0.5171 - acc: 0.8410 - val_loss: 0.9424 - val_acc: 0.7432 Epoch 00007: val_acc improved from 0.73930 to 0.74320, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 8/100 391/391 [==============================] - 179s 457ms/step - loss: 0.4767 - acc: 0.8544 - val_loss: 0.8891 - val_acc: 0.7581 Epoch 00008: val_acc improved from 0.74320 to 0.75810, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 9/100 391/391 [==============================] - 178s 455ms/step - loss: 0.4326 - acc: 0.8660 - val_loss: 0.9155 - val_acc: 0.7550 Epoch 00009: val_acc did not improve from 0.75810 Epoch 10/100 391/391 [==============================] - 178s 456ms/step - loss: 0.4012 - acc: 0.8755 - val_loss: 0.9171 - val_acc: 0.7544 Epoch 00010: val_acc did not improve from 0.75810 Epoch 11/100 391/391 [==============================] - 179s 457ms/step - loss: 0.3686 - acc: 0.8856 - val_loss: 0.9026 - val_acc: 0.7599 Epoch 00011: val_acc improved from 0.75810 to 0.75990, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 12/100 391/391 [==============================] - 179s 458ms/step - loss: 0.3393 - acc: 0.8954 - val_loss: 1.0303 - val_acc: 0.7399 Epoch 00012: val_acc did not improve from 0.75990 Epoch 13/100 391/391 [==============================] - 178s 456ms/step - loss: 0.3221 - acc: 0.9011 - val_loss: 0.9879 - val_acc: 0.7550 Epoch 00013: val_acc did not improve from 0.75990 Epoch 14/100 391/391 [==============================] - 179s 457ms/step - loss: 0.2955 - acc: 0.9081 - val_loss: 0.9063 - val_acc: 0.7609 Epoch 00014: val_acc improved from 0.75990 to 0.76090, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 15/100 391/391 [==============================] - 178s 456ms/step - loss: 0.2780 - acc: 0.9140 - val_loss: 0.9361 - val_acc: 0.7626 Epoch 00015: val_acc improved from 0.76090 to 0.76260, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 16/100 391/391 [==============================] - 179s 458ms/step - loss: 1.0478 - acc: 0.6974 - val_loss: 2.0535 - val_acc: 0.5705 Epoch 00016: val_acc did not improve from 0.76260 Epoch 17/100 391/391 [==============================] - 179s 457ms/step - loss: 0.5366 - acc: 0.8374 - val_loss: 0.8038 - val_acc: 0.7734 Epoch 00017: val_acc improved from 0.76260 to 0.77340, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 18/100 391/391 [==============================] - 179s 457ms/step - loss: 0.3469 - acc: 0.8952 - val_loss: 0.7776 - val_acc: 0.7876 Epoch 00018: val_acc improved from 0.77340 to 0.78760, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 19/100 391/391 [==============================] - 179s 457ms/step - loss: 0.2823 - acc: 0.9137 - val_loss: 0.7753 - val_acc: 0.7940 Epoch 00019: val_acc improved from 0.78760 to 0.79400, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 20/100 391/391 [==============================] - 178s 456ms/step - loss: 0.2419 - acc: 0.9262 - val_loss: 0.7797 - val_acc: 0.7936 Epoch 00020: val_acc did not improve from 0.79400 Epoch 21/100 391/391 [==============================] - 178s 456ms/step - loss: 0.2175 - acc: 0.9347 - val_loss: 0.8229 - val_acc: 0.7880 Epoch 00021: val_acc did not improve from 0.79400 Epoch 22/100 391/391 [==============================] - 178s 456ms/step - loss: 0.1976 - acc: 0.9403 - val_loss: 0.8246 - val_acc: 0.7911 Epoch 00022: val_acc did not improve from 0.79400 Epoch 23/100 391/391 [==============================] - 179s 457ms/step - loss: 0.1786 - acc: 0.9477 - val_loss: 0.8362 - val_acc: 0.7902 Epoch 00023: val_acc did not improve from 0.79400 Epoch 24/100 391/391 [==============================] - 178s 456ms/step - loss: 0.1649 - acc: 0.9520 - val_loss: 0.8262 - val_acc: 0.7909 Epoch 00024: val_acc did not improve from 0.79400 Epoch 25/100 391/391 [==============================] - 179s 457ms/step - loss: 0.1562 - acc: 0.9534 - val_loss: 0.8414 - val_acc: 0.7893 Epoch 00025: val_acc did not improve from 0.79400 Epoch 26/100 391/391 [==============================] - 178s 456ms/step - loss: 0.1447 - acc: 0.9576 - val_loss: 0.8416 - val_acc: 0.7896 Epoch 00026: val_acc did not improve from 0.79400 Epoch 27/100 391/391 [==============================] - 178s 456ms/step - loss: 0.1422 - acc: 0.9574 - val_loss: 0.8558 - val_acc: 0.7918 Epoch 00027: val_acc did not improve from 0.79400 Epoch 28/100 391/391 [==============================] - 178s 456ms/step - loss: 0.1323 - acc: 0.9607 - val_loss: 0.8532 - val_acc: 0.7924 Epoch 00028: val_acc did not improve from 0.79400 Epoch 29/100 391/391 [==============================] - 178s 456ms/step - loss: 0.1261 - acc: 0.9628 - val_loss: 0.8479 - val_acc: 0.7946 Epoch 00029: val_acc improved from 0.79400 to 0.79460, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 30/100 391/391 [==============================] - 178s 456ms/step - loss: 0.1132 - acc: 0.9682 - val_loss: 0.8551 - val_acc: 0.7941 Epoch 00030: val_acc did not improve from 0.79460 Epoch 31/100 391/391 [==============================] - 179s 458ms/step - loss: 0.1123 - acc: 0.9670 - val_loss: 0.8399 - val_acc: 0.7972 Epoch 00031: val_acc improved from 0.79460 to 0.79720, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 32/100 391/391 [==============================] - 178s 455ms/step - loss: 0.1035 - acc: 0.9703 - val_loss: 0.8720 - val_acc: 0.7908 Epoch 00032: val_acc did not improve from 0.79720 Epoch 33/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0928 - acc: 0.9738 - val_loss: 0.8403 - val_acc: 0.7952 Epoch 00033: val_acc did not improve from 0.79720 Epoch 34/100 391/391 [==============================] - 177s 454ms/step - loss: 0.0854 - acc: 0.9758 - val_loss: 0.8532 - val_acc: 0.7980 Epoch 00034: val_acc improved from 0.79720 to 0.79800, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 35/100 391/391 [==============================] - 177s 454ms/step - loss: 0.0800 - acc: 0.9777 - val_loss: 0.8207 - val_acc: 0.8023 Epoch 00035: val_acc improved from 0.79800 to 0.80230, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 36/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0731 - acc: 0.9793 - val_loss: 0.8210 - val_acc: 0.8059 Epoch 00036: val_acc improved from 0.80230 to 0.80590, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 37/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0691 - acc: 0.9804 - val_loss: 0.8296 - val_acc: 0.8046 Epoch 00037: val_acc did not improve from 0.80590 Epoch 38/100 391/391 [==============================] - 179s 457ms/step - loss: 0.0637 - acc: 0.9824 - val_loss: 0.8162 - val_acc: 0.8077 Epoch 00038: val_acc improved from 0.80590 to 0.80770, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 39/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0601 - acc: 0.9834 - val_loss: 0.8142 - val_acc: 0.8057 Epoch 00039: val_acc did not improve from 0.80770 Epoch 40/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0557 - acc: 0.9846 - val_loss: 0.8178 - val_acc: 0.8081 Epoch 00040: val_acc improved from 0.80770 to 0.80810, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 41/100 391/391 [==============================] - 179s 457ms/step - loss: 0.0529 - acc: 0.9859 - val_loss: 0.8090 - val_acc: 0.8095 Epoch 00041: val_acc improved from 0.80810 to 0.80950, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 42/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0487 - acc: 0.9872 - val_loss: 0.8091 - val_acc: 0.8043 Epoch 00042: val_acc did not improve from 0.80950 Epoch 43/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0520 - acc: 0.9862 - val_loss: 0.8063 - val_acc: 0.8071 Epoch 00043: val_acc did not improve from 0.80950 Epoch 44/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0459 - acc: 0.9884 - val_loss: 0.8085 - val_acc: 0.8090 Epoch 00044: val_acc did not improve from 0.80950 Epoch 45/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0463 - acc: 0.9879 - val_loss: 0.8072 - val_acc: 0.8082 Epoch 00045: val_acc did not improve from 0.80950 Epoch 46/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0439 - acc: 0.9880 - val_loss: 0.8002 - val_acc: 0.8131 Epoch 00046: val_acc improved from 0.80950 to 0.81310, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 47/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0416 - acc: 0.9891 - val_loss: 0.7974 - val_acc: 0.8099 Epoch 00047: val_acc did not improve from 0.81310 Epoch 48/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0422 - acc: 0.9888 - val_loss: 0.8042 - val_acc: 0.8090 Epoch 00048: val_acc did not improve from 0.81310 Epoch 49/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0381 - acc: 0.9901 - val_loss: 0.7994 - val_acc: 0.8109 Epoch 00049: val_acc did not improve from 0.81310 Epoch 50/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0410 - acc: 0.9899 - val_loss: 0.8007 - val_acc: 0.8108 Epoch 00050: val_acc did not improve from 0.81310 Epoch 51/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0404 - acc: 0.9896 - val_loss: 0.7964 - val_acc: 0.8119 Epoch 00051: val_acc did not improve from 0.81310 Epoch 52/100 391/391 [==============================] - 179s 457ms/step - loss: 0.0391 - acc: 0.9899 - val_loss: 0.8002 - val_acc: 0.8110 Epoch 00052: val_acc did not improve from 0.81310 Epoch 53/100 391/391 [==============================] - 179s 457ms/step - loss: 0.0389 - acc: 0.9898 - val_loss: 0.7957 - val_acc: 0.8115 Epoch 00053: val_acc did not improve from 0.81310 Epoch 54/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0381 - acc: 0.9907 - val_loss: 0.7948 - val_acc: 0.8119 Epoch 00054: val_acc did not improve from 0.81310 Epoch 55/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0374 - acc: 0.9907 - val_loss: 0.7925 - val_acc: 0.8108 Epoch 00055: val_acc did not improve from 0.81310 Epoch 56/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0358 - acc: 0.9916 - val_loss: 0.7959 - val_acc: 0.8118 Epoch 00056: val_acc did not improve from 0.81310 Epoch 57/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0373 - acc: 0.9906 - val_loss: 0.7964 - val_acc: 0.8115 Epoch 00057: val_acc did not improve from 0.81310 Epoch 58/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0355 - acc: 0.9908 - val_loss: 0.7961 - val_acc: 0.8110 Epoch 00058: val_acc did not improve from 0.81310 Epoch 59/100 391/391 [==============================] - 177s 453ms/step - loss: 0.0354 - acc: 0.9912 - val_loss: 0.7963 - val_acc: 0.8110 Epoch 00059: val_acc did not improve from 0.81310 Epoch 60/100 391/391 [==============================] - 177s 453ms/step - loss: 0.0374 - acc: 0.9904 - val_loss: 0.7965 - val_acc: 0.8107 Epoch 00060: val_acc did not improve from 0.81310 Epoch 61/100 391/391 [==============================] - 177s 453ms/step - loss: 0.0347 - acc: 0.9911 - val_loss: 0.7953 - val_acc: 0.8102 Epoch 00061: val_acc did not improve from 0.81310 Epoch 62/100 391/391 [==============================] - 177s 453ms/step - loss: 0.0352 - acc: 0.9910 - val_loss: 0.7961 - val_acc: 0.8093 Epoch 00062: val_acc did not improve from 0.81310 Epoch 63/100 391/391 [==============================] - 178s 454ms/step - loss: 0.0333 - acc: 0.9921 - val_loss: 0.7956 - val_acc: 0.8109 Epoch 00063: val_acc did not improve from 0.81310 Epoch 64/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0356 - acc: 0.9910 - val_loss: 0.7947 - val_acc: 0.8099 Epoch 00064: val_acc did not improve from 0.81310 Epoch 65/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0353 - acc: 0.9909 - val_loss: 0.7954 - val_acc: 0.8104 Epoch 00065: val_acc did not improve from 0.81310 Epoch 66/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0353 - acc: 0.9912 - val_loss: 0.7957 - val_acc: 0.8120 Epoch 00066: val_acc did not improve from 0.81310 Epoch 67/100 391/391 [==============================] - 179s 458ms/step - loss: 0.0337 - acc: 0.9920 - val_loss: 0.7941 - val_acc: 0.8112 Epoch 00067: val_acc did not improve from 0.81310 Epoch 68/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0336 - acc: 0.9913 - val_loss: 0.7948 - val_acc: 0.8115 Epoch 00068: val_acc did not improve from 0.81310 Epoch 69/100 391/391 [==============================] - 179s 457ms/step - loss: 0.0345 - acc: 0.9912 - val_loss: 0.7934 - val_acc: 0.8121 Epoch 00069: val_acc did not improve from 0.81310 Epoch 70/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0362 - acc: 0.9907 - val_loss: 0.7947 - val_acc: 0.8124 Epoch 00070: val_acc did not improve from 0.81310 Epoch 71/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0343 - acc: 0.9911 - val_loss: 0.7944 - val_acc: 0.8111 Epoch 00071: val_acc did not improve from 0.81310 Epoch 72/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0348 - acc: 0.9916 - val_loss: 0.7935 - val_acc: 0.8124 Epoch 00072: val_acc did not improve from 0.81310 Epoch 73/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0341 - acc: 0.9913 - val_loss: 0.7945 - val_acc: 0.8117 Epoch 00073: val_acc did not improve from 0.81310 Epoch 74/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0322 - acc: 0.9924 - val_loss: 0.7942 - val_acc: 0.8110 Epoch 00074: val_acc did not improve from 0.81310 Epoch 75/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0341 - acc: 0.9915 - val_loss: 0.7944 - val_acc: 0.8121 Epoch 00075: val_acc did not improve from 0.81310 Epoch 76/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0341 - acc: 0.9915 - val_loss: 0.7950 - val_acc: 0.8117 Epoch 00076: val_acc did not improve from 0.81310 Epoch 77/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0333 - acc: 0.9913 - val_loss: 0.7945 - val_acc: 0.8117 Epoch 00077: val_acc did not improve from 0.81310 Epoch 78/100 391/391 [==============================] - 179s 458ms/step - loss: 0.0346 - acc: 0.9914 - val_loss: 0.7945 - val_acc: 0.8115 Epoch 00078: val_acc did not improve from 0.81310 Epoch 79/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0332 - acc: 0.9916 - val_loss: 0.7950 - val_acc: 0.8112 Epoch 00079: val_acc did not improve from 0.81310 Epoch 80/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0326 - acc: 0.9923 - val_loss: 0.7945 - val_acc: 0.8120 Epoch 00080: val_acc did not improve from 0.81310 Epoch 81/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0310 - acc: 0.9927 - val_loss: 0.7949 - val_acc: 0.8116 Epoch 00081: val_acc did not improve from 0.81310 Epoch 82/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0329 - acc: 0.9918 - val_loss: 0.7958 - val_acc: 0.8121 Epoch 00082: val_acc did not improve from 0.81310 Epoch 83/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0332 - acc: 0.9914 - val_loss: 0.7946 - val_acc: 0.8115 Epoch 00083: val_acc did not improve from 0.81310 Epoch 84/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0336 - acc: 0.9918 - val_loss: 0.7943 - val_acc: 0.8119 Epoch 00084: val_acc did not improve from 0.81310 Epoch 85/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0334 - acc: 0.9915 - val_loss: 0.7951 - val_acc: 0.8117 Epoch 00085: val_acc did not improve from 0.81310 Epoch 86/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0330 - acc: 0.9918 - val_loss: 0.7950 - val_acc: 0.8121 Epoch 00086: val_acc did not improve from 0.81310 Epoch 87/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0330 - acc: 0.9916 - val_loss: 0.7951 - val_acc: 0.8113 Epoch 00087: val_acc did not improve from 0.81310 Epoch 88/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0326 - acc: 0.9920 - val_loss: 0.7945 - val_acc: 0.8122 Epoch 00088: val_acc did not improve from 0.81310 Epoch 89/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0321 - acc: 0.9920 - val_loss: 0.7942 - val_acc: 0.8114 Epoch 00089: val_acc did not improve from 0.81310 Epoch 90/100 391/391 [==============================] - 177s 453ms/step - loss: 0.0327 - acc: 0.9922 - val_loss: 0.7952 - val_acc: 0.8114 Epoch 00090: val_acc did not improve from 0.81310 Epoch 91/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0325 - acc: 0.9919 - val_loss: 0.7947 - val_acc: 0.8105 Epoch 00091: val_acc did not improve from 0.81310 Epoch 92/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0329 - acc: 0.9921 - val_loss: 0.7953 - val_acc: 0.8117 Epoch 00092: val_acc did not improve from 0.81310 Epoch 93/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0329 - acc: 0.9918 - val_loss: 0.7949 - val_acc: 0.8109 Epoch 00093: val_acc did not improve from 0.81310 Epoch 94/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0333 - acc: 0.9916 - val_loss: 0.7943 - val_acc: 0.8111 Epoch 00094: val_acc did not improve from 0.81310 Epoch 95/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0316 - acc: 0.9924 - val_loss: 0.7950 - val_acc: 0.8117 Epoch 00095: val_acc did not improve from 0.81310 Epoch 96/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0326 - acc: 0.9926 - val_loss: 0.7947 - val_acc: 0.8113 Epoch 00096: val_acc did not improve from 0.81310 Epoch 97/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0333 - acc: 0.9919 - val_loss: 0.7950 - val_acc: 0.8118 Epoch 00097: val_acc did not improve from 0.81310 Epoch 98/100 391/391 [==============================] - 178s 456ms/step - loss: 0.0326 - acc: 0.9921 - val_loss: 0.7954 - val_acc: 0.8113 Epoch 00098: val_acc did not improve from 0.81310 Epoch 99/100 391/391 [==============================] - 178s 455ms/step - loss: 0.0332 - acc: 0.9920 - val_loss: 0.7947 - val_acc: 0.8107 Epoch 00099: val_acc did not improve from 0.81310 Epoch 100/100 391/391 [==============================] - 179s 457ms/step - loss: 0.0330 - acc: 0.9921 - val_loss: 0.7945 - val_acc: 0.8110 Epoch 00100: val_acc did not improve from 0.81310 . &lt;keras.callbacks.History at 0x7feeee498358&gt; . Val accuracy reached 80.23 at the end of 35th epoch and 81.31 at the end of 100 epochs .We have aleady reached our target of 80% val accuracy . Let us train another 100 epochs to see how much further we can push this validation accuracy . def scheduler2(epoch): if epoch &lt; 15: return 0.002 elif 15 &lt; epoch &lt; 30: return 0.001 elif 13 &lt; epoch &lt; 50: return 0.0005 else: return 0.0005 * math.exp(0.5 * (50 - epoch)) lr_callback = keras.callbacks.LearningRateScheduler(scheduler2) . opt=SGD(lr=0.002, momentum=0.9, nesterov=True) model.compile(optimizer=opt, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . train_datagen=ImageDataGenerator( #preprocessing_function=img_aug2, horizontal_flip=True,width_shift_range=0.05, height_shift_range=0.05 ) val_datagen= ImageDataGenerator( ) training_generator = train_datagen.flow_from_dataframe(train_df, directory=&#39;./data/train/&#39;, x_col=&#39;File&#39;, y_col=&#39;Class&#39;, target_size=(224, 224), color_mode=&#39;rgb&#39;, interpolation=&#39;bicubic&#39;, class_mode=&#39;categorical&#39;, batch_size=batch_size, shuffle=True, seed=42) validation_generator = val_datagen.flow_from_dataframe(test_df, directory=&#39;./data/test/&#39;, x_col=&#39;File&#39;, y_col=&#39;Class&#39;, target_size=(224, 224),interpolation=&#39;bicubic&#39;, color_mode=&#39;rgb&#39;, class_mode=&#39;categorical&#39;, batch_size=batch_size, shuffle=True, seed=42) . Found 50000 validated image filenames belonging to 100 classes. Found 10000 validated image filenames belonging to 100 classes. . model.fit_generator(training_generator, epochs=EPOCHS, steps_per_epoch=np.ceil(train_features.shape[0]/batch_size), validation_steps=np.ceil(test_features.shape[0]/batch_size), validation_data=validation_generator, shuffle=True, callbacks=[chkpoint_model,lr_callback], verbose=1) . Epoch 1/100 391/391 [==============================] - 572s 1s/step - loss: 0.0033 - acc: 0.9996 - val_loss: 0.7933 - val_acc: 0.8118 Epoch 00001: val_acc did not improve from 0.81310 Epoch 2/100 391/391 [==============================] - 560s 1s/step - loss: 0.0027 - acc: 0.9997 - val_loss: 0.7897 - val_acc: 0.8113 Epoch 00002: val_acc did not improve from 0.81310 Epoch 3/100 391/391 [==============================] - 563s 1s/step - loss: 0.0022 - acc: 0.9997 - val_loss: 0.7869 - val_acc: 0.8114 Epoch 00003: val_acc did not improve from 0.81310 Epoch 4/100 391/391 [==============================] - 557s 1s/step - loss: 0.0022 - acc: 0.9997 - val_loss: 0.7868 - val_acc: 0.8123 Epoch 00004: val_acc did not improve from 0.81310 Epoch 5/100 391/391 [==============================] - 558s 1s/step - loss: 0.0020 - acc: 0.9998 - val_loss: 0.7862 - val_acc: 0.8109 Epoch 00005: val_acc did not improve from 0.81310 Epoch 6/100 391/391 [==============================] - 559s 1s/step - loss: 0.0020 - acc: 0.9998 - val_loss: 0.7859 - val_acc: 0.8126 Epoch 00006: val_acc did not improve from 0.81310 Epoch 7/100 391/391 [==============================] - 556s 1s/step - loss: 0.0020 - acc: 0.9998 - val_loss: 0.7855 - val_acc: 0.8132 Epoch 00007: val_acc improved from 0.81310 to 0.81320, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 8/100 391/391 [==============================] - 556s 1s/step - loss: 0.0019 - acc: 0.9998 - val_loss: 0.7852 - val_acc: 0.8131 Epoch 00008: val_acc did not improve from 0.81320 Epoch 9/100 391/391 [==============================] - 557s 1s/step - loss: 0.0019 - acc: 0.9997 - val_loss: 0.7850 - val_acc: 0.8131 Epoch 00009: val_acc did not improve from 0.81320 Epoch 10/100 391/391 [==============================] - 556s 1s/step - loss: 0.0018 - acc: 0.9998 - val_loss: 0.7852 - val_acc: 0.8137 Epoch 00010: val_acc improved from 0.81320 to 0.81370, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 11/100 391/391 [==============================] - 554s 1s/step - loss: 0.0019 - acc: 0.9997 - val_loss: 0.7851 - val_acc: 0.8135 Epoch 00011: val_acc did not improve from 0.81370 Epoch 12/100 391/391 [==============================] - 559s 1s/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.7853 - val_acc: 0.8138 Epoch 00012: val_acc improved from 0.81370 to 0.81380, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 13/100 391/391 [==============================] - 560s 1s/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.7848 - val_acc: 0.8145 Epoch 00013: val_acc improved from 0.81380 to 0.81450, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 14/100 391/391 [==============================] - 559s 1s/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.7845 - val_acc: 0.8145 Epoch 00014: val_acc did not improve from 0.81450 Epoch 15/100 391/391 [==============================] - 562s 1s/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.7855 - val_acc: 0.8138 Epoch 00015: val_acc did not improve from 0.81450 Epoch 16/100 391/391 [==============================] - 564s 1s/step - loss: 0.0016 - acc: 0.9998 - val_loss: 0.7857 - val_acc: 0.8139 Epoch 00016: val_acc did not improve from 0.81450 Epoch 17/100 391/391 [==============================] - 563s 1s/step - loss: 0.0016 - acc: 0.9998 - val_loss: 0.7855 - val_acc: 0.8147 Epoch 00017: val_acc improved from 0.81450 to 0.81470, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 18/100 391/391 [==============================] - 570s 1s/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.7842 - val_acc: 0.8146 Epoch 00018: val_acc did not improve from 0.81470 Epoch 19/100 391/391 [==============================] - 569s 1s/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.7848 - val_acc: 0.8147 Epoch 00019: val_acc did not improve from 0.81470 Epoch 20/100 391/391 [==============================] - 565s 1s/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.7859 - val_acc: 0.8149 Epoch 00020: val_acc improved from 0.81470 to 0.81490, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 21/100 391/391 [==============================] - 567s 1s/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.7853 - val_acc: 0.8148 Epoch 00021: val_acc did not improve from 0.81490 Epoch 22/100 391/391 [==============================] - 568s 1s/step - loss: 0.0014 - acc: 0.9999 - val_loss: 0.7847 - val_acc: 0.8144 Epoch 00022: val_acc did not improve from 0.81490 Epoch 23/100 391/391 [==============================] - 561s 1s/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.7857 - val_acc: 0.8144 Epoch 00023: val_acc did not improve from 0.81490 Epoch 24/100 391/391 [==============================] - 561s 1s/step - loss: 0.0015 - acc: 0.9997 - val_loss: 0.7852 - val_acc: 0.8145 Epoch 00024: val_acc did not improve from 0.81490 Epoch 25/100 391/391 [==============================] - 567s 1s/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.7851 - val_acc: 0.8148 Epoch 00025: val_acc did not improve from 0.81490 Epoch 26/100 391/391 [==============================] - 575s 1s/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.7857 - val_acc: 0.8144 Epoch 00026: val_acc did not improve from 0.81490 Epoch 27/100 391/391 [==============================] - 565s 1s/step - loss: 0.0015 - acc: 0.9997 - val_loss: 0.7848 - val_acc: 0.8152 Epoch 00027: val_acc improved from 0.81490 to 0.81520, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5 Epoch 28/100 371/391 [===========================&gt;..] - ETA: 28s - loss: 0.0015 - acc: 0.9997 . Runtime disconnected after 27 epochs . Val accuracy has reached 81.52 . We could load the model again and train further to see how much farther we could go. But we will stop here for the purposes of this assignment. . Load the model saved best model , evaluate and print val loss and val accuracy . model= keras.models.load_model(&#39;/gdrive/My Drive/EVA/session20/best_model2.h5&#39;) . WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead. WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead. . score=model.evaluate_generator(validation_generator) . print(&#39;validation loss =&#39;,score[0] , &#39;, Validation accuracy =&#39;,score[1]) . validation loss = 0.7847665718078614 , Validation accuracy = 0.8152 . We used a pre-trained a ResNet34 model to classify images in the CIFAR100 dataset. We aded our own prediction layer on top of the base model and trained it to achieve 81.52 max validation accuracy .",
            "url": "https://ravindrabharathi.github.io/blog/resnet/cifar-100/image%20classification/cnn/transfer%20learning/2020/03/01/cifar-100.html",
            "relUrl": "/resnet/cifar-100/image%20classification/cnn/transfer%20learning/2020/03/01/cifar-100.html",
            "date": " • Mar 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ravindrabharathi.github.io/blog/jupyter/2020/02/22/test2.html",
            "relUrl": "/jupyter/2020/02/22/test2.html",
            "date": " • Feb 22, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "LR Finder",
            "content": "We will use CIFAR-10 dataset for this exercise and build the model using Keras .. . import necessary modules . from keras import backend as K import time import matplotlib.pyplot as plt import numpy as np % matplotlib inline np.random.seed(2017) from keras import regularizers from keras.models import Sequential from keras.layers.convolutional import Convolution2D, MaxPooling2D,AveragePooling2D from keras.layers import Activation, Flatten, Dense, Dropout from keras.layers.normalization import BatchNormalization from keras.utils import np_utils from keras.preprocessing.image import ImageDataGenerator . Using TensorFlow backend. . get CIFAR10 dataset and set the train and test data . from keras.datasets import cifar10 (train_features, train_labels), (test_features, test_labels) = cifar10.load_data() num_train, img_rows, img_cols,img_channels = train_features.shape num_test, _, _, _ = test_features.shape num_classes = len(np.unique(train_labels)) . Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz 170500096/170498071 [==============================] - 9s 0us/step . print (num_classes) print (num_train) print (train_features.shape) . 10 50000 (50000, 32, 32, 3) . inspect some of the images from the dataset by printing . class_names = [&#39;airplane&#39;,&#39;automobile&#39;,&#39;bird&#39;,&#39;cat&#39;,&#39;deer&#39;, &#39;dog&#39;,&#39;frog&#39;,&#39;horse&#39;,&#39;ship&#39;,&#39;truck&#39;] fig = plt.figure(figsize=(8,3)) for i in range(num_classes): ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[]) idx = np.where(train_labels[:]==i)[0] features_idx = train_features[idx,::] img_num = np.random.randint(features_idx.shape[0]) im = features_idx[img_num] ax.set_title(class_names[i]) plt.imshow(im) plt.show() . function for plotting accuracy vs number of epochs . def plot_model_history(model_history): fig, axs = plt.subplots(1,2,figsize=(15,5)) # summarize history for accuracy axs[0].plot(range(1,len(model_history.history[&#39;acc&#39;])+1),model_history.history[&#39;acc&#39;]) axs[0].plot(range(1,len(model_history.history[&#39;val_acc&#39;])+1),model_history.history[&#39;val_acc&#39;]) axs[0].set_title(&#39;Model Accuracy&#39;) axs[0].set_ylabel(&#39;Accuracy&#39;) axs[0].set_xlabel(&#39;Epoch&#39;) axs[0].set_xticks(np.arange(1,len(model_history.history[&#39;acc&#39;])+1),len(model_history.history[&#39;acc&#39;])/10) axs[0].legend([&#39;train&#39;, &#39;val&#39;], loc=&#39;best&#39;) # summarize history for loss axs[1].plot(range(1,len(model_history.history[&#39;loss&#39;])+1),model_history.history[&#39;loss&#39;]) axs[1].plot(range(1,len(model_history.history[&#39;val_loss&#39;])+1),model_history.history[&#39;val_loss&#39;]) axs[1].set_title(&#39;Model Loss&#39;) axs[1].set_ylabel(&#39;Loss&#39;) axs[1].set_xlabel(&#39;Epoch&#39;) axs[1].set_xticks(np.arange(1,len(model_history.history[&#39;loss&#39;])+1),len(model_history.history[&#39;loss&#39;])/10) axs[1].legend([&#39;train&#39;, &#39;val&#39;], loc=&#39;best&#39;) plt.show() . function to calculate accuracy on test data . def accuracy(test_x, test_y, model): result = model.predict(test_x) predicted_class = np.argmax(result, axis=1) true_class = np.argmax(test_y, axis=1) num_correct = np.sum(predicted_class == true_class) accuracy = float(num_correct)/result.shape[0] return (accuracy * 100) . function to get max training accuracy from model history . def get_max_train_accuracy(model_info): train_acc=model_info.history[&#39;acc&#39;] max_train_acc=max(train_acc) return (max_train_acc * 100) . function to get max validation accuracy from model history . def get_max_val_accuracy(model_info): val_acc=model_info.history[&#39;val_acc&#39;] max_val_acc=max(val_acc) return (max_val_acc * 100) . standardize pixel values of train and test images and convert train and test labels to categorical one hot encoded vectors . train_features = train_features.astype(&#39;float32&#39;)/255 test_features = test_features.astype(&#39;float32&#39;)/255 # convert class labels to binary class labels train_labels = np_utils.to_categorical(train_labels, num_classes) test_labels = np_utils.to_categorical(test_labels, num_classes) . train_labels . array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 1.], [0., 0., 0., ..., 0., 0., 1.], ..., [0., 0., 0., ..., 0., 0., 1.], [0., 1., 0., ..., 0., 0., 0.], [0., 1., 0., ..., 0., 0., 0.]], dtype=float32) . train dataset stats :mean , standard deviation for whole dataset , for a batch of 128 images , etc . (trainX, trainy), (testX, testy) = cifar10.load_data() print(&#39;Statistics train=%.3f (%.3f), test=%.3f (%.3f)&#39; % (trainX.mean(), trainX.std(), testX.mean(), testX.std())) # create generator that centers pixel values datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True) # calculate the mean on the training dataset datagen.fit(trainX) #print(&#39;Data Generator mean=%.3f, std=%.3f&#39; % (datagen.mean, datagen.std)) # demonstrate effect on a single batch of samples iterator = datagen.flow(trainX, trainy, batch_size=128) # get a batch batchX, batchy = iterator.next() # pixel stats in the batch print(batchX.shape, batchX.mean(), batchX.std()) # demonstrate effect on entire training dataset iterator = datagen.flow(trainX, trainy, batch_size=len(trainX), shuffle=False) # get a batch batchX, batchy = iterator.next() # pixel stats in the batch print(batchX.shape, batchX.mean(), batchX.std()) . Statistics train=120.708 (64.150), test=121.529 (64.061) (128, 32, 32, 3) 0.01989002 1.0052702 (50000, 32, 32, 3) -1.6605131e-06 1.0000001 . iterator1 = datagen.flow(testX, testy, batch_size=len(testX), shuffle=False) batch_testX, batch_testy = iterator1.next() X_train = batchX X_test = batch_testX y_train=batchy y_test=batch_testy . Y_train = np_utils.to_categorical(y_train, 10) Y_test = np_utils.to_categorical(y_test, 10) . Use the following standardization/regularization techniques for the model . Using Image Normalization | Making use of Batch Normalization | Making use of L2 Regularizer | Properly using Dropout | model1 = Sequential() model1.add(Convolution2D(32, 3, 3, border_mode=&#39;same&#39;,kernel_regularizer=regularizers.l2(0.0001), input_shape=(32, 32, 3))) model1.add(Activation(&#39;relu&#39;)) model1.add(BatchNormalization()) model1.add(Convolution2D(64, 3, 3,kernel_regularizer=regularizers.l2(0.0001),border_mode=&#39;same&#39;)) model1.add(Activation(&#39;relu&#39;)) model1.add(BatchNormalization()) model1.add(MaxPooling2D(pool_size=(2, 2))) model1.add(Dropout(0.2)) model1.add(Convolution2D(32, 1, 1)) model1.add(Convolution2D(64, 3, 3,kernel_regularizer=regularizers.l2(0.0001),border_mode=&#39;same&#39;)) model1.add(Activation(&#39;relu&#39;)) model1.add(BatchNormalization()) model1.add(Convolution2D(128, 3, 3,kernel_regularizer=regularizers.l2(0.0001),border_mode=&#39;same&#39;)) model1.add(Activation(&#39;relu&#39;)) model1.add(BatchNormalization()) model1.add(MaxPooling2D(pool_size=(2, 2))) model1.add(Dropout(0.3)) model1.add(Convolution2D(32, 1, 1)) model1.add(Convolution2D(128, 3, 3,kernel_regularizer=regularizers.l2(0.0001), border_mode=&#39;same&#39;)) model1.add(Activation(&#39;relu&#39;)) model1.add(BatchNormalization()) model1.add(Convolution2D(256, 3, 3,kernel_regularizer=regularizers.l2(0.0001), border_mode=&#39;same&#39;, name=&#39;LC1&#39;)) model1.add(Activation(&#39;relu&#39;,name=&#39;R1&#39;)) model1.add(BatchNormalization(name=&#39;BN1&#39;)) model1.add(MaxPooling2D(pool_size=(2, 2))) model1.add(Dropout(0.5)) model1.add(Convolution2D(10, 1, 1, name=&quot;red1&quot;)) model1.add(AveragePooling2D(pool_size = (4,4))) model1.add(Flatten()) model1.add(Activation(&#39;softmax&#39;)) . WARNING: Logging before flag parsing goes to stderr. W0720 16:04:14.356874 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead. /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), kernel_regularizer=&lt;keras.reg..., input_shape=(32, 32, 3..., padding=&#34;same&#34;)` W0720 16:04:14.388915 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead. W0720 16:04:14.394831 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead. W0720 16:04:14.438685 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead. W0720 16:04:14.439589 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead. W0720 16:04:16.704932 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead. /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), kernel_regularizer=&lt;keras.reg..., padding=&#34;same&#34;)` W0720 16:04:16.960443 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead. W0720 16:04:16.970278 140478930995072 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating: Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1))` del sys.path[0] /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), kernel_regularizer=&lt;keras.reg..., padding=&#34;same&#34;)` app.launch_new_instance() /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), kernel_regularizer=&lt;keras.reg..., padding=&#34;same&#34;)` /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1))` /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), kernel_regularizer=&lt;keras.reg..., padding=&#34;same&#34;)` /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), kernel_regularizer=&lt;keras.reg..., name=&#34;LC1&#34;, padding=&#34;same&#34;)` /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), name=&#34;red1&#34;)` W0720 16:04:17.392197 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead. . print model summary . model1.summary() . _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 32, 32, 32) 896 _________________________________________________________________ activation_1 (Activation) (None, 32, 32, 32) 0 _________________________________________________________________ batch_normalization_1 (Batch (None, 32, 32, 32) 128 _________________________________________________________________ conv2d_2 (Conv2D) (None, 32, 32, 64) 18496 _________________________________________________________________ activation_2 (Activation) (None, 32, 32, 64) 0 _________________________________________________________________ batch_normalization_2 (Batch (None, 32, 32, 64) 256 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 16, 16, 64) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 16, 16, 32) 2080 _________________________________________________________________ conv2d_4 (Conv2D) (None, 16, 16, 64) 18496 _________________________________________________________________ activation_3 (Activation) (None, 16, 16, 64) 0 _________________________________________________________________ batch_normalization_3 (Batch (None, 16, 16, 64) 256 _________________________________________________________________ conv2d_5 (Conv2D) (None, 16, 16, 128) 73856 _________________________________________________________________ activation_4 (Activation) (None, 16, 16, 128) 0 _________________________________________________________________ batch_normalization_4 (Batch (None, 16, 16, 128) 512 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 8, 8, 128) 0 _________________________________________________________________ dropout_2 (Dropout) (None, 8, 8, 128) 0 _________________________________________________________________ conv2d_6 (Conv2D) (None, 8, 8, 32) 4128 _________________________________________________________________ conv2d_7 (Conv2D) (None, 8, 8, 128) 36992 _________________________________________________________________ activation_5 (Activation) (None, 8, 8, 128) 0 _________________________________________________________________ batch_normalization_5 (Batch (None, 8, 8, 128) 512 _________________________________________________________________ LC1 (Conv2D) (None, 8, 8, 256) 295168 _________________________________________________________________ R1 (Activation) (None, 8, 8, 256) 0 _________________________________________________________________ BN1 (BatchNormalization) (None, 8, 8, 256) 1024 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 4, 4, 256) 0 _________________________________________________________________ dropout_3 (Dropout) (None, 4, 4, 256) 0 _________________________________________________________________ red1 (Conv2D) (None, 4, 4, 10) 2570 _________________________________________________________________ average_pooling2d_1 (Average (None, 1, 1, 10) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 10) 0 _________________________________________________________________ activation_6 (Activation) (None, 10) 0 ================================================================= Total params: 455,370 Trainable params: 454,026 Non-trainable params: 1,344 _________________________________________________________________ . Total params: 455,370 . LR Finder . Leslie N. Smith&#39;s 2015 paper “Cyclical Learning Rates for Training Neural Networks&quot; describes an effective technique of finding a range of learning rates for neural network where loss descends steeply and where the training diverges due to training rate being too large. . This page http://puzzlemusa.com/2018/05/14/learning-rate-finder-using-keras/ describes how to use this technique of LR Finder . We will use LR Finder to fix our initial learning rate to train the model . In order to use the model with LR Finder compile the model with SGD optimizer . model1.compile(optimizer=&#39;sgd&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . W0720 16:04:17.436660 140478930995072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. . Get LR finder callback from this reference http://puzzlemusa.com/2018/05/14/learning-rate-finder-using-keras/ . Add functions to print LR at min loss and min smoothed loss . from keras.callbacks import Callback class LR_Finder(Callback): def __init__(self, start_lr=1e-5, end_lr=10, step_size=None, beta=.98): super().__init__() self.start_lr = start_lr self.end_lr = end_lr self.step_size = step_size self.beta = beta self.lr_mult = (end_lr / start_lr) ** (1 / step_size) #print(&quot;lr mult : &quot;+str(self.lr_mult)) def on_train_begin(self, logs=None): self.best_loss = 1e9 self.avg_loss = 0 self.losses, self.smoothed_losses, self.lrs, self.iterations = [], [], [], [] self.iteration = 0 logs = logs or {} K.set_value(self.model.optimizer.lr, self.start_lr) def on_batch_end(self, epoch, logs=None): logs = logs or {} loss = logs.get(&#39;loss&#39;) self.iteration += 1 self.avg_loss = self.beta * self.avg_loss + (1 - self.beta) * loss smoothed_loss = self.avg_loss / (1 - self.beta ** self.iteration) # Check if the loss is not exploding if self.iteration &gt; 1 and smoothed_loss &gt; self.best_loss * 4: self.model.stop_training = True return if smoothed_loss &lt; self.best_loss or self.iteration == 1: self.best_loss = smoothed_loss lr = self.start_lr * (self.lr_mult ** self.iteration) #print(&quot;lr = &quot;+str(lr)) self.losses.append(loss) self.smoothed_losses.append(smoothed_loss) self.lrs.append(lr) self.iterations.append(self.iteration) K.set_value(self.model.optimizer.lr, lr) def plot_lr(self): plt.figure(figsize=(18,12)) plt.xlabel(&#39;Iterations&#39;) plt.ylabel(&#39;Learning rate&#39;) plt.plot(self.iterations, self.lrs) def plot(self, n_skip=10): plt.figure(figsize=(18,12)) plt.ylabel(&#39;Loss&#39;) plt.xlabel(&#39;Learning rate (log scale)&#39;) plt.plot(self.lrs[n_skip:-5], self.losses[n_skip:-5]) plt.xscale(&#39;log&#39;) def plot_smoothed_loss(self, n_skip=10): plt.figure(figsize=(18,12)) plt.ylabel(&#39;Smoothed Losses&#39;) plt.xlabel(&#39;Learning rate (log scale)&#39;) plt.plot(self.lrs[n_skip:-5], self.smoothed_losses[n_skip:-5]) plt.xscale(&#39;log&#39;) def plot_loss(self): plt.figure(figsize=(18,12)) plt.ylabel(&#39;Losses&#39;) plt.xlabel(&#39;Iterations&#39;) plt.plot(self.iterations[10:], self.losses[10:]) def get_best_loss(self): return self.best_loss def find_lr_at_best_loss(self): print(&quot;====================================================================&quot;) print(&quot;LR at min loss &quot;) print(&quot;LR at min loss : &quot;+str(self.lrs[np.argmin(self.losses)])) print(&quot;LR at min smoothed loss : &quot;+str(self.lrs[np.argmin(self.smoothed_losses)])) print(&quot;====================================================================&quot;) . Run LR_finder for 1 epoch with start_lr=1e-5 , end_lr=10 . batch_size=128 lr_finder = LR_Finder(start_lr=1e-5, end_lr=10, step_size=np.ceil(X_train.shape[0]/batch_size)) model1.fit(X_train, Y_train, epochs=1, batch_size=batch_size,callbacks=[lr_finder] ) . W0720 16:04:17.724291 140478930995072 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where . Epoch 1/1 50000/50000 [==============================] - 14s 283us/step - loss: 3.2580 - acc: 0.1908 . &lt;keras.callbacks.History at 0x7fc3a9a1d588&gt; . plot the following . 1. LR vs iterations . 2. Loss vs LR(log scale) . 3. Smoothed Loss vs LR(log scale) . From the smoothed loss vs LR plot we can see that the max descent for loss is between lr of 0.01 and 0.1 . lr_finder.plot_lr() . lr_finder.plot() . lr_finder.plot_smoothed_loss() . lr_finder.find_lr_at_best_loss() . ==================================================================== LR at min loss LR at min loss : 0.04489251258218551 LR at min smoothed loss : 0.04489251258218551 ==================================================================== . K.eval(lr_finder.model.optimizer.lr) . 10.0 . From the loss vs lr plots ,The max rate of descent seems to be between 0.01 and 0.1 and it looks like min loss is between lr values of 0.1 and 0.01 . printing the lr corresponding to the min loss and min smoothed loss confirms this observation . Let us pick an initial learning rate of 0.045 since that is where the smoothened loss curve starts going up . we will use SGD optimizer with lr=0.045 and momentum =0.9 to compile the model . from keras import optimizers opt= optimizers.SGD(lr=0.05,momentum=0.9) model1.compile(optimizer=opt, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . Cutout Augmentation . Cutout was first presented as an effective augmentation technique in these two papers : Improved Regularization of Convolutional Neural Networks with Cutout and Random Erasing Data Augmentation The idea is to randomly cut away patches of information from images that a model is training on to force it to learn from more parts of the image. This would help the model learn more features about a class instead of depending on some simple assumptions using smaller areas within the image . This helps the model generalize better and make better predictions . We will use python code for random erasing found at https://github.com/yu4u/cutout-random-erasing . !wget https://raw.githubusercontent.com/yu4u/cutout-random-erasing/master/random_eraser.py . --2019-07-20 16:05:13-- https://raw.githubusercontent.com/yu4u/cutout-random-erasing/master/random_eraser.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 888 [text/plain] Saving to: ‘random_eraser.py’ random_eraser.py 100%[===================&gt;] 888 --.-KB/s in 0s 2019-07-20 16:05:13 (158 MB/s) - ‘random_eraser.py’ saved [888/888] . train the model for 100 epochs . Use image augmentation of random cutout , horizontal flip . plot accuracy vs epochs , print accuracy and max val accuracy . from keras.preprocessing.image import ImageDataGenerator from random_eraser import get_random_eraser batch_size=128 train_datagen=ImageDataGenerator( featurewise_center=True, # set input mean to 0 over the dataset #samplewise_center=False, # set each sample mean to 0 featurewise_std_normalization=True, # divide inputs by std of the dataset #samplewise_std_normalization=False, # divide each input by its std preprocessing_function=get_random_eraser(v_l=0, v_h=1), horizontal_flip=True ) val_datagen= ImageDataGenerator( featurewise_center=True, # set input mean to 0 over the dataset featurewise_std_normalization=True, # divide inputs by std of the dataset ) train_datagen.fit(X_train) val_datagen.fit(X_test) training_generator=train_datagen.flow(X_train, Y_train, batch_size=batch_size,shuffle=True,seed=42) validation_generator=val_datagen.flow(X_test, Y_test, batch_size=batch_size,shuffle=True,seed=42) # train the model start = time.time() # Train the model model_info = model1.fit_generator(training_generator, epochs=100, steps_per_epoch=np.ceil(X_train.shape[0]/batch_size), validation_steps=np.ceil(X_test.shape[0]/batch_size), validation_data=validation_generator, shuffle=True, verbose=0) end = time.time() print (&quot;Model took %0.2f seconds to train&quot;%(end - start)) # plot model history plot_model_history(model_info) # compute test accuracy print (&quot;Accuracy on test data is: %0.2f&quot;%accuracy(X_test, Y_test, model1)) print (&quot;Max training accuracy is: %0.2f&quot;%get_max_train_accuracy(model_info)) print (&quot;Max validation accuracy is: %0.2f&quot;%get_max_val_accuracy(model_info)) . Model took 1151.59 seconds to train . Accuracy on test data is: 86.24 Max training accuracy is: 87.53 Max validation accuracy is: 87.81 . the model was trained for 100 epochs and reached a max val accuracy of 87.81 .We also notice that it almost the same as the max training accuracy . Training more epochs would yield better accuracy values . We will stop at 100 epochs and prepare to print Grad-CAM visualization on some misclassified images from this model&#39;s prediction on the test data . Grad-CAM . Now let us define the function for Grad-CAM visualization . This function named gradcam takes as input the model , the set of images , the labels for each image and the layer to be used for calculating gradients . It returns a list of dictionaries containing original image , the heatmap, the titles to display during visualization . import cv2 from mpl_toolkits.axes_grid1 import ImageGrid from google.colab.patches import cv2_imshow from IPython.core.display import display, HTML #select test images and corresponding labels to print heatmap #x=np.array([test_features[41],test_features[410],test_features[222],test_features[950]]) #y=[test_labels[41],test_labels[410],test_labels[222],test_labels[950]] def gradcam(model1,x,y,which_layer): # results=[] #make prediction for these 4 images preds = model1.predict(x) for j in range(x.shape[0]): #get class id from the prediction values class_idx = np.argmax(preds[j]) class_output = model1.output[:, class_idx] ## choose the layer nearest to prediction that has a size of about 7x7 or 8x8 #in this case it is the layer being sent to the gradcam function last_conv_layer = model1.get_layer(which_layer) # compute gradients and from heatmap grads = K.gradients(class_output, last_conv_layer.output)[0] pooled_grads = K.mean(grads, axis=(0, 1, 2)) iterate = K.function([model1.input], [pooled_grads, last_conv_layer.output[0]]) pooled_grads_value, conv_layer_output_value = iterate([x]) #apply the pooled grad value to the conv layer channels for i in range(256): conv_layer_output_value[:, :, i] *= pooled_grads_value[i] #get the mean of the weighted values and assign to heatmap heatmap = np.mean(conv_layer_output_value, axis=-1) #retain only positive values (or 0) in heatmap heatmap = np.maximum(heatmap, 0) #convert values between 0 and 1 using divide by max value heatmap /= np.max(heatmap) #we now have a heatmap with size equal to the output size of the layer we chose #img is the image we are running gradcam on img = x[j] #resize heatmap 8x8 to image size of 32x32 heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0])) #convert pixel values to be between 0 and 255 heatmap = np.uint8(255 * heatmap) #apply suitable cv2 colormap . In this case colormap_JET heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET) # convert from BGR to RGB if we want to display using matplotlib heatmap1 = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB) # create superimposed image if we want to print using cv2 (cv2_imshow supported in colab) superimposed_img = cv2.addWeighted(img, 0.5, heatmap1, 0.5, 0,dtype=5) #create a dictionary object with details of image, heatmap, its title title1=str(j+1)+&quot;: &quot;+ class_names[np.argmax(y[j])]+&quot; predicted as &quot;+str(class_names[class_idx]) title2=&#39;superimposed heatmap&#39; image1=img image2=heatmap1 image3=superimposed_img imageObj={&#39;image1&#39;:image1,&#39;image2&#39;:image2,&#39;image3&#39;:image3,&#39;title1&#39;:title1,&#39;title2&#39;:title2} #append the image dict object to results list results.append(imageObj) #print(j) #return grad-cam results as a list of dictionary objects , each containing an image and its heatmap return results . Define the function to display the Grad-CAM visualizations . This function displays a set of two images with heatmap visuals per row . def displayRow(images): # we will plot 2 images in a row # cv.imshow does not work in jupyter notebooks and colab # cv2_imshow patch works on colab but matplotlib gives us a little more flexibility in formatting the display # we will use matplotlib to print the image and its heatmap fig = plt.figure(1, (13,13)) grid = ImageGrid(fig, 111, nrows_ncols=(1,5), axes_pad=1,label_mode=&quot;1&quot; ) #horizontal spacer #grid[0].imshow(np.ones((32, 10)),alpha=0) #grid[0].axis(&#39;off&#39;) #first image #print(&quot; original class is :&quot;+class_names[np.argmax(y[j])]+&quot; and predicted class is :&quot;+str(class_names[class_idx])) grid[0].imshow(images[0][&#39;image1&#39;]) grid[0].set_title(images[0][&#39;title1&#39;]) grid[0].axis(&#39;off&#39;) #print the original image and on top of it place the heat map at 60% transparency grid[1].imshow(images[0][&#39;image1&#39;],alpha=0.9) grid[1].imshow(images[0][&#39;image2&#39;],alpha=0.6) grid[1].set_title(images[0][&#39;title2&#39;]) grid[1].axis(&#39;off&#39;) #vertical separator grid[2].imshow(np.ones((32, 1))) grid[2].axis(&#39;off&#39;) #second image #print(&quot; original class is :&quot;+class_names[np.argmax(y[j])]+&quot; and predicted class is :&quot;+str(class_names[class_idx])) grid[3].imshow(images[1][&#39;image1&#39;]) grid[3].set_title(images[1][&#39;title1&#39;]) grid[3].axis(&#39;off&#39;) #print the original image and on top of it place the heat map at 60% transparency grid[4].imshow(images[1][&#39;image1&#39;],alpha=0.9) grid[4].imshow(images[1][&#39;image2&#39;],alpha=0.6) grid[4].set_title(images[1][&#39;title2&#39;]) grid[4].axis(&#39;off&#39;) plt.show() display(HTML(&quot;&lt;hr size=&#39;5&#39; color=&#39;black&#39; width=&#39;100%&#39; align=&#39;center&#39; /&gt;&quot;)) . Make predictions using the model and collect all the images that were classified wrongly . pred=model1.predict(X_test) pred2=np.argmax(pred,axis=1) wrong_set=[] correct_set=[] wrong_labels=[] true_labels=[] wrong_indices=[] for i in range(X_test.shape[0]): if (pred2[i]==np.argmax(test_labels[i])): correct_set.append(X_test[i]) else: wrong_indices.append(i) wrong_labels.append(class_names[pred2[i]]) true_labels.append(class_names[np.argmax(test_labels[i])]) wrong_set.append(X_test[i]) . Now take the first 26 images and the corresponding labels to create the data for Grad-CAM visualization . w_list=wrong_indices[:26] x=[] y=[] for i in range(len(w_list)): x.append(test_features[w_list[i]]) y.append(test_labels[w_list[i]]) #convert the image list to numpy array x=np.array(x) . Obtain results from the gradcam function . results=gradcam(model1,x,y,&#39;R1&#39;) # we choose this layer as the layer nearest to prediction having a size of 8x8 . display the results from gradcam function with images and corresponding heatmap visuals . display(HTML(&quot;&lt;h2 align=&#39;center&#39;&gt;First 26 misclassified images with Grad-CAM heatmap &lt;/h2&gt;&lt;hr size=&#39;5&#39; color=&#39;black&#39; width=&#39;100%&#39; align=&#39;center&#39; /&gt;&quot;)) for i in range(0,len(results),2): images=[] images.append(results[i]) images.append(results[i+1]) displayRow(images) . First 26 misclassified images with Grad-CAM heatmap . . . . . . . . . . . . . . We used LR finder to fix an optimum learning rate of 0.045 for training the model on cifar 10 dataset . The model reached a max val accuracy of 87.81 and was almost the same as the max training accuracy indicating that this model would reach even higher accuracies with more epochs . We then used Grad-CAM to visualize the heatmaps for a set of 26 images that this model misclassifed. .",
            "url": "https://ravindrabharathi.github.io/blog/lr%20finder/grad-cam/heatmap%20visualization/optimimum%20lr/2020/02/10/LR-Finder.html",
            "relUrl": "/lr%20finder/grad-cam/heatmap%20visualization/optimimum%20lr/2020/02/10/LR-Finder.html",
            "date": " • Feb 10, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ravindrabharathi.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ravindrabharathi.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ravindrabharathi.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}