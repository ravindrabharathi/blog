{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR-100-ResNet34.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKmpsK1nO73y"
      },
      "source": [
        "# Use Transfer Learning to Classify images in CIFAR-100 Dataset\n",
        "> Transfer Learning - Use a pretrained Resnet model trained on ImageNet to classify CIFAR-100 images \n",
        "\n",
        "- toc: false \n",
        "- badges: false\n",
        "- comments: false\n",
        "- categories: [ResNET, CIFAR-100, Image classification, CNN, Transfer Learning]\n",
        "- image: images/transfer-learning-small.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBXbawN8gw8h"
      },
      "source": [
        "In this post we will see how to use transfer learning (where we use the information/patterns that a model has learned in one task to solve another similar task) . \n",
        "![Transfer Learning](https://raw.githubusercontent.com/ravindrabharathi/blog/master/images/Transfer-learning-big.png)\n",
        "\n",
        "We will use a pretrained ResNet model trained on ImageNet dataset to learn and classify images in the CIFAR-100 dataset. \n",
        "\n",
        "We will use a ResNet34 pretrained model from [https://github.com/qubvel/classification_models](https://github.com/qubvel/classification_models)\n",
        "\n",
        "\n",
        "We will use Resnet34 model to try and achieve 80% validation accuracy . Since pretrained weights are only available for imagenet and models expect a 224x224 image size , we will resize the cifar100 images to 224x224 while training .\n",
        "\n",
        "In the pretrained model we will remove the top prediction layers and freeze the last 11 layers . We will add a GlobalAveragepooling2D layer , a dense layer and a softmax activation to form our prediction layer for cifar100. \n",
        "The first part will be to train with the frozen layers in base model . After training for about 30 epochs , we will unfreeze the layers and train further ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10OT_tMpl0wq"
      },
      "source": [
        "### Install the required files from qubvel keras applications project in order to get the pretrained ResNet model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdO_-6W0aiiD",
        "outputId": "6d0324a1-2e92-424b-b882-a073320d040e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "#hide_output\n",
        "!pip install git+https://github.com/qubvel/classification_models.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/qubvel/classification_models.git\n",
            "  Cloning https://github.com/qubvel/classification_models.git to /tmp/pip-req-build-5x6yx4oj\n",
            "  Running command git clone -q https://github.com/qubvel/classification_models.git /tmp/pip-req-build-5x6yx4oj\n",
            "  Running command git submodule update --init --recursive -q\n",
            "Requirement already satisfied: keras_applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.6/dist-packages (from image-classifiers==1.0.0) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras_applications<=1.0.8,>=1.0.7->image-classifiers==1.0.0) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras_applications<=1.0.8,>=1.0.7->image-classifiers==1.0.0) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras_applications<=1.0.8,>=1.0.7->image-classifiers==1.0.0) (1.12.0)\n",
            "Building wheels for collected packages: image-classifiers\n",
            "  Building wheel for image-classifiers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for image-classifiers: filename=image_classifiers-1.0.0-cp36-none-any.whl size=19950 sha256=be0aa5db89758bc9c55b9e6009bebb1222b086b087e45a6c54c4fb8e56b48877\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cw05kyqp/wheels/de/2b/fd/29a6d33edb8c28bc7d94e95ea1d39c9a218ac500a3cfb1b197\n",
            "Successfully built image-classifiers\n",
            "Installing collected packages: image-classifiers\n",
            "Successfully installed image-classifiers-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRgvbPkimOEw"
      },
      "source": [
        "### Import necessary keras modules , numpy and matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pOT3ZW_b6aR",
        "outputId": "d4093633-083b-4569-ceab-f31ae5504034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "#hide_output\n",
        "from keras import backend as K\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "% matplotlib inline\n",
        "np.random.seed(2017) \n",
        "#from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.layers import Activation, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGn-_2Z6mYDN"
      },
      "source": [
        "### import ResNet34 and image preprocessing from the project we installed earlier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSRkgTHcaUNo"
      },
      "source": [
        "import keras\n",
        "import cv2\n",
        "#from classification_models.resnet import ResNet34, preprocess_input\n",
        "from classification_models.keras import Classifiers\n",
        "ResNet34, preprocess_input = Classifiers.get('resnet34')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLnmMg5tmsmR"
      },
      "source": [
        "### get cifar100 dataset from keras datasets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5u03OosdQFB",
        "outputId": "69ea6220-f437-4e31-d2f3-38256e5d6a50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.datasets import cifar100\n",
        "(train_features, train_labels), (test_features, test_labels) = cifar100.load_data()\n",
        "num_train, img_channels, img_rows, img_cols =  train_features.shape\n",
        "num_test, _, _, _ =  test_features.shape\n",
        "num_classes = len(np.unique(train_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTKEOt03nLqg"
      },
      "source": [
        "### preprocess the images to make sure that they are in the format required by the pretrained model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaOr_FcrHuzl"
      },
      "source": [
        "train_features = preprocess_input(train_features)\n",
        "\n",
        "test_features = preprocess_input(test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvtJ_SmH6aAg"
      },
      "source": [
        "print max and min pixel values in the images which we can use in the ramdom-erase/cutout augmentation later "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxpYeFRBsTf9",
        "outputId": "349156f6-dd04-46eb-9a3a-858c9bef8370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(np.max(train_features),np.min(train_features))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "255 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsqHkD_-r3Na"
      },
      "source": [
        "Store cifar100 train and test images in a local data folder. We will load these images using an imagedatagenerator and resize to 224x224 which is default size for Resnet-imagenet models  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv6SlC8B_hyB",
        "outputId": "673638d1-e7ca-40fa-d1bd-b46cbaec98c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#hide_output\n",
        "!rm -R ./data/  # remove old data direrctory to clean up "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove './data/': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpNG95OwOzLM"
      },
      "source": [
        "sub_dir='train'\n",
        "data_dir='./data'\n",
        "if not os.path.exists(data_dir):\n",
        "  os.mkdir(data_dir)\n",
        "image_dir='./data/'+sub_dir+'/'\n",
        "if not os.path.exists(image_dir):\n",
        "  os.mkdir(image_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjhF9yrBQXMT",
        "outputId": "1d0fc405-1245-401e-f490-aa6ae1cc1052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#hide\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw7dD5P6QPBk",
        "outputId": "5273a029-976b-49ec-9495-7270eea3126a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#hide\n",
        "os.path.exists('./data/train')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eXYnX0Byqqq"
      },
      "source": [
        "\n",
        "def save_img(images,sub_dir):\n",
        "  c=0\n",
        "  os.chdir('/content/')\n",
        "  curr_dir = os.getcwd()\n",
        "  image_dir='./data/'+sub_dir+'/'\n",
        "  if not os.path.exists(image_dir):\n",
        "\n",
        "    os.mkdir(image_dir)\n",
        "  os.chdir(image_dir)\n",
        "  print('current working directory is '+os.getcwd())\n",
        "  for img in images:\n",
        "    c +=1\n",
        "    filename=str(c)+'.jpg'\n",
        "    \n",
        "    \n",
        "    cv2.imwrite(filename,img)\n",
        "  print(\"files resized and saved to \"+image_dir)\n",
        "  os.chdir(curr_dir)\n",
        "  print('current working directory is '+os.getcwd())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ81DFAUNKp3",
        "outputId": "50049f0a-f69b-459b-d657-dcd0a8ae9055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "save_img(train_features,'train')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current working directory is /content/data/train\n",
            "files resized and saved to ./data/train/\n",
            "current working directory is /content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2G9ej_DPMS2",
        "outputId": "10174d58-b057-4883-f1b8-c92fb245140c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "save_img(test_features,'test')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current working directory is /content/data/test\n",
            "files resized and saved to ./data/test/\n",
            "current working directory is /content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4eK6e1Xy219",
        "outputId": "afcdd319-1f1c-4234-9dbc-2cec4e1dfa00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls ./data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test  train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC8CyKMZsYiH"
      },
      "source": [
        "### Mount google drive to save best model while training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgQSzBEsz_Wk",
        "outputId": "abd5f909-7969-4064-bc2a-dd7fe6b93de0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#hide_output\n",
        "from google.colab import drive \n",
        "drive.mount('/gdrive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yUjpM7Lsf3T"
      },
      "source": [
        "### Import pandas and create a dataframe with image files and labels information. We will use this dataframe with Keras imagedatagenerator to load images for training and testing and calculate loss using the corresponding label values "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmbWyCzcdN5O"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrmZZjP3dRq6"
      },
      "source": [
        "def form_df(label_type='train'):\n",
        "  if label_type=='train':\n",
        "    labels=train_labels\n",
        "  else:\n",
        "    labels=test_labels  \n",
        "\n",
        "  file_name=[]\n",
        "  class_label=[]\n",
        "  for i in range(len(labels)):\n",
        "    filename=str(i+1)+'.jpg'\n",
        "    file_name.append(filename)\n",
        "    class_label.append(str(labels[i][0]))\n",
        "\n",
        "  df=pd.DataFrame({'File':file_name,'Class':class_label})  \n",
        "  return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ7Jz-GSfi72",
        "outputId": "6cb9e4fd-dbd9-462d-e2d1-91e98b9bc8a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "train_df=form_df('train')\n",
        "print(train_df.head())\n",
        "train_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    File Class\n",
            "0  1.jpg    19\n",
            "1  2.jpg    29\n",
            "2  3.jpg     0\n",
            "3  4.jpg    11\n",
            "4  5.jpg     1\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 2 columns):\n",
            "File     50000 non-null object\n",
            "Class    50000 non-null object\n",
            "dtypes: object(2)\n",
            "memory usage: 781.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWCJ7C58f4JV",
        "outputId": "097ca609-4a73-4057-ffcc-1287fe0efe39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train_df.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>49996.jpg</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>49997.jpg</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>49998.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>49999.jpg</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>50000.jpg</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            File Class\n",
              "49995  49996.jpg    80\n",
              "49996  49997.jpg     7\n",
              "49997  49998.jpg     3\n",
              "49998  49999.jpg     7\n",
              "49999  50000.jpg    73"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_1vY2akgYv5",
        "outputId": "33d7aaa7-026c-4f81-c470-04ee5d20939e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "test_df=form_df('test')\n",
        "print(test_df.head())\n",
        "print(test_df.tail())\n",
        "print(test_df.info())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    File Class\n",
            "0  1.jpg    49\n",
            "1  2.jpg    33\n",
            "2  3.jpg    72\n",
            "3  4.jpg    51\n",
            "4  5.jpg    71\n",
            "           File Class\n",
            "9995   9996.jpg    83\n",
            "9996   9997.jpg    14\n",
            "9997   9998.jpg    51\n",
            "9998   9999.jpg    42\n",
            "9999  10000.jpg    70\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 2 columns):\n",
            "File     10000 non-null object\n",
            "Class    10000 non-null object\n",
            "dtypes: object(2)\n",
            "memory usage: 156.4+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VngJOhEis_Cz"
      },
      "source": [
        "### Custom function for random-pad-crop augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YTx0BWJeKxK"
      },
      "source": [
        "def pad4(img):\n",
        "  pad_size=img.shape[1]//8\n",
        "  img=np.pad(img, [ (pad_size, pad_size), (pad_size, pad_size), (0, 0)], mode='reflect')  \n",
        "  return img \n",
        "\n",
        "\n",
        "def random_pad_crop_img(img,crop_size=224):\n",
        "  crop_size=img.shape[1]\n",
        "  img=pad4(img)\n",
        "  pad=img.shape[1]-crop_size\n",
        "  x1=np.random.randint(pad)\n",
        "  x2=x1+crop_size\n",
        "  y1=np.random.randint(pad)\n",
        "  y2=y1+crop_size\n",
        "  img=img[x1:x2,y1:y2,:]\n",
        "  return img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDeN9Suw4xi"
      },
      "source": [
        "### We will now get the ResNet34 model weights for imagenet (Cifar is not available in this library). \n",
        "### input shape set to 224,224,3\n",
        "\n",
        "### Add GlobalAveragePooling to convert these to 1D inputs suitable for the softmax prediction layer \n",
        "### Add a Dense Layer instead of the one we removed from the pretrained model \n",
        "### Add softmax prediction \n",
        "\n",
        "### for the first train run we will freeze the all layers of the pretrained model except the last 11 layers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SusCi6AJcKYB",
        "outputId": "de8934cf-fbc3-41a0-9c92-513fd8c994cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#hide_output\n",
        "# build model\n",
        "from keras.layers import GlobalAveragePooling2D, Add, Lambda, Dense, GlobalMaxPooling2D\n",
        "\n",
        "#base modek from REsnet34 \n",
        "base_model = ResNet34(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
        "\n",
        "#Freeze all but last 11 layers \n",
        "for layer in base_model.layers[:-11]:\n",
        "  layer.trainable=False\n",
        "for layer in base_model.layers:\n",
        "    print(layer, layer.trainable) \n",
        "\n",
        "#Add our own Top/Prediction layers \n",
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "\n",
        "\n",
        "\n",
        "x= Dense(num_classes,use_bias=False)(x)\n",
        "\n",
        "output = keras.layers.Activation('softmax')(x)\n",
        "\n",
        "model = keras.models.Model(inputs=[base_model.input], outputs=[output]) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Downloading data from https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000_no_top.h5\n",
            "85524480/85521592 [==============================] - 3s 0us/step\n",
            "<keras.engine.input_layer.InputLayer object at 0x7fef643e3400> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fef64b22128> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7fef643e50b8> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fef5436d160> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fef543195f8> False\n",
            "<keras.layers.core.Activation object at 0x7fef54319d30> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7fef54319f60> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7fef54323400> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fef542df3c8> False\n",
            "<keras.layers.core.Activation object at 0x7fef542df898> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7fef54298278> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fef542dfc18> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fef542df6a0> False\n",
            "<keras.layers.core.Activation object at 0x7fef542a6a58> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7fef542a6e80> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fef542a6fd0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fef542df860> False\n",
            "<keras.layers.merge.Add object at 0x7fef54271160> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fef54271208> False\n",
            "<keras.layers.core.Activation object at 0x7fef542717f0> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7fef54271710> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fef54212518> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fef5422cfd0> False\n",
            "<keras.layers.core.Activation object at 0x7fef54236e48> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7fef54236dd8> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fef54242748> False\n",
            "<keras.layers.merge.Add object at 0x7fef541eacc0> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fef541ead68> False\n",
            "<keras.layers.core.Activation object at 0x7fef541eafd0> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7fef541f1390> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fef541fc630> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fef541b1c88> False\n",
            "<keras.layers.core.Activation object at 0x7fef541b1f98> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7fef541b8a90> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fef541c6d30> False\n",
            "<keras.layers.merge.Add object at 0x7fef5417ba58> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fef5417bb00> False\n",
            "<keras.layers.core.Activation object at 0x7fef5417bf98> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7fef5413a828> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fef5411bf28> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fef54147748> False\n",
            "<keras.layers.core.Activation object at 0x7fef54147d68> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7fef54147f28> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fef540d06d8> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fef5417f550> False\n",
            "<keras.layers.merge.Add object at 0x7fef54090748> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fef540907f0> False\n",
            "<keras.layers.core.Activation object at 0x7fef54090cf8> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7fef540975c0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fef540974a8> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fef54058748> False\n",
            "<keras.layers.core.Activation object at 0x7fef54058e10> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7fef54058e48> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fef5405d4a8> False\n",
            "<keras.layers.merge.Add object at 0x7fef5401f518> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fef5401f5c0> False\n",
            "<keras.layers.core.Activation object at 0x7fef5401f908> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7fef5401fa20> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fef54025278> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fef501364e0> False\n",
            "<keras.layers.core.Activation object at 0x7fef50136e10> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7fef50136f98> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fef5013c240> False\n",
            "<keras.layers.merge.Add object at 0x7fef500fe2b0> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fef500fe358> False\n",
            "<keras.layers.core.Activation object at 0x7fef500fe860> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7fef500fe6a0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fef500fefd0> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fef500c5278> False\n",
            "<keras.layers.core.Activation object at 0x7fef500c5fd0> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7fef500c5e80> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fef50067550> False\n",
            "<keras.layers.merge.Add object at 0x7fef50089048> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fef500890f0> False\n",
            "<keras.layers.core.Activation object at 0x7fef500896d8> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7fef50045d30> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fef50089d68> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fef50055cf8> False\n",
            "<keras.layers.core.Activation object at 0x7fef5004ae10> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7fef5005ad30> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7feef01b66a0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fef500895f8> False\n",
            "<keras.layers.merge.Add object at 0x7feef016bcf8> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7feef016bf28> False\n",
            "<keras.layers.core.Activation object at 0x7feef01c7eb8> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7feef01733c8> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7feef01957b8> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7feef0133cf8> False\n",
            "<keras.layers.core.Activation object at 0x7feef01a6eb8> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7feef013bb00> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7feef0142860> False\n",
            "<keras.layers.merge.Add object at 0x7feef00faac8> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7feef00fab70> False\n",
            "<keras.layers.core.Activation object at 0x7feef00faf98> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7feef0101208> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7feef0110d68> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7feef00c1a90> False\n",
            "<keras.layers.core.Activation object at 0x7feef00c1f98> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7feef00c1fd0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7feef00c97f0> False\n",
            "<keras.layers.merge.Add object at 0x7feef0088860> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7feef0088908> False\n",
            "<keras.layers.core.Activation object at 0x7feef0088e10> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7feef008f6d8> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7feef008f5c0> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7feef004f828> False\n",
            "<keras.layers.core.Activation object at 0x7feef004fef0> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7feef004ff28> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7feef0058588> False\n",
            "<keras.layers.merge.Add object at 0x7feef00185f8> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7feef00186a0> False\n",
            "<keras.layers.core.Activation object at 0x7feef0018b00> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7feef0020400> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7feef0020358> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7feeeffe15c0> False\n",
            "<keras.layers.core.Activation object at 0x7feeeffe1cc0> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7feeeffe1ef0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7feeeffe8320> False\n",
            "<keras.layers.merge.Add object at 0x7feeeffa7390> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7feeeffa7438> False\n",
            "<keras.layers.core.Activation object at 0x7feeeffa7780> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7feeeffa7ef0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7feeeff37940> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7feeefef2358> False\n",
            "<keras.layers.core.Activation object at 0x7feeefef2c88> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7feeefef2e10> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7feeefef2f60> False\n",
            "<keras.layers.merge.Add object at 0x7feeefeba128> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7feeefeba1d0> False\n",
            "<keras.layers.core.Activation object at 0x7feeefeba7b8> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7feeefe72e10> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7feeefe78320> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7feeefe82dd8> False\n",
            "<keras.layers.core.Activation object at 0x7feeefe8ae80> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7feeefe8ae10> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7feeefe2d860> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7feeefeba6d8> False\n",
            "<keras.layers.merge.Add object at 0x7feeefe47dd8> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7feeefe47ef0> False\n",
            "<keras.layers.core.Activation object at 0x7feeefe39f98> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7feeefe504a8> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7feeefe57d30> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7feeefe11dd8> False\n",
            "<keras.layers.core.Activation object at 0x7feeefdfff98> False\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7feeefe18be0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7feeefe24eb8> False\n",
            "<keras.layers.merge.Add object at 0x7feeefdd7ba8> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7feeefdd7c50> True\n",
            "<keras.layers.core.Activation object at 0x7feeefdd7f98> True\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7feeefddf278> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7feeefddf908> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7feeefd9eb70> True\n",
            "<keras.layers.core.Activation object at 0x7feeefd9ef60> True\n",
            "<keras.layers.convolutional.ZeroPadding2D object at 0x7feeefda6978> True\n",
            "<keras.layers.convolutional.Conv2D object at 0x7feeefd33fd0> True\n",
            "<keras.layers.merge.Add object at 0x7feeefd68940> True\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fef542df390> True\n",
            "<keras.layers.core.Activation object at 0x7feeefd68e10> True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUVB1aC7_Zp9"
      },
      "source": [
        "#hide\n",
        "### print model summary  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNPeR3japM16",
        "outputId": "f07a578b-47cf-4681-8ed0-fe7db2109d26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#hide\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "data (InputLayer)               (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bn_data (BatchNormalization)    (None, 224, 224, 3)  9           data[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 230, 230, 3)  0           bn_data[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv0 (Conv2D)                  (None, 112, 112, 64) 9408        zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn0 (BatchNormalization)        (None, 112, 112, 64) 256         conv0[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "relu0 (Activation)              (None, 112, 112, 64) 0           bn0[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_2 (ZeroPadding2D (None, 114, 114, 64) 0           relu0[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "pooling0 (MaxPooling2D)         (None, 56, 56, 64)   0           zero_padding2d_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit1_bn1 (BatchNormaliz (None, 56, 56, 64)   256         pooling0[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit1_relu1 (Activation) (None, 56, 56, 64)   0           stage1_unit1_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_3 (ZeroPadding2D (None, 58, 58, 64)   0           stage1_unit1_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit1_conv1 (Conv2D)     (None, 56, 56, 64)   36864       zero_padding2d_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit1_bn2 (BatchNormaliz (None, 56, 56, 64)   256         stage1_unit1_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit1_relu2 (Activation) (None, 56, 56, 64)   0           stage1_unit1_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_4 (ZeroPadding2D (None, 58, 58, 64)   0           stage1_unit1_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit1_conv2 (Conv2D)     (None, 56, 56, 64)   36864       zero_padding2d_4[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit1_sc (Conv2D)        (None, 56, 56, 64)   4096        stage1_unit1_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 56, 56, 64)   0           stage1_unit1_conv2[0][0]         \n",
            "                                                                 stage1_unit1_sc[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit2_bn1 (BatchNormaliz (None, 56, 56, 64)   256         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit2_relu1 (Activation) (None, 56, 56, 64)   0           stage1_unit2_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_5 (ZeroPadding2D (None, 58, 58, 64)   0           stage1_unit2_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit2_conv1 (Conv2D)     (None, 56, 56, 64)   36864       zero_padding2d_5[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit2_bn2 (BatchNormaliz (None, 56, 56, 64)   256         stage1_unit2_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit2_relu2 (Activation) (None, 56, 56, 64)   0           stage1_unit2_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_6 (ZeroPadding2D (None, 58, 58, 64)   0           stage1_unit2_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit2_conv2 (Conv2D)     (None, 56, 56, 64)   36864       zero_padding2d_6[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 56, 56, 64)   0           stage1_unit2_conv2[0][0]         \n",
            "                                                                 add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit3_bn1 (BatchNormaliz (None, 56, 56, 64)   256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit3_relu1 (Activation) (None, 56, 56, 64)   0           stage1_unit3_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_7 (ZeroPadding2D (None, 58, 58, 64)   0           stage1_unit3_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit3_conv1 (Conv2D)     (None, 56, 56, 64)   36864       zero_padding2d_7[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit3_bn2 (BatchNormaliz (None, 56, 56, 64)   256         stage1_unit3_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit3_relu2 (Activation) (None, 56, 56, 64)   0           stage1_unit3_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_8 (ZeroPadding2D (None, 58, 58, 64)   0           stage1_unit3_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit3_conv2 (Conv2D)     (None, 56, 56, 64)   36864       zero_padding2d_8[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 56, 56, 64)   0           stage1_unit3_conv2[0][0]         \n",
            "                                                                 add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit1_bn1 (BatchNormaliz (None, 56, 56, 64)   256         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit1_relu1 (Activation) (None, 56, 56, 64)   0           stage2_unit1_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_9 (ZeroPadding2D (None, 58, 58, 64)   0           stage2_unit1_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit1_conv1 (Conv2D)     (None, 28, 28, 128)  73728       zero_padding2d_9[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit1_bn2 (BatchNormaliz (None, 28, 28, 128)  512         stage2_unit1_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit1_relu2 (Activation) (None, 28, 28, 128)  0           stage2_unit1_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_10 (ZeroPadding2 (None, 30, 30, 128)  0           stage2_unit1_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit1_conv2 (Conv2D)     (None, 28, 28, 128)  147456      zero_padding2d_10[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit1_sc (Conv2D)        (None, 28, 28, 128)  8192        stage2_unit1_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 28, 28, 128)  0           stage2_unit1_conv2[0][0]         \n",
            "                                                                 stage2_unit1_sc[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit2_bn1 (BatchNormaliz (None, 28, 28, 128)  512         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit2_relu1 (Activation) (None, 28, 28, 128)  0           stage2_unit2_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_11 (ZeroPadding2 (None, 30, 30, 128)  0           stage2_unit2_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit2_conv1 (Conv2D)     (None, 28, 28, 128)  147456      zero_padding2d_11[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit2_bn2 (BatchNormaliz (None, 28, 28, 128)  512         stage2_unit2_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit2_relu2 (Activation) (None, 28, 28, 128)  0           stage2_unit2_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_12 (ZeroPadding2 (None, 30, 30, 128)  0           stage2_unit2_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit2_conv2 (Conv2D)     (None, 28, 28, 128)  147456      zero_padding2d_12[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 28, 28, 128)  0           stage2_unit2_conv2[0][0]         \n",
            "                                                                 add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit3_bn1 (BatchNormaliz (None, 28, 28, 128)  512         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit3_relu1 (Activation) (None, 28, 28, 128)  0           stage2_unit3_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_13 (ZeroPadding2 (None, 30, 30, 128)  0           stage2_unit3_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit3_conv1 (Conv2D)     (None, 28, 28, 128)  147456      zero_padding2d_13[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit3_bn2 (BatchNormaliz (None, 28, 28, 128)  512         stage2_unit3_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit3_relu2 (Activation) (None, 28, 28, 128)  0           stage2_unit3_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_14 (ZeroPadding2 (None, 30, 30, 128)  0           stage2_unit3_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit3_conv2 (Conv2D)     (None, 28, 28, 128)  147456      zero_padding2d_14[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 28, 28, 128)  0           stage2_unit3_conv2[0][0]         \n",
            "                                                                 add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit4_bn1 (BatchNormaliz (None, 28, 28, 128)  512         add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit4_relu1 (Activation) (None, 28, 28, 128)  0           stage2_unit4_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_15 (ZeroPadding2 (None, 30, 30, 128)  0           stage2_unit4_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit4_conv1 (Conv2D)     (None, 28, 28, 128)  147456      zero_padding2d_15[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit4_bn2 (BatchNormaliz (None, 28, 28, 128)  512         stage2_unit4_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit4_relu2 (Activation) (None, 28, 28, 128)  0           stage2_unit4_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_16 (ZeroPadding2 (None, 30, 30, 128)  0           stage2_unit4_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit4_conv2 (Conv2D)     (None, 28, 28, 128)  147456      zero_padding2d_16[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 28, 28, 128)  0           stage2_unit4_conv2[0][0]         \n",
            "                                                                 add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit1_bn1 (BatchNormaliz (None, 28, 28, 128)  512         add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit1_relu1 (Activation) (None, 28, 28, 128)  0           stage3_unit1_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_17 (ZeroPadding2 (None, 30, 30, 128)  0           stage3_unit1_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit1_conv1 (Conv2D)     (None, 14, 14, 256)  294912      zero_padding2d_17[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit1_bn2 (BatchNormaliz (None, 14, 14, 256)  1024        stage3_unit1_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit1_relu2 (Activation) (None, 14, 14, 256)  0           stage3_unit1_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_18 (ZeroPadding2 (None, 16, 16, 256)  0           stage3_unit1_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit1_conv2 (Conv2D)     (None, 14, 14, 256)  589824      zero_padding2d_18[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit1_sc (Conv2D)        (None, 14, 14, 256)  32768       stage3_unit1_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 14, 14, 256)  0           stage3_unit1_conv2[0][0]         \n",
            "                                                                 stage3_unit1_sc[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit2_bn1 (BatchNormaliz (None, 14, 14, 256)  1024        add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit2_relu1 (Activation) (None, 14, 14, 256)  0           stage3_unit2_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_19 (ZeroPadding2 (None, 16, 16, 256)  0           stage3_unit2_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit2_conv1 (Conv2D)     (None, 14, 14, 256)  589824      zero_padding2d_19[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit2_bn2 (BatchNormaliz (None, 14, 14, 256)  1024        stage3_unit2_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit2_relu2 (Activation) (None, 14, 14, 256)  0           stage3_unit2_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_20 (ZeroPadding2 (None, 16, 16, 256)  0           stage3_unit2_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit2_conv2 (Conv2D)     (None, 14, 14, 256)  589824      zero_padding2d_20[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 14, 14, 256)  0           stage3_unit2_conv2[0][0]         \n",
            "                                                                 add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit3_bn1 (BatchNormaliz (None, 14, 14, 256)  1024        add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit3_relu1 (Activation) (None, 14, 14, 256)  0           stage3_unit3_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_21 (ZeroPadding2 (None, 16, 16, 256)  0           stage3_unit3_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit3_conv1 (Conv2D)     (None, 14, 14, 256)  589824      zero_padding2d_21[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit3_bn2 (BatchNormaliz (None, 14, 14, 256)  1024        stage3_unit3_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit3_relu2 (Activation) (None, 14, 14, 256)  0           stage3_unit3_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_22 (ZeroPadding2 (None, 16, 16, 256)  0           stage3_unit3_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit3_conv2 (Conv2D)     (None, 14, 14, 256)  589824      zero_padding2d_22[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 14, 14, 256)  0           stage3_unit3_conv2[0][0]         \n",
            "                                                                 add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit4_bn1 (BatchNormaliz (None, 14, 14, 256)  1024        add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit4_relu1 (Activation) (None, 14, 14, 256)  0           stage3_unit4_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_23 (ZeroPadding2 (None, 16, 16, 256)  0           stage3_unit4_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit4_conv1 (Conv2D)     (None, 14, 14, 256)  589824      zero_padding2d_23[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit4_bn2 (BatchNormaliz (None, 14, 14, 256)  1024        stage3_unit4_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit4_relu2 (Activation) (None, 14, 14, 256)  0           stage3_unit4_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_24 (ZeroPadding2 (None, 16, 16, 256)  0           stage3_unit4_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit4_conv2 (Conv2D)     (None, 14, 14, 256)  589824      zero_padding2d_24[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 14, 14, 256)  0           stage3_unit4_conv2[0][0]         \n",
            "                                                                 add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit5_bn1 (BatchNormaliz (None, 14, 14, 256)  1024        add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit5_relu1 (Activation) (None, 14, 14, 256)  0           stage3_unit5_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_25 (ZeroPadding2 (None, 16, 16, 256)  0           stage3_unit5_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit5_conv1 (Conv2D)     (None, 14, 14, 256)  589824      zero_padding2d_25[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit5_bn2 (BatchNormaliz (None, 14, 14, 256)  1024        stage3_unit5_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit5_relu2 (Activation) (None, 14, 14, 256)  0           stage3_unit5_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_26 (ZeroPadding2 (None, 16, 16, 256)  0           stage3_unit5_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit5_conv2 (Conv2D)     (None, 14, 14, 256)  589824      zero_padding2d_26[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 14, 14, 256)  0           stage3_unit5_conv2[0][0]         \n",
            "                                                                 add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit6_bn1 (BatchNormaliz (None, 14, 14, 256)  1024        add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit6_relu1 (Activation) (None, 14, 14, 256)  0           stage3_unit6_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_27 (ZeroPadding2 (None, 16, 16, 256)  0           stage3_unit6_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit6_conv1 (Conv2D)     (None, 14, 14, 256)  589824      zero_padding2d_27[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit6_bn2 (BatchNormaliz (None, 14, 14, 256)  1024        stage3_unit6_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit6_relu2 (Activation) (None, 14, 14, 256)  0           stage3_unit6_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_28 (ZeroPadding2 (None, 16, 16, 256)  0           stage3_unit6_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit6_conv2 (Conv2D)     (None, 14, 14, 256)  589824      zero_padding2d_28[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 14, 14, 256)  0           stage3_unit6_conv2[0][0]         \n",
            "                                                                 add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage4_unit1_bn1 (BatchNormaliz (None, 14, 14, 256)  1024        add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage4_unit1_relu1 (Activation) (None, 14, 14, 256)  0           stage4_unit1_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_29 (ZeroPadding2 (None, 16, 16, 256)  0           stage4_unit1_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage4_unit1_conv1 (Conv2D)     (None, 7, 7, 512)    1179648     zero_padding2d_29[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage4_unit1_bn2 (BatchNormaliz (None, 7, 7, 512)    2048        stage4_unit1_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage4_unit1_relu2 (Activation) (None, 7, 7, 512)    0           stage4_unit1_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_30 (ZeroPadding2 (None, 9, 9, 512)    0           stage4_unit1_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage4_unit1_conv2 (Conv2D)     (None, 7, 7, 512)    2359296     zero_padding2d_30[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage4_unit1_sc (Conv2D)        (None, 7, 7, 512)    131072      stage4_unit1_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 7, 7, 512)    0           stage4_unit1_conv2[0][0]         \n",
            "                                                                 stage4_unit1_sc[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "stage4_unit2_bn1 (BatchNormaliz (None, 7, 7, 512)    2048        add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage4_unit2_relu1 (Activation) (None, 7, 7, 512)    0           stage4_unit2_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_31 (ZeroPadding2 (None, 9, 9, 512)    0           stage4_unit2_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage4_unit2_conv1 (Conv2D)     (None, 7, 7, 512)    2359296     zero_padding2d_31[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage4_unit2_bn2 (BatchNormaliz (None, 7, 7, 512)    2048        stage4_unit2_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage4_unit2_relu2 (Activation) (None, 7, 7, 512)    0           stage4_unit2_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_32 (ZeroPadding2 (None, 9, 9, 512)    0           stage4_unit2_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage4_unit2_conv2 (Conv2D)     (None, 7, 7, 512)    2359296     zero_padding2d_32[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 7, 7, 512)    0           stage4_unit2_conv2[0][0]         \n",
            "                                                                 add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage4_unit3_bn1 (BatchNormaliz (None, 7, 7, 512)    2048        add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage4_unit3_relu1 (Activation) (None, 7, 7, 512)    0           stage4_unit3_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_33 (ZeroPadding2 (None, 9, 9, 512)    0           stage4_unit3_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage4_unit3_conv1 (Conv2D)     (None, 7, 7, 512)    2359296     zero_padding2d_33[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage4_unit3_bn2 (BatchNormaliz (None, 7, 7, 512)    2048        stage4_unit3_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage4_unit3_relu2 (Activation) (None, 7, 7, 512)    0           stage4_unit3_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_34 (ZeroPadding2 (None, 9, 9, 512)    0           stage4_unit3_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage4_unit3_conv2 (Conv2D)     (None, 7, 7, 512)    2359296     zero_padding2d_34[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 7, 7, 512)    0           stage4_unit3_conv2[0][0]         \n",
            "                                                                 add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "bn1 (BatchNormalization)        (None, 7, 7, 512)    2048        add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "relu1 (Activation)              (None, 7, 7, 512)    0           bn1[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 512)          0           relu1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 100)          51200       global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 100)          0           dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 21,353,673\n",
            "Trainable params: 4,772,864\n",
            "Non-trainable params: 16,580,809\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SienK_5yAPrO"
      },
      "source": [
        "\n",
        "\n",
        "### Compile the model using Stochastic Gradient descent optimizer with momentum of 0.9 and lr of 0.015"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryhxOK4Lctku",
        "outputId": "f7fabd5c-9f2e-4939-80c8-7d22bccfb1a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# train\n",
        "from keras.optimizers import SGD\n",
        "opt=SGD(lr=0.015,  momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5E19gnfAz9k"
      },
      "source": [
        "### we want to get the model with best validation accuracy for the prediction task and so we will save the best model from the various epochs in Google Drive using ModelCheckpoint callback available in Keras \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcDjaHdoBae_"
      },
      "source": [
        "### define a Modelcheckpoint to save the best Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h-6Bv_PV-Lg"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "model_save_path='/gdrive/My Drive/EVA/session20/best_model2.h5'\n",
        "\n",
        "chkpoint_model=ModelCheckpoint(model_save_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='max')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mzn1wpS4JoU"
      },
      "source": [
        "##Cutout Augmentation\n",
        "Cutout was first presented as an effective augmentation technique in these two papers :\n",
        "\n",
        "[Improved Regularization of Convolutional Neural Networks with Cutout](https://arxiv.org/abs/1708.04552) and [Random Erasing Data Augmentation](https://arxiv.org/abs/1708.04896)\n",
        "\n",
        "The idea is to randomly cut away patches of information from images that a model is training on to force it to learn from more parts of the image. This would help the model learn more features about a class instead of depending on some simple assumptions using smaller areas within the image . This helps the model generalize better and make better predictions .\n",
        "\n",
        "We will use python code for random erasing found at [https://github.com/yu4u/cutout-random-erasing](https://github.com/yu4u/cutout-random-erasing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcjCx4Xx3jsK",
        "outputId": "fb86a474-3dfe-4d19-bfb3-b00677a69fe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#hide_output\n",
        "#get code for random erasing from https://github.com/yu4u/cutout-random-erasing\n",
        "!wget https://raw.githubusercontent.com/yu4u/cutout-random-erasing/master/random_eraser.py\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-04 02:57:37--  https://raw.githubusercontent.com/yu4u/cutout-random-erasing/master/random_eraser.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 888 [text/plain]\n",
            "Saving to: ‘random_eraser.py’\n",
            "\n",
            "\rrandom_eraser.py      0%[                    ]       0  --.-KB/s               \rrandom_eraser.py    100%[===================>]     888  --.-KB/s    in 0s      \n",
            "\n",
            "2019-12-04 02:57:37 (191 MB/s) - ‘random_eraser.py’ saved [888/888]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIAhZlQWBhCF"
      },
      "source": [
        "### Train the model for 100 epochs using a batch size of 128  . We will use a ImageDataGenerator to apply image augmentation of random-pad-crop, horizontal Flip and CutOut augmentation for the training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wU6acAz-6UDv"
      },
      "source": [
        "from random_eraser import get_random_eraser\n",
        "eraser = get_random_eraser(p=0.8, s_l=0.15, s_h=0.25,r_1=0.5, r_2=1/0.5,v_l=0,v_h=255,pixel_level=False)\n",
        "def img_aug1(img):\n",
        "  \n",
        "  \n",
        "  \n",
        "  img=random_pad_crop_img(img)\n",
        "  img=eraser(img)\n",
        "  return img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1l33sDoj_ML"
      },
      "source": [
        "def scheduler(epoch):\n",
        "  if epoch < 30:\n",
        "    return 0.01\n",
        "  elif 30 < epoch < 50: \n",
        "    return 0.008 \n",
        "  else:\n",
        "    return 0.008 * tensorflow.math.exp(0.1 * (50 - epoch))\n",
        "\n",
        "lr_callback = keras.callbacks.LearningRateScheduler(scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIovD9qGWSmx",
        "outputId": "df7ce741-e73a-4bfb-bfc4-737cba400bae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "EPOCHS=100\n",
        "batch_size=128\n",
        "\n",
        "train_datagen=ImageDataGenerator(\n",
        "    \n",
        "        \n",
        "        \n",
        "        preprocessing_function=img_aug1,\n",
        "        horizontal_flip=True\n",
        "    \n",
        ")\n",
        "\n",
        "val_datagen= ImageDataGenerator(\n",
        "    \n",
        "        \n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "training_generator = train_datagen.flow_from_dataframe(train_df, directory='./data/train/', \n",
        "                                                         x_col='File', y_col='Class', target_size=(224, 224),\n",
        "                                                    color_mode='rgb', interpolation='bicubic',\n",
        "                                                    class_mode='categorical', \n",
        "                                                    batch_size=batch_size, shuffle=True, seed=42)\n",
        "validation_generator = val_datagen.flow_from_dataframe(test_df, directory='./data/test/',\n",
        "                                                         x_col='File', y_col='Class', \n",
        "                                                         target_size=(224, 224),interpolation='bicubic',\n",
        "                                                    color_mode='rgb', class_mode='categorical', \n",
        "                                                    batch_size=batch_size, shuffle=True, seed=42)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 50000 validated image filenames belonging to 100 classes.\n",
            "Found 10000 validated image filenames belonging to 100 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tugm_GOvlBGZ"
      },
      "source": [
        "def scheduler(epoch):\n",
        "  if epoch < 5:\n",
        "    return 0.02\n",
        "  elif 5 < epoch < 12: \n",
        "    return 0.015 \n",
        "  elif 12 < epoch < 20: \n",
        "    return 0.010\n",
        "  elif 20 < epoch < 25: \n",
        "    return 0.007      \n",
        "  else:\n",
        "    return 0.003\n",
        "\n",
        "lr_callback = keras.callbacks.LearningRateScheduler(scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzofTD20sFch",
        "outputId": "50dd96c6-e6d9-4769-e6ef-35869efc7246",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#hide_output\n",
        "model.fit_generator(training_generator, epochs=30, \n",
        "                        steps_per_epoch=np.ceil(train_features.shape[0]/batch_size), \n",
        "                    validation_steps=np.ceil(test_features.shape[0]/batch_size), \n",
        "                    validation_data=validation_generator,\n",
        "                                 shuffle=True,\n",
        "                                callbacks=[chkpoint_model,lr_callback],\n",
        "                                 verbose=1)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/30\n",
            "391/391 [==============================] - 143s 367ms/step - loss: 3.0429 - acc: 0.2592 - val_loss: 3.3339 - val_acc: 0.2888\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.28880, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 2/30\n",
            "391/391 [==============================] - 131s 334ms/step - loss: 2.4142 - acc: 0.3753 - val_loss: 2.7019 - val_acc: 0.3728\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.28880 to 0.37280, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 3/30\n",
            "391/391 [==============================] - 130s 332ms/step - loss: 2.2485 - acc: 0.4119 - val_loss: 2.9838 - val_acc: 0.3559\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.37280\n",
            "Epoch 4/30\n",
            "391/391 [==============================] - 130s 332ms/step - loss: 2.1262 - acc: 0.4380 - val_loss: 2.9980 - val_acc: 0.3657\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.37280\n",
            "Epoch 5/30\n",
            "391/391 [==============================] - 130s 333ms/step - loss: 2.0500 - acc: 0.4526 - val_loss: 3.5103 - val_acc: 0.3361\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.37280\n",
            "Epoch 6/30\n",
            "391/391 [==============================] - 129s 331ms/step - loss: 1.8938 - acc: 0.4893 - val_loss: 3.0115 - val_acc: 0.3799\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.37280 to 0.37990, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 7/30\n",
            "391/391 [==============================] - 129s 331ms/step - loss: 1.9202 - acc: 0.4823 - val_loss: 3.1336 - val_acc: 0.3733\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.37990\n",
            "Epoch 8/30\n",
            "391/391 [==============================] - 129s 330ms/step - loss: 1.8907 - acc: 0.4902 - val_loss: 3.0686 - val_acc: 0.3787\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.37990\n",
            "Epoch 9/30\n",
            "391/391 [==============================] - 130s 332ms/step - loss: 1.8420 - acc: 0.5018 - val_loss: 3.2207 - val_acc: 0.3743\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.37990\n",
            "Epoch 10/30\n",
            "391/391 [==============================] - 129s 331ms/step - loss: 1.8068 - acc: 0.5090 - val_loss: 3.5011 - val_acc: 0.3613\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.37990\n",
            "Epoch 11/30\n",
            "391/391 [==============================] - 129s 330ms/step - loss: 1.7793 - acc: 0.5141 - val_loss: 3.0006 - val_acc: 0.3945\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.37990 to 0.39450, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 12/30\n",
            "391/391 [==============================] - 129s 331ms/step - loss: 1.7491 - acc: 0.5239 - val_loss: 3.0046 - val_acc: 0.4053\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.39450 to 0.40530, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 13/30\n",
            "391/391 [==============================] - 130s 332ms/step - loss: 1.6473 - acc: 0.5471 - val_loss: 3.0496 - val_acc: 0.4013\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.40530\n",
            "Epoch 14/30\n",
            "391/391 [==============================] - 129s 331ms/step - loss: 1.6570 - acc: 0.5460 - val_loss: 3.2120 - val_acc: 0.3886\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.40530\n",
            "Epoch 15/30\n",
            "391/391 [==============================] - 129s 330ms/step - loss: 1.6426 - acc: 0.5465 - val_loss: 2.8877 - val_acc: 0.4152\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.40530 to 0.41520, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 16/30\n",
            "391/391 [==============================] - 128s 328ms/step - loss: 1.6199 - acc: 0.5533 - val_loss: 3.2430 - val_acc: 0.3968\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.41520\n",
            "Epoch 17/30\n",
            "391/391 [==============================] - 122s 311ms/step - loss: 1.6093 - acc: 0.5553 - val_loss: 3.3633 - val_acc: 0.3853\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.41520\n",
            "Epoch 18/30\n",
            "391/391 [==============================] - 122s 312ms/step - loss: 1.5891 - acc: 0.5611 - val_loss: 3.5916 - val_acc: 0.3703\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.41520\n",
            "Epoch 19/30\n",
            "391/391 [==============================] - 121s 310ms/step - loss: 1.5707 - acc: 0.5665 - val_loss: 2.8044 - val_acc: 0.4265\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.41520 to 0.42650, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 20/30\n",
            "391/391 [==============================] - 122s 311ms/step - loss: 1.5550 - acc: 0.5687 - val_loss: 2.9007 - val_acc: 0.4178\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.42650\n",
            "Epoch 21/30\n",
            "391/391 [==============================] - 122s 312ms/step - loss: 1.4868 - acc: 0.5854 - val_loss: 3.1454 - val_acc: 0.4037\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.42650\n",
            "Epoch 22/30\n",
            "391/391 [==============================] - 121s 310ms/step - loss: 1.5035 - acc: 0.5839 - val_loss: 3.3994 - val_acc: 0.3845\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.42650\n",
            "Epoch 23/30\n",
            "391/391 [==============================] - 121s 310ms/step - loss: 1.4860 - acc: 0.5860 - val_loss: 3.1657 - val_acc: 0.4043\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.42650\n",
            "Epoch 24/30\n",
            "391/391 [==============================] - 120s 308ms/step - loss: 1.4809 - acc: 0.5875 - val_loss: 3.0020 - val_acc: 0.4094\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.42650\n",
            "Epoch 25/30\n",
            "391/391 [==============================] - 121s 309ms/step - loss: 1.4730 - acc: 0.5906 - val_loss: 2.8147 - val_acc: 0.4342\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.42650 to 0.43420, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 26/30\n",
            "391/391 [==============================] - 122s 312ms/step - loss: 1.4163 - acc: 0.6054 - val_loss: 2.9704 - val_acc: 0.4182\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.43420\n",
            "Epoch 27/30\n",
            "391/391 [==============================] - 121s 309ms/step - loss: 1.4087 - acc: 0.6058 - val_loss: 2.8172 - val_acc: 0.4336\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.43420\n",
            "Epoch 28/30\n",
            "391/391 [==============================] - 121s 309ms/step - loss: 1.3998 - acc: 0.6061 - val_loss: 2.9284 - val_acc: 0.4253\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.43420\n",
            "Epoch 29/30\n",
            "391/391 [==============================] - 120s 308ms/step - loss: 1.3893 - acc: 0.6134 - val_loss: 2.9667 - val_acc: 0.4245\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.43420\n",
            "Epoch 30/30\n",
            "391/391 [==============================] - 121s 309ms/step - loss: 1.3943 - acc: 0.6102 - val_loss: 2.9288 - val_acc: 0.4246\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.43420\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7feeefb15898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJsRwIo_uHsu"
      },
      "source": [
        "### After the initial 30 epochs of training the last few layers , now unfreeze all the layers and train again for 100 epochs \n",
        "\n",
        "### Unfreeze all layers in base model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbSyWgjcN-ns"
      },
      "source": [
        "for layer in model.layers:\n",
        "  layer.trainable=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6kXYlwZOHOX"
      },
      "source": [
        "opt=SGD(lr=0.01,  momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he5BwWD9IlCn"
      },
      "source": [
        "import math\n",
        "\n",
        "def scheduler1(epoch):\n",
        "  if epoch < 15:\n",
        "    return 0.01\n",
        "  elif 15 < epoch < 30: \n",
        "    return 0.008 \n",
        "  else:\n",
        "    return 0.008 * math.exp(0.1 * (30 - epoch))\n",
        "\n",
        "lr_callback = keras.callbacks.LearningRateScheduler(scheduler1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i_6qAl6OU-K",
        "outputId": "b7db7cc9-1b33-479e-dd48-488b80b7c5ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#hide_output\n",
        "model.fit_generator(training_generator, epochs=EPOCHS, \n",
        "                        steps_per_epoch=np.ceil(train_features.shape[0]/batch_size), \n",
        "                    validation_steps=np.ceil(test_features.shape[0]/batch_size), \n",
        "                    validation_data=validation_generator,\n",
        "                                 shuffle=True,\n",
        "                                callbacks=[chkpoint_model,lr_callback],\n",
        "                                 verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "391/391 [==============================] - 186s 476ms/step - loss: 1.2726 - acc: 0.6361 - val_loss: 1.2386 - val_acc: 0.6603\n",
            "\n",
            "Epoch 00001: val_acc improved from 0.43420 to 0.66030, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 2/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.9419 - acc: 0.7234 - val_loss: 1.0315 - val_acc: 0.7018\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.66030 to 0.70180, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 3/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.8065 - acc: 0.7602 - val_loss: 0.9088 - val_acc: 0.7393\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.70180 to 0.73930, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 4/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.7124 - acc: 0.7846 - val_loss: 0.9617 - val_acc: 0.7312\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.73930\n",
            "Epoch 5/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.6331 - acc: 0.8057 - val_loss: 0.9588 - val_acc: 0.7385\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.73930\n",
            "Epoch 6/100\n",
            "391/391 [==============================] - 179s 457ms/step - loss: 0.5726 - acc: 0.8238 - val_loss: 1.0137 - val_acc: 0.7265\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.73930\n",
            "Epoch 7/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.5171 - acc: 0.8410 - val_loss: 0.9424 - val_acc: 0.7432\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.73930 to 0.74320, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 8/100\n",
            "391/391 [==============================] - 179s 457ms/step - loss: 0.4767 - acc: 0.8544 - val_loss: 0.8891 - val_acc: 0.7581\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.74320 to 0.75810, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 9/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.4326 - acc: 0.8660 - val_loss: 0.9155 - val_acc: 0.7550\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.75810\n",
            "Epoch 10/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.4012 - acc: 0.8755 - val_loss: 0.9171 - val_acc: 0.7544\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.75810\n",
            "Epoch 11/100\n",
            "391/391 [==============================] - 179s 457ms/step - loss: 0.3686 - acc: 0.8856 - val_loss: 0.9026 - val_acc: 0.7599\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.75810 to 0.75990, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 12/100\n",
            "391/391 [==============================] - 179s 458ms/step - loss: 0.3393 - acc: 0.8954 - val_loss: 1.0303 - val_acc: 0.7399\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.75990\n",
            "Epoch 13/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.3221 - acc: 0.9011 - val_loss: 0.9879 - val_acc: 0.7550\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.75990\n",
            "Epoch 14/100\n",
            "391/391 [==============================] - 179s 457ms/step - loss: 0.2955 - acc: 0.9081 - val_loss: 0.9063 - val_acc: 0.7609\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.75990 to 0.76090, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 15/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.2780 - acc: 0.9140 - val_loss: 0.9361 - val_acc: 0.7626\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.76090 to 0.76260, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 16/100\n",
            "391/391 [==============================] - 179s 458ms/step - loss: 1.0478 - acc: 0.6974 - val_loss: 2.0535 - val_acc: 0.5705\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.76260\n",
            "Epoch 17/100\n",
            "391/391 [==============================] - 179s 457ms/step - loss: 0.5366 - acc: 0.8374 - val_loss: 0.8038 - val_acc: 0.7734\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.76260 to 0.77340, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 18/100\n",
            "391/391 [==============================] - 179s 457ms/step - loss: 0.3469 - acc: 0.8952 - val_loss: 0.7776 - val_acc: 0.7876\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.77340 to 0.78760, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 19/100\n",
            "391/391 [==============================] - 179s 457ms/step - loss: 0.2823 - acc: 0.9137 - val_loss: 0.7753 - val_acc: 0.7940\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.78760 to 0.79400, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 20/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.2419 - acc: 0.9262 - val_loss: 0.7797 - val_acc: 0.7936\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.79400\n",
            "Epoch 21/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.2175 - acc: 0.9347 - val_loss: 0.8229 - val_acc: 0.7880\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.79400\n",
            "Epoch 22/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.1976 - acc: 0.9403 - val_loss: 0.8246 - val_acc: 0.7911\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.79400\n",
            "Epoch 23/100\n",
            "391/391 [==============================] - 179s 457ms/step - loss: 0.1786 - acc: 0.9477 - val_loss: 0.8362 - val_acc: 0.7902\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.79400\n",
            "Epoch 24/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.1649 - acc: 0.9520 - val_loss: 0.8262 - val_acc: 0.7909\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.79400\n",
            "Epoch 25/100\n",
            "391/391 [==============================] - 179s 457ms/step - loss: 0.1562 - acc: 0.9534 - val_loss: 0.8414 - val_acc: 0.7893\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.79400\n",
            "Epoch 26/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.1447 - acc: 0.9576 - val_loss: 0.8416 - val_acc: 0.7896\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.79400\n",
            "Epoch 27/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.1422 - acc: 0.9574 - val_loss: 0.8558 - val_acc: 0.7918\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.79400\n",
            "Epoch 28/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.1323 - acc: 0.9607 - val_loss: 0.8532 - val_acc: 0.7924\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.79400\n",
            "Epoch 29/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.1261 - acc: 0.9628 - val_loss: 0.8479 - val_acc: 0.7946\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.79400 to 0.79460, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 30/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.1132 - acc: 0.9682 - val_loss: 0.8551 - val_acc: 0.7941\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.79460\n",
            "Epoch 31/100\n",
            "391/391 [==============================] - 179s 458ms/step - loss: 0.1123 - acc: 0.9670 - val_loss: 0.8399 - val_acc: 0.7972\n",
            "\n",
            "Epoch 00031: val_acc improved from 0.79460 to 0.79720, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 32/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.1035 - acc: 0.9703 - val_loss: 0.8720 - val_acc: 0.7908\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.79720\n",
            "Epoch 33/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0928 - acc: 0.9738 - val_loss: 0.8403 - val_acc: 0.7952\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.79720\n",
            "Epoch 34/100\n",
            "391/391 [==============================] - 177s 454ms/step - loss: 0.0854 - acc: 0.9758 - val_loss: 0.8532 - val_acc: 0.7980\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.79720 to 0.79800, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 35/100\n",
            "391/391 [==============================] - 177s 454ms/step - loss: 0.0800 - acc: 0.9777 - val_loss: 0.8207 - val_acc: 0.8023\n",
            "\n",
            "Epoch 00035: val_acc improved from 0.79800 to 0.80230, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 36/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.0731 - acc: 0.9793 - val_loss: 0.8210 - val_acc: 0.8059\n",
            "\n",
            "Epoch 00036: val_acc improved from 0.80230 to 0.80590, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 37/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0691 - acc: 0.9804 - val_loss: 0.8296 - val_acc: 0.8046\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.80590\n",
            "Epoch 38/100\n",
            "391/391 [==============================] - 179s 457ms/step - loss: 0.0637 - acc: 0.9824 - val_loss: 0.8162 - val_acc: 0.8077\n",
            "\n",
            "Epoch 00038: val_acc improved from 0.80590 to 0.80770, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 39/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0601 - acc: 0.9834 - val_loss: 0.8142 - val_acc: 0.8057\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.80770\n",
            "Epoch 40/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0557 - acc: 0.9846 - val_loss: 0.8178 - val_acc: 0.8081\n",
            "\n",
            "Epoch 00040: val_acc improved from 0.80770 to 0.80810, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 41/100\n",
            "391/391 [==============================] - 179s 457ms/step - loss: 0.0529 - acc: 0.9859 - val_loss: 0.8090 - val_acc: 0.8095\n",
            "\n",
            "Epoch 00041: val_acc improved from 0.80810 to 0.80950, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 42/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.0487 - acc: 0.9872 - val_loss: 0.8091 - val_acc: 0.8043\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.80950\n",
            "Epoch 43/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.0520 - acc: 0.9862 - val_loss: 0.8063 - val_acc: 0.8071\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.80950\n",
            "Epoch 44/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.0459 - acc: 0.9884 - val_loss: 0.8085 - val_acc: 0.8090\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.80950\n",
            "Epoch 45/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.0463 - acc: 0.9879 - val_loss: 0.8072 - val_acc: 0.8082\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.80950\n",
            "Epoch 46/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.0439 - acc: 0.9880 - val_loss: 0.8002 - val_acc: 0.8131\n",
            "\n",
            "Epoch 00046: val_acc improved from 0.80950 to 0.81310, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 47/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0416 - acc: 0.9891 - val_loss: 0.7974 - val_acc: 0.8099\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.81310\n",
            "Epoch 48/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.0422 - acc: 0.9888 - val_loss: 0.8042 - val_acc: 0.8090\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.81310\n",
            "Epoch 49/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0381 - acc: 0.9901 - val_loss: 0.7994 - val_acc: 0.8109\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.81310\n",
            "Epoch 50/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.0410 - acc: 0.9899 - val_loss: 0.8007 - val_acc: 0.8108\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.81310\n",
            "Epoch 51/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0404 - acc: 0.9896 - val_loss: 0.7964 - val_acc: 0.8119\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.81310\n",
            "Epoch 52/100\n",
            "391/391 [==============================] - 179s 457ms/step - loss: 0.0391 - acc: 0.9899 - val_loss: 0.8002 - val_acc: 0.8110\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.81310\n",
            "Epoch 53/100\n",
            "391/391 [==============================] - 179s 457ms/step - loss: 0.0389 - acc: 0.9898 - val_loss: 0.7957 - val_acc: 0.8115\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.81310\n",
            "Epoch 54/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0381 - acc: 0.9907 - val_loss: 0.7948 - val_acc: 0.8119\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.81310\n",
            "Epoch 55/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.0374 - acc: 0.9907 - val_loss: 0.7925 - val_acc: 0.8108\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.81310\n",
            "Epoch 56/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0358 - acc: 0.9916 - val_loss: 0.7959 - val_acc: 0.8118\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.81310\n",
            "Epoch 57/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0373 - acc: 0.9906 - val_loss: 0.7964 - val_acc: 0.8115\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.81310\n",
            "Epoch 58/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0355 - acc: 0.9908 - val_loss: 0.7961 - val_acc: 0.8110\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.81310\n",
            "Epoch 59/100\n",
            "391/391 [==============================] - 177s 453ms/step - loss: 0.0354 - acc: 0.9912 - val_loss: 0.7963 - val_acc: 0.8110\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.81310\n",
            "Epoch 60/100\n",
            "391/391 [==============================] - 177s 453ms/step - loss: 0.0374 - acc: 0.9904 - val_loss: 0.7965 - val_acc: 0.8107\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.81310\n",
            "Epoch 61/100\n",
            "391/391 [==============================] - 177s 453ms/step - loss: 0.0347 - acc: 0.9911 - val_loss: 0.7953 - val_acc: 0.8102\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.81310\n",
            "Epoch 62/100\n",
            "391/391 [==============================] - 177s 453ms/step - loss: 0.0352 - acc: 0.9910 - val_loss: 0.7961 - val_acc: 0.8093\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.81310\n",
            "Epoch 63/100\n",
            "391/391 [==============================] - 178s 454ms/step - loss: 0.0333 - acc: 0.9921 - val_loss: 0.7956 - val_acc: 0.8109\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.81310\n",
            "Epoch 64/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.0356 - acc: 0.9910 - val_loss: 0.7947 - val_acc: 0.8099\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.81310\n",
            "Epoch 65/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0353 - acc: 0.9909 - val_loss: 0.7954 - val_acc: 0.8104\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.81310\n",
            "Epoch 66/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0353 - acc: 0.9912 - val_loss: 0.7957 - val_acc: 0.8120\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.81310\n",
            "Epoch 67/100\n",
            "391/391 [==============================] - 179s 458ms/step - loss: 0.0337 - acc: 0.9920 - val_loss: 0.7941 - val_acc: 0.8112\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.81310\n",
            "Epoch 68/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.0336 - acc: 0.9913 - val_loss: 0.7948 - val_acc: 0.8115\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.81310\n",
            "Epoch 69/100\n",
            "391/391 [==============================] - 179s 457ms/step - loss: 0.0345 - acc: 0.9912 - val_loss: 0.7934 - val_acc: 0.8121\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.81310\n",
            "Epoch 70/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0362 - acc: 0.9907 - val_loss: 0.7947 - val_acc: 0.8124\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.81310\n",
            "Epoch 71/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0343 - acc: 0.9911 - val_loss: 0.7944 - val_acc: 0.8111\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.81310\n",
            "Epoch 72/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0348 - acc: 0.9916 - val_loss: 0.7935 - val_acc: 0.8124\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.81310\n",
            "Epoch 73/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.0341 - acc: 0.9913 - val_loss: 0.7945 - val_acc: 0.8117\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.81310\n",
            "Epoch 74/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.0322 - acc: 0.9924 - val_loss: 0.7942 - val_acc: 0.8110\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.81310\n",
            "Epoch 75/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0341 - acc: 0.9915 - val_loss: 0.7944 - val_acc: 0.8121\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.81310\n",
            "Epoch 76/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0341 - acc: 0.9915 - val_loss: 0.7950 - val_acc: 0.8117\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.81310\n",
            "Epoch 77/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.0333 - acc: 0.9913 - val_loss: 0.7945 - val_acc: 0.8117\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.81310\n",
            "Epoch 78/100\n",
            "391/391 [==============================] - 179s 458ms/step - loss: 0.0346 - acc: 0.9914 - val_loss: 0.7945 - val_acc: 0.8115\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.81310\n",
            "Epoch 79/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0332 - acc: 0.9916 - val_loss: 0.7950 - val_acc: 0.8112\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.81310\n",
            "Epoch 80/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0326 - acc: 0.9923 - val_loss: 0.7945 - val_acc: 0.8120\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.81310\n",
            "Epoch 81/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0310 - acc: 0.9927 - val_loss: 0.7949 - val_acc: 0.8116\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.81310\n",
            "Epoch 82/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0329 - acc: 0.9918 - val_loss: 0.7958 - val_acc: 0.8121\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.81310\n",
            "Epoch 83/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0332 - acc: 0.9914 - val_loss: 0.7946 - val_acc: 0.8115\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.81310\n",
            "Epoch 84/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0336 - acc: 0.9918 - val_loss: 0.7943 - val_acc: 0.8119\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.81310\n",
            "Epoch 85/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0334 - acc: 0.9915 - val_loss: 0.7951 - val_acc: 0.8117\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.81310\n",
            "Epoch 86/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0330 - acc: 0.9918 - val_loss: 0.7950 - val_acc: 0.8121\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.81310\n",
            "Epoch 87/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.0330 - acc: 0.9916 - val_loss: 0.7951 - val_acc: 0.8113\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.81310\n",
            "Epoch 88/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.0326 - acc: 0.9920 - val_loss: 0.7945 - val_acc: 0.8122\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.81310\n",
            "Epoch 89/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.0321 - acc: 0.9920 - val_loss: 0.7942 - val_acc: 0.8114\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.81310\n",
            "Epoch 90/100\n",
            "391/391 [==============================] - 177s 453ms/step - loss: 0.0327 - acc: 0.9922 - val_loss: 0.7952 - val_acc: 0.8114\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.81310\n",
            "Epoch 91/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0325 - acc: 0.9919 - val_loss: 0.7947 - val_acc: 0.8105\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.81310\n",
            "Epoch 92/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.0329 - acc: 0.9921 - val_loss: 0.7953 - val_acc: 0.8117\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.81310\n",
            "Epoch 93/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0329 - acc: 0.9918 - val_loss: 0.7949 - val_acc: 0.8109\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.81310\n",
            "Epoch 94/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.0333 - acc: 0.9916 - val_loss: 0.7943 - val_acc: 0.8111\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.81310\n",
            "Epoch 95/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0316 - acc: 0.9924 - val_loss: 0.7950 - val_acc: 0.8117\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.81310\n",
            "Epoch 96/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0326 - acc: 0.9926 - val_loss: 0.7947 - val_acc: 0.8113\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.81310\n",
            "Epoch 97/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.0333 - acc: 0.9919 - val_loss: 0.7950 - val_acc: 0.8118\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.81310\n",
            "Epoch 98/100\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.0326 - acc: 0.9921 - val_loss: 0.7954 - val_acc: 0.8113\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.81310\n",
            "Epoch 99/100\n",
            "391/391 [==============================] - 178s 455ms/step - loss: 0.0332 - acc: 0.9920 - val_loss: 0.7947 - val_acc: 0.8107\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.81310\n",
            "Epoch 100/100\n",
            "391/391 [==============================] - 179s 457ms/step - loss: 0.0330 - acc: 0.9921 - val_loss: 0.7945 - val_acc: 0.8110\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.81310\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7feeee498358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzKciQ-auS_d"
      },
      "source": [
        "### Val accuracy reached 80.23 at the end of 35th epoch and 81.31 at the end of 100 epochs .We have aleady reached our target of 80% val accuracy . Let us train another 100 epochs to see how much further we can push this validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf0LhAkReeHY"
      },
      "source": [
        "def scheduler2(epoch):\n",
        "  if epoch < 15:\n",
        "    return 0.002\n",
        "  elif 15 < epoch < 30: \n",
        "    return 0.001 \n",
        "  elif 13 < epoch < 50: \n",
        "    return 0.0005   \n",
        "  else:\n",
        "    return 0.0005 * math.exp(0.5 * (50 - epoch))\n",
        "\n",
        "lr_callback = keras.callbacks.LearningRateScheduler(scheduler2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4-sWyAkfOYB"
      },
      "source": [
        "opt=SGD(lr=0.002,  momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV-GZ-aseZ59",
        "outputId": "dcd12154-7603-4995-c85c-c080fec46a1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_datagen=ImageDataGenerator(\n",
        "    \n",
        "        \n",
        "        \n",
        "        #preprocessing_function=img_aug2,\n",
        "        horizontal_flip=True,width_shift_range=0.05, height_shift_range=0.05\n",
        "    \n",
        ")\n",
        "\n",
        "val_datagen= ImageDataGenerator(\n",
        "    \n",
        "        \n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "training_generator = train_datagen.flow_from_dataframe(train_df, directory='./data/train/', \n",
        "                                                         x_col='File', y_col='Class', target_size=(224, 224),\n",
        "                                                    color_mode='rgb', interpolation='bicubic',\n",
        "                                                    class_mode='categorical', \n",
        "                                                    batch_size=batch_size, shuffle=True, seed=42)\n",
        "validation_generator = val_datagen.flow_from_dataframe(test_df, directory='./data/test/',\n",
        "                                                         x_col='File', y_col='Class', \n",
        "                                                         target_size=(224, 224),interpolation='bicubic',\n",
        "                                                    color_mode='rgb', class_mode='categorical', \n",
        "                                                    batch_size=batch_size, shuffle=True, seed=42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 50000 validated image filenames belonging to 100 classes.\n",
            "Found 10000 validated image filenames belonging to 100 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC-aMgy0feGq",
        "outputId": "e256db12-af8f-41ac-a691-7b2389e0cb6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#hide_output\n",
        "model.fit_generator(training_generator, epochs=EPOCHS, \n",
        "                        steps_per_epoch=np.ceil(train_features.shape[0]/batch_size), \n",
        "                    validation_steps=np.ceil(test_features.shape[0]/batch_size), \n",
        "                    validation_data=validation_generator,\n",
        "                                 shuffle=True,\n",
        "                                callbacks=[chkpoint_model,lr_callback],\n",
        "                                 verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "391/391 [==============================] - 572s 1s/step - loss: 0.0033 - acc: 0.9996 - val_loss: 0.7933 - val_acc: 0.8118\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.81310\n",
            "Epoch 2/100\n",
            "391/391 [==============================] - 560s 1s/step - loss: 0.0027 - acc: 0.9997 - val_loss: 0.7897 - val_acc: 0.8113\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.81310\n",
            "Epoch 3/100\n",
            "391/391 [==============================] - 563s 1s/step - loss: 0.0022 - acc: 0.9997 - val_loss: 0.7869 - val_acc: 0.8114\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.81310\n",
            "Epoch 4/100\n",
            "391/391 [==============================] - 557s 1s/step - loss: 0.0022 - acc: 0.9997 - val_loss: 0.7868 - val_acc: 0.8123\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.81310\n",
            "Epoch 5/100\n",
            "391/391 [==============================] - 558s 1s/step - loss: 0.0020 - acc: 0.9998 - val_loss: 0.7862 - val_acc: 0.8109\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.81310\n",
            "Epoch 6/100\n",
            "391/391 [==============================] - 559s 1s/step - loss: 0.0020 - acc: 0.9998 - val_loss: 0.7859 - val_acc: 0.8126\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.81310\n",
            "Epoch 7/100\n",
            "391/391 [==============================] - 556s 1s/step - loss: 0.0020 - acc: 0.9998 - val_loss: 0.7855 - val_acc: 0.8132\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.81310 to 0.81320, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 8/100\n",
            "391/391 [==============================] - 556s 1s/step - loss: 0.0019 - acc: 0.9998 - val_loss: 0.7852 - val_acc: 0.8131\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.81320\n",
            "Epoch 9/100\n",
            "391/391 [==============================] - 557s 1s/step - loss: 0.0019 - acc: 0.9997 - val_loss: 0.7850 - val_acc: 0.8131\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.81320\n",
            "Epoch 10/100\n",
            "391/391 [==============================] - 556s 1s/step - loss: 0.0018 - acc: 0.9998 - val_loss: 0.7852 - val_acc: 0.8137\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.81320 to 0.81370, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 11/100\n",
            "391/391 [==============================] - 554s 1s/step - loss: 0.0019 - acc: 0.9997 - val_loss: 0.7851 - val_acc: 0.8135\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.81370\n",
            "Epoch 12/100\n",
            "391/391 [==============================] - 559s 1s/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.7853 - val_acc: 0.8138\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.81370 to 0.81380, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 13/100\n",
            "391/391 [==============================] - 560s 1s/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.7848 - val_acc: 0.8145\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.81380 to 0.81450, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 14/100\n",
            "391/391 [==============================] - 559s 1s/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.7845 - val_acc: 0.8145\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.81450\n",
            "Epoch 15/100\n",
            "391/391 [==============================] - 562s 1s/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.7855 - val_acc: 0.8138\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.81450\n",
            "Epoch 16/100\n",
            "391/391 [==============================] - 564s 1s/step - loss: 0.0016 - acc: 0.9998 - val_loss: 0.7857 - val_acc: 0.8139\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.81450\n",
            "Epoch 17/100\n",
            "391/391 [==============================] - 563s 1s/step - loss: 0.0016 - acc: 0.9998 - val_loss: 0.7855 - val_acc: 0.8147\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.81450 to 0.81470, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 18/100\n",
            "391/391 [==============================] - 570s 1s/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.7842 - val_acc: 0.8146\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.81470\n",
            "Epoch 19/100\n",
            "391/391 [==============================] - 569s 1s/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.7848 - val_acc: 0.8147\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.81470\n",
            "Epoch 20/100\n",
            "391/391 [==============================] - 565s 1s/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.7859 - val_acc: 0.8149\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.81470 to 0.81490, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 21/100\n",
            "391/391 [==============================] - 567s 1s/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.7853 - val_acc: 0.8148\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.81490\n",
            "Epoch 22/100\n",
            "391/391 [==============================] - 568s 1s/step - loss: 0.0014 - acc: 0.9999 - val_loss: 0.7847 - val_acc: 0.8144\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.81490\n",
            "Epoch 23/100\n",
            "391/391 [==============================] - 561s 1s/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.7857 - val_acc: 0.8144\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.81490\n",
            "Epoch 24/100\n",
            "391/391 [==============================] - 561s 1s/step - loss: 0.0015 - acc: 0.9997 - val_loss: 0.7852 - val_acc: 0.8145\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.81490\n",
            "Epoch 25/100\n",
            "391/391 [==============================] - 567s 1s/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.7851 - val_acc: 0.8148\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.81490\n",
            "Epoch 26/100\n",
            "391/391 [==============================] - 575s 1s/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.7857 - val_acc: 0.8144\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.81490\n",
            "Epoch 27/100\n",
            "391/391 [==============================] - 565s 1s/step - loss: 0.0015 - acc: 0.9997 - val_loss: 0.7848 - val_acc: 0.8152\n",
            "\n",
            "Epoch 00027: val_acc improved from 0.81490 to 0.81520, saving model to /gdrive/My Drive/EVA/session20/best_model2.h5\n",
            "Epoch 28/100\n",
            "371/391 [===========================>..] - ETA: 28s - loss: 0.0015 - acc: 0.9997"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyfKo1jqutqt"
      },
      "source": [
        "Runtime disconnected after 27 epochs . Val accuracy has reached 81.52 . We will stop here although we could load the model again and train for more epochs to see how much farther we could go. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPQKLqgp6A3Q"
      },
      "source": [
        "### Load the model saved best model from google drive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkZPFChWJxT9",
        "outputId": "87281cb9-0eed-424b-9acd-a832bf78dd08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "#hide_output\n",
        "model= keras.models.load_model('/gdrive/My Drive/EVA/session20/best_model2.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYf_hNV-R-aF"
      },
      "source": [
        "Evaluate and print validation loss and validation accuracy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ituaPOAa29pu"
      },
      "source": [
        "score=model.evaluate_generator(validation_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgrDB4jp3LQR",
        "outputId": "eb212c4d-a16b-462a-ef57-bd10bff1ad26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('validation loss =',score[0] , ', Validation accuracy =',score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation loss = 0.7847665718078614 , Validation accuracy = 0.8152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgzqgiueEetl"
      },
      "source": [
        "### We used the technique of Transfer Learning and fine-tuned a pre-trained a ResNet34 model with Imagenet weights to classify images in the CIFAR100 dataset. In order to achieve this we added our own prediction layer on top of the base model and trained it to achieve 81.52 max validation accuracy ."
      ]
    }
  ]
}